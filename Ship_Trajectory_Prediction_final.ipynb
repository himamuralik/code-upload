{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himamuralik/code-upload/blob/main/Ship_Trajectory_Prediction_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyEP4EiVbhDE",
        "outputId": "e8f49762-3f91-4d57-a59b-648694f3c60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import logging\n",
        "import pandas as pd\n",
        "\n",
        "# Base directory (adjust if using Google Drive)\n",
        "data_directory = '/content/data/'\n",
        "# Year range\n",
        "start_year = 2015\n",
        "end_year = 2017\n",
        "years = range(start_year, end_year + 1)\n",
        "\n",
        "\n",
        "# Data source\n",
        "base_url = 'https://coast.noaa.gov/htdata/CMSP/AISDataHandler/'\n",
        "\n",
        "# Trajectory segmentation and filtering\n",
        "new_trajectory_time_gap = 120 * 60  # 2 hours\n",
        "sog_cutoff = 30\n",
        "empirical_speed_cutoff = 40\n",
        "interpolation_time_gap = 5 * 60     # 5 minutes\n",
        "\n",
        "# History and prediction window (3h each)\n",
        "length_of_history = int(3 * 60 * 60 / interpolation_time_gap) + 1\n",
        "length_into_the_future = int(3 * 60 * 60 / interpolation_time_gap) - 1\n",
        "min_track_length = (length_of_history + length_into_the_future) * interpolation_time_gap\n",
        "\n",
        "# Filtering vessel types and statuses\n",
        "vessel_types = [\n",
        "    'cargo', 'passenger', 'fishing',\n",
        "    'tug tow', 'tanker', 'pleasure craft or sailing',\n",
        "]\n",
        "desired_statuses = [\n",
        "    'under way sailing', 'under way using engine', 'undefined'\n",
        "]\n",
        "\n",
        "# Temporal resolution (30-min only)\n",
        "time_gaps = [30 * 60]\n",
        "\n",
        "# Categorical (static) feature\n",
        "categorical_columns = ['vessel_group']\n",
        "\n",
        "# Feature columns for models\n",
        "dynamic_columns = ['lat', 'lon', 'sog', 'cog', 'year', 'day', 'month']\n",
        "static_columns = ['month', 'year']\n",
        "\n",
        "# Load label mappings\n",
        "try:\n",
        "    statuses = pd.read_csv('navigational_statuses.csv')\n",
        "    types = pd.read_csv('vessel_types.csv')\n",
        "except FileNotFoundError:\n",
        "    statuses = pd.DataFrame()\n",
        "    types = pd.DataFrame()\n",
        "\n",
        "# Logging\n",
        "log_level = logging.WARNING\n",
        "\n",
        "def set_log_level(level):\n",
        "    global log_level\n",
        "    levels = [\n",
        "        logging.CRITICAL, logging.ERROR, logging.WARNING,\n",
        "        logging.INFO, logging.DEBUG\n",
        "    ]\n",
        "    log_level = levels[level]\n",
        "    logging.basicConfig(level=log_level)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python config.py"
      ],
      "metadata": {
        "id": "fwLNRTNphHKw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_config.py\n",
        "class DatasetConfig:\n",
        "    def __init__(self, dataset_name,\n",
        "                 lat_1, lat_2, lon_1, lon_2,\n",
        "                 sliding_window_movement,\n",
        "                 depth_1=0, depth_2=0,\n",
        "                 min_pts_to_try=None, eps_to_try=None,\n",
        "                 min_pts_to_use=None, eps_to_use=None):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.lat_1 = min(lat_1, lat_2)\n",
        "        self.lat_2 = max(lat_1, lat_2)\n",
        "        self.lon_1 = min(lon_1, lon_2)\n",
        "        self.lon_2 = max(lon_1, lon_2)\n",
        "        self.corner_1 = (lat_1, lon_1)\n",
        "        self.corner_2 = (lat_2, lon_2)\n",
        "        self.sliding_window_movement = sliding_window_movement\n",
        "        self.min_pts_to_try = min_pts_to_try\n",
        "        self.eps_to_try = eps_to_try\n",
        "        self.min_pts_to_use = min_pts_to_use\n",
        "        self.eps_to_use = eps_to_use\n",
        "        self.depth_1 = depth_1\n",
        "        self.depth_2 = depth_2\n",
        "\n",
        "datasets = {\n",
        "    'new_york': DatasetConfig(\n",
        "        dataset_name='new_york',\n",
        "        lat_1=39.50, lon_1=-74.50, lat_2=41.50, lon_2=-71.50,\n",
        "        sliding_window_movement=60 * 60,\n",
        "        min_pts_to_try=[4, 10, 20, 50, 100, 250, 500],\n",
        "        eps_to_try=[\n",
        "            0.0001, 0.00025, 0.0005, 0.00075,\n",
        "            0.001, 0.0025, 0.005, 0.0075,\n",
        "            0.01, 0.025, 0.05, 0.075,\n",
        "            0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,\n",
        "            1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5,\n",
        "            6, 7, 8, 9, 10\n",
        "        ],\n",
        "        min_pts_to_use=50,\n",
        "        eps_to_use=3\n",
        "    )\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-uaKXCshs7l",
        "outputId": "f8fe3e73-9571-47ed-aa9f-7c01299cb566"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_config.py"
      ],
      "metadata": {
        "id": "sLJSg_0Uj5bl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile downloader.py\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import pandas as pd\n",
        "import time\n",
        "import shutil  # New import for streaming\n",
        "import tempfile  # New import for temporary file handling\n",
        "from calendar import monthrange\n",
        "from datetime import date, timedelta\n",
        "\n",
        "import config\n",
        "import data_config\n",
        "\n",
        "# ---- Dataset Configuration ---- #\n",
        "# This loads the 'new_york' dataset configuration from data_config.py\n",
        "dataset_config = data_config.datasets['new_york']\n",
        "\n",
        "# Assign geographical boundaries from the config\n",
        "LAT_MIN = dataset_config.lat_1\n",
        "LAT_MAX = dataset_config.lat_2\n",
        "LON_MIN = dataset_config.lon_1\n",
        "LON_MAX = dataset_config.lon_2\n",
        "YEARS = config.years\n",
        "BASE_URL = config.base_url\n",
        "\n",
        "# ---- File Paths ---- #\n",
        "# Create the base and filtered directories from the config.\n",
        "BASE_DIR = os.path.join(config.data_directory, dataset_config.dataset_name, 'downloads')\n",
        "FILTERED_DIR = os.path.join(BASE_DIR, 'filtered')\n",
        "PROGRESS_LOG = os.path.join(BASE_DIR, 'progress_log.txt')\n",
        "os.makedirs(FILTERED_DIR, exist_ok=True)\n",
        "\n",
        "# ---- Retry-Enabled Download ---- #\n",
        "def safe_request(url, retries=5, wait=10, timeout=300):\n",
        "    \"\"\"\n",
        "    Handles downloading a file with a more robust retry mechanism.\n",
        "\n",
        "    Tries to make a GET request and returns the response if successful (status code 200).\n",
        "    It will retry a maximum of `retries` times with a `wait` period between attempts,\n",
        "    and uses a longer `timeout` for large downloads.\n",
        "    \"\"\"\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            # Use stream=True for large downloads\n",
        "            print(f\" - Attempt {attempt}/{retries}: Requesting {url} with a {timeout}s timeout...\")\n",
        "            r = requests.get(url, stream=True, timeout=timeout)\n",
        "            if r.status_code == 200:\n",
        "                print(f\" - Download successful on attempt {attempt}.\")\n",
        "                return r\n",
        "            print(f\" - Attempt {attempt}: HTTP {r.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\" - Attempt {attempt}: An error occurred: {e}\")\n",
        "        time.sleep(wait)\n",
        "    print(f\" - Failed to download {url} after {retries} attempts.\")\n",
        "    return None\n",
        "\n",
        "# ---- Progress Tracking ---- #\n",
        "def load_completed():\n",
        "    \"\"\"Reads the progress log file and returns a set of completed specifiers.\"\"\"\n",
        "    if not os.path.exists(PROGRESS_LOG):\n",
        "        return set()\n",
        "    with open(PROGRESS_LOG, 'r') as f:\n",
        "        return set(line.strip() for line in f)\n",
        "\n",
        "def log_completed(specifier):\n",
        "    \"\"\"Appends a new specifier to the progress log file.\"\"\"\n",
        "    with open(PROGRESS_LOG, 'a') as f:\n",
        "        f.write(specifier + '\\n')\n",
        "\n",
        "# ---- Download -> Filter -> Save ---- #\n",
        "def download_and_filter(specifier):\n",
        "    \"\"\"\n",
        "    Downloads a single file by streaming to disk, filters it, and saves it.\n",
        "    \"\"\"\n",
        "    # specifier will be something like \"2015/AIS_2015_01_18.zip\"\n",
        "    year_str, fname = specifier.split('/')\n",
        "\n",
        "    # Drop the \"AIS_\" prefix and \".zip\" when naming your parquet file.\n",
        "    # The output format is already day-by-day (YYYY_MM_DD).\n",
        "    datepart = fname[len(\"AIS_\"):-len(\".zip\")]\n",
        "    outname = os.path.join(\n",
        "        FILTERED_DIR,\n",
        "        f\"{dataset_config.dataset_name}_{datepart}.parquet\"\n",
        "    )\n",
        "\n",
        "    # Skip if the filtered file already exists on disk.\n",
        "    if os.path.exists(outname):\n",
        "        print(f\" - Skipping {specifier}: Parquet file already exists at {outname}\")\n",
        "        # Log it as completed to prevent re-downloading\n",
        "        log_completed(specifier)\n",
        "        return\n",
        "\n",
        "    # Construct the full URL for the download.\n",
        "    url = f\"{BASE_URL.rstrip('/')}/{specifier}\"\n",
        "    print(f\"Processing {url} ...\")\n",
        "    r = safe_request(url)\n",
        "    if not r:\n",
        "        print(f\" - Failed to download {specifier}\")\n",
        "        return\n",
        "\n",
        "    # Use a temporary file to stream the download to disk, which is more memory-efficient.\n",
        "    temp_file_path = None\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "            print(\" - Streaming download to temporary file...\")\n",
        "            shutil.copyfileobj(r.raw, temp_file)\n",
        "            temp_file_path = temp_file.name\n",
        "\n",
        "        # Now that the file is on disk, we can process it from there.\n",
        "        with zipfile.ZipFile(temp_file_path, 'r') as z:\n",
        "            csv_name = next(f for f in z.namelist() if f.endswith('.csv'))\n",
        "            with z.open(csv_name) as csvfile:\n",
        "                # Read the CSV in chunks to handle very large files efficiently.\n",
        "                chunks = pd.read_csv(\n",
        "                    csvfile,\n",
        "                    low_memory=False,\n",
        "                    chunksize=100_000\n",
        "                )\n",
        "\n",
        "                filtered_parts = []\n",
        "                for chunk in chunks:\n",
        "                    # Apply the bounding-box filter using the geographical boundaries from the config.\n",
        "                    sub = chunk[\n",
        "                        (chunk[\"LAT\"] >= LAT_MIN) & (chunk[\"LAT\"] <= LAT_MAX) &\n",
        "                        (chunk[\"LON\"] >= LON_MIN) & (chunk[\"LON\"] <= LON_MAX)\n",
        "                    ]\n",
        "                    if not sub.empty:\n",
        "                        filtered_parts.append(sub)\n",
        "\n",
        "        if not filtered_parts:\n",
        "            print(\" - No data in region, skipping.\")\n",
        "            # Log as completed to prevent retrying the same file repeatedly.\n",
        "            log_completed(specifier)\n",
        "            return\n",
        "\n",
        "        # Concatenate all filtered chunks, sort, and save to a parquet file.\n",
        "        df = pd.concat(filtered_parts, ignore_index=True)\n",
        "        df = df.sort_values([\"MMSI\", \"BaseDateTime\"])\n",
        "        df.to_parquet(outname, index=False, compression='snappy')\n",
        "        print(f\" - Saved {len(df):,} records to {outname}\")\n",
        "\n",
        "        # Record progress so we can resume later.\n",
        "        log_completed(specifier)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" - An error occurred while processing {specifier}: {e}\")\n",
        "    finally:\n",
        "        # Always remove the temporary file, regardless of success or failure.\n",
        "        if temp_file_path and os.path.exists(temp_file_path):\n",
        "            os.remove(temp_file_path)\n",
        "\n",
        "# ---- Main Loop ---- #\n",
        "if __name__ == \"__main__\":\n",
        "    # Build a list of all file specifiers for the years and months we want to download.\n",
        "    specifiers = []\n",
        "    for year in YEARS:\n",
        "        # Since 'desired_months' is not in config.py, we'll iterate through all 12 months.\n",
        "        for month in range(1, 13):\n",
        "            days_in_month = monthrange(year, month)[1]\n",
        "            for day in range(1, days_in_month + 1):\n",
        "                # Correctly format the filename to match the NOAA server's structure (YYYY_MM_DD).\n",
        "                fname = f\"AIS_{year}_{month:02d}_{day:02d}.zip\"\n",
        "                specifiers.append(f\"{year}/{fname}\")\n",
        "\n",
        "    completed_specifiers = load_completed()\n",
        "    for spec in specifiers:\n",
        "        # We check both the log and if the final parquet file exists to avoid re-work.\n",
        "        if spec in completed_specifiers:\n",
        "            print(f\"Skipping {spec} (already done)\")\n",
        "            continue\n",
        "        download_and_filter(spec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BML-C5Xeg_Ao",
        "outputId": "44f9254f-c234-4c53-fbfd-a8698b453e25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting downloader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python downloader.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVzX6sjun5Lb",
        "outputId": "60801e4a-db91-49c6-bd97-530ad391120e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping 2015/AIS_2015_01_01.zip (already done)\n",
            "Skipping 2015/AIS_2015_01_02.zip (already done)\n",
            "Skipping 2015/AIS_2015_01_03.zip (already done)\n",
            "Skipping 2015/AIS_2015_01_04.zip (already done)\n",
            "Skipping 2015/AIS_2015_01_05.zip (already done)\n",
            "Skipping 2015/AIS_2015_01_06.zip (already done)\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_07.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_07.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 424,905 records to /content/data/new_york/downloads/filtered/new_york_2015_01_07.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_08.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_08.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 293,151 records to /content/data/new_york/downloads/filtered/new_york_2015_01_08.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_09.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_09.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 288,596 records to /content/data/new_york/downloads/filtered/new_york_2015_01_09.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_10.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_10.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 274,905 records to /content/data/new_york/downloads/filtered/new_york_2015_01_10.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_11.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_11.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 280,140 records to /content/data/new_york/downloads/filtered/new_york_2015_01_11.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_12.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_12.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 290,445 records to /content/data/new_york/downloads/filtered/new_york_2015_01_12.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_13.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_13.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 287,181 records to /content/data/new_york/downloads/filtered/new_york_2015_01_13.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_14.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_14.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 284,348 records to /content/data/new_york/downloads/filtered/new_york_2015_01_14.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_15.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_15.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 257,090 records to /content/data/new_york/downloads/filtered/new_york_2015_01_15.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_16.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_16.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 275,531 records to /content/data/new_york/downloads/filtered/new_york_2015_01_16.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_17.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_17.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 252,709 records to /content/data/new_york/downloads/filtered/new_york_2015_01_17.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_18.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_18.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 212,437 records to /content/data/new_york/downloads/filtered/new_york_2015_01_18.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_19.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_19.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 259,444 records to /content/data/new_york/downloads/filtered/new_york_2015_01_19.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_20.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_20.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 273,777 records to /content/data/new_york/downloads/filtered/new_york_2015_01_20.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_21.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_21.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 281,768 records to /content/data/new_york/downloads/filtered/new_york_2015_01_21.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_22.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_22.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 279,464 records to /content/data/new_york/downloads/filtered/new_york_2015_01_22.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_23.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_23.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 293,420 records to /content/data/new_york/downloads/filtered/new_york_2015_01_23.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_24.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_24.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 258,705 records to /content/data/new_york/downloads/filtered/new_york_2015_01_24.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_25.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_25.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 250,436 records to /content/data/new_york/downloads/filtered/new_york_2015_01_25.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_26.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_26.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 258,229 records to /content/data/new_york/downloads/filtered/new_york_2015_01_26.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_27.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_27.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 241,859 records to /content/data/new_york/downloads/filtered/new_york_2015_01_27.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_28.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_28.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 265,346 records to /content/data/new_york/downloads/filtered/new_york_2015_01_28.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_29.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_29.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 282,066 records to /content/data/new_york/downloads/filtered/new_york_2015_01_29.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_30.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_30.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 274,989 records to /content/data/new_york/downloads/filtered/new_york_2015_01_30.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_31.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_01_31.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 245,333 records to /content/data/new_york/downloads/filtered/new_york_2015_01_31.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_01.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_01.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 239,211 records to /content/data/new_york/downloads/filtered/new_york_2015_02_01.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_02.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_02.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 423,294 records to /content/data/new_york/downloads/filtered/new_york_2015_02_02.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_03.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_03.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 263,503 records to /content/data/new_york/downloads/filtered/new_york_2015_02_03.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_04.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_04.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 273,735 records to /content/data/new_york/downloads/filtered/new_york_2015_02_04.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_05.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_05.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 273,175 records to /content/data/new_york/downloads/filtered/new_york_2015_02_05.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_06.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_06.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 271,390 records to /content/data/new_york/downloads/filtered/new_york_2015_02_06.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_07.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_07.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 252,241 records to /content/data/new_york/downloads/filtered/new_york_2015_02_07.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_08.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_08.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 246,494 records to /content/data/new_york/downloads/filtered/new_york_2015_02_08.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_09.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_09.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 275,456 records to /content/data/new_york/downloads/filtered/new_york_2015_02_09.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_10.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_10.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 269,926 records to /content/data/new_york/downloads/filtered/new_york_2015_02_10.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_11.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_11.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 270,159 records to /content/data/new_york/downloads/filtered/new_york_2015_02_11.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_12.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_12.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 256,782 records to /content/data/new_york/downloads/filtered/new_york_2015_02_12.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_13.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_13.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 266,421 records to /content/data/new_york/downloads/filtered/new_york_2015_02_13.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_14.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_14.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 246,001 records to /content/data/new_york/downloads/filtered/new_york_2015_02_14.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_15.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_15.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 235,380 records to /content/data/new_york/downloads/filtered/new_york_2015_02_15.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_16.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_16.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 241,511 records to /content/data/new_york/downloads/filtered/new_york_2015_02_16.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_17.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_17.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 262,617 records to /content/data/new_york/downloads/filtered/new_york_2015_02_17.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_18.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_18.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 273,312 records to /content/data/new_york/downloads/filtered/new_york_2015_02_18.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_19.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_19.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 240,475 records to /content/data/new_york/downloads/filtered/new_york_2015_02_19.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_20.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_20.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 258,361 records to /content/data/new_york/downloads/filtered/new_york_2015_02_20.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_21.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_21.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 258,218 records to /content/data/new_york/downloads/filtered/new_york_2015_02_21.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_22.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_22.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 250,122 records to /content/data/new_york/downloads/filtered/new_york_2015_02_22.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_23.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_23.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 274,269 records to /content/data/new_york/downloads/filtered/new_york_2015_02_23.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_24.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_24.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 273,582 records to /content/data/new_york/downloads/filtered/new_york_2015_02_24.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_25.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_25.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 275,130 records to /content/data/new_york/downloads/filtered/new_york_2015_02_25.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_26.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_26.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 273,340 records to /content/data/new_york/downloads/filtered/new_york_2015_02_26.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_27.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_27.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 285,160 records to /content/data/new_york/downloads/filtered/new_york_2015_02_27.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_28.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_02_28.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 258,195 records to /content/data/new_york/downloads/filtered/new_york_2015_02_28.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_01.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_01.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 260,764 records to /content/data/new_york/downloads/filtered/new_york_2015_03_01.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_02.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_02.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 275,632 records to /content/data/new_york/downloads/filtered/new_york_2015_03_02.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_03.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_03.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 239,270 records to /content/data/new_york/downloads/filtered/new_york_2015_03_03.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_04.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_04.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 281,248 records to /content/data/new_york/downloads/filtered/new_york_2015_03_04.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_05.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_05.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 278,242 records to /content/data/new_york/downloads/filtered/new_york_2015_03_05.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_06.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_06.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 284,092 records to /content/data/new_york/downloads/filtered/new_york_2015_03_06.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_07.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_07.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 265,247 records to /content/data/new_york/downloads/filtered/new_york_2015_03_07.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_08.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_08.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 258,346 records to /content/data/new_york/downloads/filtered/new_york_2015_03_08.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_09.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_09.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 294,392 records to /content/data/new_york/downloads/filtered/new_york_2015_03_09.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_10.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_10.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 280,561 records to /content/data/new_york/downloads/filtered/new_york_2015_03_10.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_11.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_11.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 562,140 records to /content/data/new_york/downloads/filtered/new_york_2015_03_11.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_12.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_12.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 279,864 records to /content/data/new_york/downloads/filtered/new_york_2015_03_12.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_13.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_13.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 587,907 records to /content/data/new_york/downloads/filtered/new_york_2015_03_13.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_14.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_14.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 265,546 records to /content/data/new_york/downloads/filtered/new_york_2015_03_14.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_15.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_15.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 222 records to /content/data/new_york/downloads/filtered/new_york_2015_03_15.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_16.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_16.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 265,797 records to /content/data/new_york/downloads/filtered/new_york_2015_03_16.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_17.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_17.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 277,608 records to /content/data/new_york/downloads/filtered/new_york_2015_03_17.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_18.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_18.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 286,306 records to /content/data/new_york/downloads/filtered/new_york_2015_03_18.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_19.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_19.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 292,074 records to /content/data/new_york/downloads/filtered/new_york_2015_03_19.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_20.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_20.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 288,268 records to /content/data/new_york/downloads/filtered/new_york_2015_03_20.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_21.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_21.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 247,077 records to /content/data/new_york/downloads/filtered/new_york_2015_03_21.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_22.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_22.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 254,066 records to /content/data/new_york/downloads/filtered/new_york_2015_03_22.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_23.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_23.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 277,348 records to /content/data/new_york/downloads/filtered/new_york_2015_03_23.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_24.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_24.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 286,284 records to /content/data/new_york/downloads/filtered/new_york_2015_03_24.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_25.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_25.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 296,600 records to /content/data/new_york/downloads/filtered/new_york_2015_03_25.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_26.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_26.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 298,818 records to /content/data/new_york/downloads/filtered/new_york_2015_03_26.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_27.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_27.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 293,646 records to /content/data/new_york/downloads/filtered/new_york_2015_03_27.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_28.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_28.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 259,794 records to /content/data/new_york/downloads/filtered/new_york_2015_03_28.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_29.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_29.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 246,294 records to /content/data/new_york/downloads/filtered/new_york_2015_03_29.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_30.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_30.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 249,571 records to /content/data/new_york/downloads/filtered/new_york_2015_03_30.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_31.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_03_31.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 257,695 records to /content/data/new_york/downloads/filtered/new_york_2015_03_31.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_01.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_01.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 291,835 records to /content/data/new_york/downloads/filtered/new_york_2015_04_01.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_02.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_02.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 305,224 records to /content/data/new_york/downloads/filtered/new_york_2015_04_02.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_03.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_03.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 285,452 records to /content/data/new_york/downloads/filtered/new_york_2015_04_03.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_04.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_04.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 259,728 records to /content/data/new_york/downloads/filtered/new_york_2015_04_04.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_05.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_05.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 240,970 records to /content/data/new_york/downloads/filtered/new_york_2015_04_05.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_06.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_06.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 279,262 records to /content/data/new_york/downloads/filtered/new_york_2015_04_06.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_07.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_07.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 273,813 records to /content/data/new_york/downloads/filtered/new_york_2015_04_07.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_08.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_08.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 268,337 records to /content/data/new_york/downloads/filtered/new_york_2015_04_08.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_09.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_09.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 277,088 records to /content/data/new_york/downloads/filtered/new_york_2015_04_09.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_10.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_10.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 284,645 records to /content/data/new_york/downloads/filtered/new_york_2015_04_10.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_11.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_11.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 258,594 records to /content/data/new_york/downloads/filtered/new_york_2015_04_11.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_12.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_12.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 259,158 records to /content/data/new_york/downloads/filtered/new_york_2015_04_12.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_13.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_13.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 302,454 records to /content/data/new_york/downloads/filtered/new_york_2015_04_13.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_14.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_14.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 298,920 records to /content/data/new_york/downloads/filtered/new_york_2015_04_14.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_15.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_15.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 284,146 records to /content/data/new_york/downloads/filtered/new_york_2015_04_15.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_16.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_16.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 286,788 records to /content/data/new_york/downloads/filtered/new_york_2015_04_16.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_17.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_17.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 284,749 records to /content/data/new_york/downloads/filtered/new_york_2015_04_17.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_18.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_18.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 274,740 records to /content/data/new_york/downloads/filtered/new_york_2015_04_18.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_19.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_19.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 258,864 records to /content/data/new_york/downloads/filtered/new_york_2015_04_19.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_20.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_20.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 275,329 records to /content/data/new_york/downloads/filtered/new_york_2015_04_20.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_21.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_21.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 280,504 records to /content/data/new_york/downloads/filtered/new_york_2015_04_21.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_22.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_22.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 302,562 records to /content/data/new_york/downloads/filtered/new_york_2015_04_22.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_23.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_23.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 305,138 records to /content/data/new_york/downloads/filtered/new_york_2015_04_23.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_24.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_24.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 313,425 records to /content/data/new_york/downloads/filtered/new_york_2015_04_24.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_25.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_25.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 282,814 records to /content/data/new_york/downloads/filtered/new_york_2015_04_25.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_26.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_26.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 285,188 records to /content/data/new_york/downloads/filtered/new_york_2015_04_26.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_27.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_27.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 305,687 records to /content/data/new_york/downloads/filtered/new_york_2015_04_27.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_28.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_28.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 303,540 records to /content/data/new_york/downloads/filtered/new_york_2015_04_28.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_29.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_29.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 299,994 records to /content/data/new_york/downloads/filtered/new_york_2015_04_29.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_30.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_04_30.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 297,599 records to /content/data/new_york/downloads/filtered/new_york_2015_04_30.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_01.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_01.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 308,281 records to /content/data/new_york/downloads/filtered/new_york_2015_05_01.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_02.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_02.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 298,746 records to /content/data/new_york/downloads/filtered/new_york_2015_05_02.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_03.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_03.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 233,189 records to /content/data/new_york/downloads/filtered/new_york_2015_05_03.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_04.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_04.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 312,922 records to /content/data/new_york/downloads/filtered/new_york_2015_05_04.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_05.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_05.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 323,930 records to /content/data/new_york/downloads/filtered/new_york_2015_05_05.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_06.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_06.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 321,292 records to /content/data/new_york/downloads/filtered/new_york_2015_05_06.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_07.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_07.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 313,257 records to /content/data/new_york/downloads/filtered/new_york_2015_05_07.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_08.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_08.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 324,883 records to /content/data/new_york/downloads/filtered/new_york_2015_05_08.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_09.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_09.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 305,474 records to /content/data/new_york/downloads/filtered/new_york_2015_05_09.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_10.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_10.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 302,051 records to /content/data/new_york/downloads/filtered/new_york_2015_05_10.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_11.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_11.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 314,243 records to /content/data/new_york/downloads/filtered/new_york_2015_05_11.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_12.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_12.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 315,246 records to /content/data/new_york/downloads/filtered/new_york_2015_05_12.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_13.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_13.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 314,095 records to /content/data/new_york/downloads/filtered/new_york_2015_05_13.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_14.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_14.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 310,448 records to /content/data/new_york/downloads/filtered/new_york_2015_05_14.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_15.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_15.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 341,730 records to /content/data/new_york/downloads/filtered/new_york_2015_05_15.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_16.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_16.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 316,159 records to /content/data/new_york/downloads/filtered/new_york_2015_05_16.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_17.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_17.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 315,746 records to /content/data/new_york/downloads/filtered/new_york_2015_05_17.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_18.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_18.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 319,860 records to /content/data/new_york/downloads/filtered/new_york_2015_05_18.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_19.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_19.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 341,453 records to /content/data/new_york/downloads/filtered/new_york_2015_05_19.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_20.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_20.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 350,182 records to /content/data/new_york/downloads/filtered/new_york_2015_05_20.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_21.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_21.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 356,977 records to /content/data/new_york/downloads/filtered/new_york_2015_05_21.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_22.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_22.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 355,895 records to /content/data/new_york/downloads/filtered/new_york_2015_05_22.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_23.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_23.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 351,024 records to /content/data/new_york/downloads/filtered/new_york_2015_05_23.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_24.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_24.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 330,262 records to /content/data/new_york/downloads/filtered/new_york_2015_05_24.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_25.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_25.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 359,411 records to /content/data/new_york/downloads/filtered/new_york_2015_05_25.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_26.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_26.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 352,601 records to /content/data/new_york/downloads/filtered/new_york_2015_05_26.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_27.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_27.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 350,937 records to /content/data/new_york/downloads/filtered/new_york_2015_05_27.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_28.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_28.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 310,603 records to /content/data/new_york/downloads/filtered/new_york_2015_05_28.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_29.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_29.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 364,959 records to /content/data/new_york/downloads/filtered/new_york_2015_05_29.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_30.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_30.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 374,297 records to /content/data/new_york/downloads/filtered/new_york_2015_05_30.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_31.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_05_31.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 330,609 records to /content/data/new_york/downloads/filtered/new_york_2015_05_31.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_01.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_01.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 335,686 records to /content/data/new_york/downloads/filtered/new_york_2015_06_01.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_02.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_02.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 334,494 records to /content/data/new_york/downloads/filtered/new_york_2015_06_02.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_03.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_03.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 326,301 records to /content/data/new_york/downloads/filtered/new_york_2015_06_03.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_04.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_04.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 456,130 records to /content/data/new_york/downloads/filtered/new_york_2015_06_04.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_05.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_05.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 346,633 records to /content/data/new_york/downloads/filtered/new_york_2015_06_05.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_06.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_06.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 363,265 records to /content/data/new_york/downloads/filtered/new_york_2015_06_06.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_07.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_07.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 352,497 records to /content/data/new_york/downloads/filtered/new_york_2015_06_07.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_08.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_08.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 383,671 records to /content/data/new_york/downloads/filtered/new_york_2015_06_08.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_09.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_09.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 363,846 records to /content/data/new_york/downloads/filtered/new_york_2015_06_09.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_10.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_10.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            " - Saved 382,335 records to /content/data/new_york/downloads/filtered/new_york_2015_06_10.parquet\n",
            "Processing https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_11.zip ...\n",
            " - Attempt 1/5: Requesting https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2015/AIS_2015_06_11.zip with a 300s timeout...\n",
            " - Download successful on attempt 1.\n",
            " - Streaming download to temporary file...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 779, in _error_catcher\n",
            "    yield\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 904, in _raw_read\n",
            "    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 887, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 473, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1314, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ssl.py\", line 1166, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/downloader.py\", line 172, in <module>\n",
            "    download_and_filter(spec)\n",
            "  File \"/content/downloader.py\", line 107, in download_and_filter\n",
            "    shutil.copyfileobj(r.raw, temp_file)\n",
            "  File \"/usr/lib/python3.11/shutil.py\", line 197, in copyfileobj\n",
            "    buf = fsrc_read(length)\n",
            "          ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 980, in read\n",
            "    data = self._raw_read(amt)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 903, in _raw_read\n",
            "    with self._error_catcher():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\", line 819, in _error_catcher\n",
            "    self._original_response.close()\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 424, in close\n",
            "    def close(self):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Zip the folder\n",
        "!zip -r /content/downloads_ny.zip /content/data/new_york/downloads\n",
        "\n",
        "# 2. Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. Copy the ZIP into MyDrive\n",
        "!cp /content/downloads_ny.zip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0PklQDOBrvm",
        "outputId": "b4b04a70-bceb-43fd-d5d5-4b47f33c877d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/data/new_york/downloads/ (stored 0%)\n",
            "  adding: content/data/new_york/downloads/progress_log.txt (deflated 72%)\n",
            "  adding: content/data/new_york/downloads/filtered/ (stored 0%)\n",
            "  adding: content/data/new_york/downloads/filtered/new_york_2015_01_02.parquet (deflated 27%)\n",
            "  adding: content/data/new_york/downloads/filtered/new_york_2015_01_06.parquet (deflated 26%)\n",
            "  adding: content/data/new_york/downloads/filtered/new_york_2015_01_04.parquet (deflated 28%)\n",
            "  adding: content/data/new_york/downloads/filtered/new_york_2015_01_05.parquet\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import urllib\n",
        "import pandas as pd\n",
        "import utm\n",
        "from calendar import monthrange\n",
        "from dateutil import rrule\n",
        "from datetime import datetime\n",
        "\n",
        "# ------------------ ZONE UTIL ------------------ #\n",
        "\n",
        "def get_zones_from_coordinates(corner_1, corner_2):\n",
        "    \"\"\"Get UTM zones from lat/lon corners.\"\"\"\n",
        "    _, _, zone_1, _ = utm.from_latlon(*corner_1)\n",
        "    _, _, zone_2, _ = utm.from_latlon(*corner_2)\n",
        "    if zone_1 > 19 or zone_2 > 19:\n",
        "        raise ValueError(\"UTM zone is outside NOAA's available data.\")\n",
        "    return range(min(zone_1, zone_2), max(zone_1, zone_2) + 1)\n",
        "\n",
        "# ------------------ FILENAME PARSING ------------------ #\n",
        "\n",
        "def get_file_specifier(year, month, zone_or_day, extension):\n",
        "    \"\"\"Construct file name for a given year/month/zone/day.\"\"\"\n",
        "    if year in (2015, 2016, 2017):\n",
        "        specifier = f\"AIS_{year}_{month:02d}_Zone{zone_or_day:02d}.{extension}\"\n",
        "    elif year >= 2018:\n",
        "        specifier = f\"AIS_{year}_{month:02d}_{zone_or_day:02d}.{extension}\"\n",
        "    else:\n",
        "        raise ValueError(f\"Year {year} not supported. Check NOAA AIS file format.\")\n",
        "    return urllib.parse.urljoin(f\"{year}/\", specifier)\n",
        "\n",
        "def get_info_from_specifier(file_name):\n",
        "    \"\"\"Extract year, month, zone/day, extension from file path.\"\"\"\n",
        "    match = re.search(r'[0-9]{4}.+AIS_([0-9]{4})_([0-9]{2})_(Zone)?([0-9]{2}|\\*)\\.(.+)', file_name)\n",
        "    if match:\n",
        "        year = match.group(1)\n",
        "        month = match.group(2)\n",
        "        zone_or_day = match.group(4)\n",
        "        extension = match.group(5)\n",
        "    else:\n",
        "        raise ValueError(f\"Cannot parse specifier from filename: {file_name}\")\n",
        "    return year, month, zone_or_day, extension\n",
        "\n",
        "def all_specifiers(zones, years, extension, dir=None):\n",
        "    \"\"\"Build list of all file specifiers and paths for the given zones and years.\"\"\"\n",
        "    specifiers = []\n",
        "    paths = []\n",
        "\n",
        "    for year in years:\n",
        "        if year in (2015, 2016, 2017):\n",
        "            for month in range(1, 13):\n",
        "                for zone in zones:\n",
        "                    spec = get_file_specifier(year, month, zone, extension)\n",
        "                    specifiers.append(spec)\n",
        "                    if dir:\n",
        "                        paths.append(os.path.join(dir, spec))\n",
        "        elif year >= 2018:\n",
        "            for dt in rrule.rrule(rrule.DAILY,\n",
        "                                  dtstart=datetime.strptime(f\"{year}-01-01\", \"%Y-%m-%d\"),\n",
        "                                  until=datetime.strptime(f\"{year}-12-31\", \"%Y-%m-%d\")):\n",
        "                spec = get_file_specifier(dt.year, dt.month, dt.day, extension)\n",
        "                specifiers.append(spec)\n",
        "                if dir:\n",
        "                    paths.append(os.path.join(dir, spec))\n",
        "\n",
        "    return {'specifiers': specifiers, 'paths': paths} if dir else {'specifiers': specifiers}\n",
        "\n",
        "# ------------------ CLEANING HELPERS ------------------ #\n",
        "\n",
        "def pd_append(values):\n",
        "    \"\"\"Append list of Series or scalars into a single pandas Series.\"\"\"\n",
        "    v1 = values[0]\n",
        "    if len(values) > 2:\n",
        "        rest = pd_append(values[1:])\n",
        "        series = pd.concat([pd.Series([v1]) if not isinstance(v1, pd.Series) else v1, rest])\n",
        "    elif len(values) == 2:\n",
        "        v2 = values[1]\n",
        "        series = pd.concat([\n",
        "            pd.Series([v1]) if not isinstance(v1, pd.Series) else v1,\n",
        "            pd.Series([v2]) if not isinstance(v2, pd.Series) else v2\n",
        "        ])\n",
        "    return series.reset_index(drop=True)\n",
        "\n",
        "def to_snake_case(name):\n",
        "    \"\"\"Convert camelCase or PascalCase to snake_case.\"\"\"\n",
        "    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
        "    name = re.sub('__([A-Z])', r'_\\1', name)\n",
        "    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n",
        "    return name.lower()\n",
        "\n",
        "# ------------------ FILE SYSTEM ------------------ #\n",
        "\n",
        "def clear_path(path):\n",
        "    \"\"\"Remove file or directory if it exists.\"\"\"\n",
        "    if os.path.exists(path):\n",
        "        if os.path.isfile(path):\n",
        "            os.remove(path)\n",
        "        else:\n",
        "            shutil.rmtree(path)\n",
        "\n",
        "# ------------------ TIME RANGE ------------------ #\n",
        "\n",
        "def get_min_max_times(specifier):\n",
        "    \"\"\"Return start and end datetime for a given file specifier.\"\"\"\n",
        "    year, month, zone_or_day, _ = get_info_from_specifier(specifier)\n",
        "    year = int(year)\n",
        "    month = int(month)\n",
        "\n",
        "    if year <= 2017:\n",
        "        min_time = pd.to_datetime(f\"{year}-{month}-01 00:00:00\")\n",
        "        last_day = monthrange(year, month)[1]\n",
        "        max_time = pd.to_datetime(f\"{year}-{month}-{last_day} 23:59:59\")\n",
        "    else:\n",
        "        day = int(zone_or_day)\n",
        "        min_time = pd.to_datetime(f\"{year}-{month}-{day} 00:00:00\")\n",
        "        max_time = pd.to_datetime(f\"{year}-{month}-{day} 23:59:59\")\n",
        "\n",
        "    return min_time, max_time\n"
      ],
      "metadata": {
        "id": "l-LABwD9iY4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python utils.py"
      ],
      "metadata": {
        "id": "yJYxnZrqidHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utm"
      ],
      "metadata": {
        "id": "aP-dzDExMKvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install haversine"
      ],
      "metadata": {
        "id": "Ha_8Qhn2kvUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define paths\n",
        "#   Adjust the path below to wherever you stored downloads_ny.zip in your Drive\n",
        "zip_path   = '/content/drive/MyDrive/downloads_ny.zip'\n",
        "target_dir = '/content/data/new_york'  # wherever you want it on Colab\n",
        "\n",
        "# 3. Make sure the target directory exists\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# 4. Copy the zip from Drive into your working folder\n",
        "shutil.copy(zip_path, target_dir)\n",
        "\n",
        "# 5. Unzip it in place\n",
        "!unzip -o \"{target_dir}/downloads_ny.zip\" -d \"{target_dir}\"\n"
      ],
      "metadata": {
        "id": "KpVKtIp2fAfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cleaner.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Cleaner pipeline using Dask for large-scale AIS data.\n",
        "\n",
        "Falls back to `fastparquet` if `pyarrow` is unavailable.\n",
        "\n",
        "Usage:\n",
        "    python cleaner.py <dataset_name> --test_fraction 0.3 --validation_fraction 0.1 --log_level 3\n",
        "Or run without flags and enter values interactively in Colab.\n",
        "\"\"\"\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "from haversine import haversine_vector, Unit\n",
        "\n",
        "#  Configure Dask for lower memory usage  #\n",
        "dask.config.set({\n",
        "    'dataframe.split_row_groups': True,\n",
        "    'dataframe.divisions': 'sorted',\n",
        "    'parquet.executor': False\n",
        "})\n",
        "\n",
        "#  Parquet engine detection  #\n",
        "try:\n",
        "    import pyarrow  # noqa: F401\n",
        "    PARQUET_ENGINE = 'pyarrow'\n",
        "except ImportError:\n",
        "    try:\n",
        "        import fastparquet  # noqa: F401\n",
        "        PARQUET_ENGINE = 'fastparquet'\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Install pyarrow or fastparquet to enable parquet I/O.\")\n",
        "\n",
        "import config\n",
        "import data_config\n",
        "from utils import clear_path, to_snake_case\n",
        "\n",
        "#  Helpers  #\n",
        "def clean_partition(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Per-partition cleaning: filtering, type fixes, track assignment.\"\"\"\n",
        "    df = df.rename(\n",
        "        columns={'BaseDateTime': 'BaseDatetime', 'TranscieverClass': 'TransceiverClass'}\n",
        "    )\n",
        "    df.columns = [to_snake_case(c) for c in df.columns]\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # derive vessel_group from vessel_type\n",
        "    if 'vessel_type' in df.columns:\n",
        "        replacement_dict = {VT: G.lower().replace('/', ' or ') for _, (G, VT, _) in config.types.iterrows()}\n",
        "        df['vessel_group'] = (\n",
        "            df['vessel_type']\n",
        "            .fillna(-1)\n",
        "            .astype(int)\n",
        "            .astype(str)\n",
        "            .replace(replacement_dict)\n",
        "        )\n",
        "\n",
        "    df['heading'] = df['heading'].replace(511, np.nan)\n",
        "    df['sog'] = df['sog'] + np.where(df['sog'] < 0, 102.6, 0)\n",
        "    df['cog'] = df['cog'] + np.where(df['cog'] < 0, 409.6, 0)\n",
        "\n",
        "    df = df[\n",
        "        (df['sog'] <= config.sog_cutoff)\n",
        "        & df['cog'].between(0, 360)\n",
        "        & (df['heading'].isna() | df['heading'].between(0, 360))\n",
        "    ]\n",
        "\n",
        "    drop_cols = {\n",
        "        'vessel_name', 'imo', 'call_sign',\n",
        "        'length', 'width', 'draft', 'cargo', 'transceiver_class'\n",
        "    }\n",
        "    df = df.drop(columns=drop_cols & set(df.columns), errors='ignore')\n",
        "\n",
        "    df['base_datetime'] = pd.to_datetime(df['base_datetime'])\n",
        "    df = df.sort_values(['mmsi', 'base_datetime'])\n",
        "\n",
        "    dlat = df['lat'].diff().abs()\n",
        "    dlon = df['lon'].diff().abs()\n",
        "    df = df[(dlat + dlon) > 0]\n",
        "\n",
        "    df['prev_lat'] = df['lat'].shift(1)\n",
        "    df['prev_lon'] = df['lon'].shift(1)\n",
        "    df['prev_time'] = df['base_datetime'].shift(1)\n",
        "    same = df['mmsi'] == df['mmsi'].shift(1)\n",
        "    df.loc[~same, ['prev_lat', 'prev_lon', 'prev_time']] = np.nan\n",
        "\n",
        "    pairs = df[['prev_lat', 'prev_lon', 'lat', 'lon']].dropna()\n",
        "    if not pairs.empty:\n",
        "        dist = haversine_vector(\n",
        "            pairs[['prev_lat', 'prev_lon']].values,\n",
        "            pairs[['lat', 'lon']].values,\n",
        "            Unit.NAUTICAL_MILES\n",
        "        )\n",
        "        delta_hours = (\n",
        "            df['base_datetime'] - df['prev_time']\n",
        "        ).dt.total_seconds() / 3600\n",
        "        df.loc[pairs.index, 'emp_speed'] = dist / delta_hours.loc[pairs.index]\n",
        "\n",
        "    df = df[df['emp_speed'].between(0.01, config.empirical_speed_cutoff)]\n",
        "    df = df.drop(columns=['prev_lat', 'prev_lon', 'prev_time', 'emp_speed'], errors='ignore')\n",
        "\n",
        "    gap = df['base_datetime'].diff().dt.total_seconds().fillna(0)\n",
        "    new_track = (\n",
        "        (df['mmsi'] != df['mmsi'].shift(1))\n",
        "        | (gap > config.new_trajectory_time_gap)\n",
        "    )\n",
        "    df['track'] = new_track.cumsum()\n",
        "\n",
        "    rng = df.groupby('track')['base_datetime'].agg(['min', 'max'])\n",
        "    valid_tracks = rng[\n",
        "        (rng['max'] - rng['min']).dt.total_seconds() > config.min_track_length\n",
        "    ].index\n",
        "    return df[df['track'].isin(valid_tracks)]\n",
        "\n",
        "\n",
        "def load_and_concat(path_or_list):\n",
        "    return dd.read_parquet(\n",
        "        path_or_list,\n",
        "        engine=PARQUET_ENGINE,\n",
        "        split_row_groups=True,\n",
        "        chunksize=\"64MB\"\n",
        "    )\n",
        "\n",
        "\n",
        "def month_split(files, frac):\n",
        "    rx = re.compile(r\".*_(\\d{4})_(\\d{2})_\\d+\\.parquet$\")\n",
        "    by_month = {}\n",
        "    for f in files:\n",
        "        m = rx.match(os.path.basename(f))\n",
        "        if m:\n",
        "            by_month.setdefault((m[1], m[2]), []).append(f)\n",
        "    months = sorted(by_month)\n",
        "    split_idx = int(len(months) * (1 - frac))\n",
        "    tm, tst = months[:split_idx], months[split_idx:]\n",
        "    train_files = [f for m in tm for f in by_month[m]]\n",
        "    test_files = [f for m in tst for f in by_month[m]]\n",
        "    return train_files, test_files\n",
        "\n",
        "\n",
        "def train_valid(ddf, val_frac):\n",
        "    \"\"\"Split tracks into training and validation sets.\"\"\"\n",
        "    unique_tracks = ddf['track'].unique().compute()\n",
        "    tracks = list(unique_tracks)\n",
        "    np.random.seed(0)\n",
        "    np.random.shuffle(tracks)\n",
        "    split_idx = int(len(tracks) * (1 - val_frac))\n",
        "    train_set = set(tracks[:split_idx])\n",
        "    valid_set = set(tracks[split_idx:])\n",
        "    return (\n",
        "        ddf[ddf['track'].isin(train_set)],\n",
        "        ddf[ddf['track'].isin(valid_set)]\n",
        "    )\n",
        "\n",
        "\n",
        "def save(ddf, out_dir, name):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    path = os.path.join(out_dir, f\"{name}.parquet\")\n",
        "    clear_path(path)\n",
        "    ddf.to_parquet(\n",
        "        path,\n",
        "        engine=PARQUET_ENGINE,\n",
        "        write_metadata_file=False,\n",
        "        compression='snappy'\n",
        "    )\n",
        "    print(f\" Saved {name} to {path}\")\n",
        "\n",
        "#  Main  #\n",
        "def main(dataset_name, test_frac, val_frac, loglvl):\n",
        "    config.set_log_level(loglvl)\n",
        "    if dataset_name not in data_config.datasets:\n",
        "        print(f\"Warning: '{dataset_name}' is not valid; using default.\")\n",
        "        dataset_name = list(data_config.datasets.keys())[0]\n",
        "    ds = data_config.datasets[dataset_name]\n",
        "\n",
        "    base = os.path.join(config.data_directory, ds.dataset_name)\n",
        "    filtered_dir = os.path.join(base, 'downloads', 'filtered')\n",
        "    cleaned_dir = os.path.join(base, 'cleaned')\n",
        "\n",
        "    files = sorted(glob.glob(os.path.join(filtered_dir, f\"{ds.dataset_name}_*.parquet\")))\n",
        "    train_files, test_files = month_split(files, test_frac)\n",
        "\n",
        "    print(\"Sampling one partition for metadata...\")\n",
        "    sample_df = load_and_concat(test_files[:1]).partitions[0].compute()\n",
        "    meta = clean_partition(sample_df)\n",
        "\n",
        "    print(f\"Cleaning TEST ({dataset_name})\")\n",
        "    test_ddf = load_and_concat(test_files)\n",
        "    test_clean = test_ddf.map_partitions(clean_partition, meta=meta)\n",
        "    save(test_clean, cleaned_dir, 'test')\n",
        "\n",
        "    print(f\"Cleaning TRAIN ({dataset_name})\")\n",
        "    train_ddf = load_and_concat(train_files)\n",
        "    train_clean = train_ddf.map_partitions(clean_partition, meta=meta)\n",
        "    train_set, valid_set = train_valid(train_clean, val_frac)\n",
        "    save(train_set, cleaned_dir, 'train')\n",
        "    save(valid_set, cleaned_dir, 'valid')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    sys.argv = [arg for arg in sys.argv if not arg.endswith('.json')]\n",
        "    parser = argparse.ArgumentParser(description='AIS Cleaner')\n",
        "    parser.add_argument('dataset_name', nargs='?', default=list(data_config.datasets.keys())[0],\n",
        "                        help=\"Which dataset to process\")\n",
        "    parser.add_argument('--test_fraction', type=float, default=None,\n",
        "                        help=\"Fraction for test split (e.g. 0.3)\")\n",
        "    parser.add_argument('--validation_fraction', type=float, default=None,\n",
        "                        help=\"Fraction for validation split (e.g. 0.1)\")\n",
        "    parser.add_argument('--log_level', type=int, choices=range(6), default=None,\n",
        "                        help=\"Log level (05)\")\n",
        "    args, _ = parser.parse_known_args()\n",
        "    if args.test_fraction is None:\n",
        "        args.test_fraction = float(input(\"Enter test_fraction: \"))\n",
        "    if args.validation_fraction is None:\n",
        "        args.validation_fraction = float(input(\"Enter validation_fraction: \"))\n",
        "    if args.log_level is None:\n",
        "        args.log_level = int(input(\"Enter log_level (05): \"))\n",
        "    main(\n",
        "        args.dataset_name,\n",
        "        args.test_fraction,\n",
        "        args.validation_fraction,\n",
        "        args.log_level\n",
        "    )\n"
      ],
      "metadata": {
        "id": "sCvHJocykZQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python cleaner.py new_york --test_fraction 0.33 --validation_fraction 0.2 --log_level 3"
      ],
      "metadata": {
        "id": "VKwvGYm8kqCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Zip the cleaned directory\n",
        "!zip -r /content/cleaned_ny.zip /content/data/new_york/cleaned\n",
        "\n",
        "# 2. Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. Copy the ZIP into your MyDrive\n",
        "!cp /content/cleaned_ny.zip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "9T_5iGk6bD69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Parquet part file\n",
        "df = pd.read_parquet(\"/content/data/new_york/cleaned/test.parquet/part.0.parquet\")\n",
        "\n",
        "# Show all columns for the first 5 rows\n",
        "print(df.head(5))\n"
      ],
      "metadata": {
        "id": "HlDy1Znczbd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the same Parquet file (or your full cleaned set)\n",
        "df = pd.read_parquet(\"/content/data/new_york/cleaned/test.parquet/part.0.parquet\")\n",
        "\n",
        "# 1) Are *all* headings NaN?\n",
        "all_nan = df['heading'].isna().all()\n",
        "print(\"All heading values NaN?\", all_nan)\n",
        "\n",
        "# 2) Count NaNs vs total rows\n",
        "nan_count   = df['heading'].isna().sum()\n",
        "total_count = len(df)\n",
        "print(f\"Heading NaNs: {nan_count} / {total_count} ({nan_count/total_count:.1%})\")\n",
        "\n",
        "# 3) If you want to see any valid headings:\n",
        "valid_headings = df['heading'].dropna().unique()\n",
        "print(\"Unique non-NaN headings:\", valid_headings)\n"
      ],
      "metadata": {
        "id": "tufNyN2n0cfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/data/new_york/interpolated_debug"
      ],
      "metadata": {
        "id": "4rTKjZLCZXff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile interpolator.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Interpolator pipeline using Dask for large-scale AIS data.\n",
        "Bootstraps metadata from the first partition to avoid Dask meta errors.\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import re\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Assuming these are available from the project structure\n",
        "import config\n",
        "import data_config\n",
        "from utils import clear_path\n",
        "\n",
        "# ------------------- Interpolation Config ------------------- #\n",
        "INTERP_FUNC = np.interp\n",
        "TIMESTAMP_COLUMN = 'base_datetime'\n",
        "COLUMNS_TO_INTERPOLATE = ['lat', 'lon', 'sog', 'cog', 'heading']\n",
        "COLUMNS_TO_USE_MOST_RECENT = []\n",
        "STABLE_COLUMNS = ['mmsi', 'vessel_group', 'status']\n",
        "COLUMNS_TO_CALCULATE = {'year': 'int16', 'month': 'byte'}\n",
        "\n",
        "# ------------------- File I/O ------------------- #\n",
        "PARQUET_ENGINE = 'pyarrow' if 'pyarrow' in sys.modules else 'fastparquet'\n",
        "\n",
        "def get_paths(dataset_name, debug=False):\n",
        "    \"\"\"Determines source and destination paths based on dataset and debug flag.\"\"\"\n",
        "    base = os.path.join(config.data_directory, dataset_name)\n",
        "    cleaned_dir = os.path.join(base, 'cleaned')\n",
        "    cleaned_dbg = os.path.join(base, 'cleaned_debug')\n",
        "    # prefer cleaned over cleaned_debug if cleaned_debug doesn't exist\n",
        "    if debug and os.path.isdir(cleaned_dbg):\n",
        "        src = cleaned_dbg\n",
        "    else:\n",
        "        src = cleaned_dir\n",
        "    dst = os.path.join(base, 'interpolated_debug' if debug else 'interpolated')\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    return src, dst\n",
        "\n",
        "# ------------------- Loading ------------------- #\n",
        "def load_datasets(src):\n",
        "    \"\"\"Loads cleaned datasets from a source directory into Dask DataFrames.\"\"\"\n",
        "    ds = {}\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        path = os.path.join(src, f\"{split}.parquet\")\n",
        "        ds[split] = dd.read_parquet(path, engine=PARQUET_ENGINE)\n",
        "        print(f\"Loaded {split}: {ds[split].shape[0].compute():,} rows\")\n",
        "    return ds\n",
        "\n",
        "# ------------------- Core interpolation ------------------- #\n",
        "def interpolate_track(track: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Interpolates a single track's data to a new time series.\n",
        "    The track DataFrame is the result of a pandas groupby operation.\n",
        "    \"\"\"\n",
        "    if track.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            TIMESTAMP_COLUMN, 'lat', 'lon', 'sog', 'cog', 'heading',\n",
        "            'mmsi', 'vessel_group', 'status', 'year', 'month'\n",
        "        ])\n",
        "\n",
        "    # Convert timestamp to seconds for interpolation\n",
        "    t0, t1 = track[TIMESTAMP_COLUMN].iloc[[0, -1]]\n",
        "    times = np.arange(t0, t1 + 1, config.interpolation_time_gap)\n",
        "\n",
        "    # Use interp1d to map original timestamps to new interpolated timestamps\n",
        "    idx_fn = interp1d(\n",
        "        track[TIMESTAMP_COLUMN], np.arange(len(track)),\n",
        "        kind='previous', assume_sorted=True,\n",
        "        bounds_error=False, fill_value=(0, len(track) - 1)\n",
        "    )\n",
        "    idx = idx_fn(times).astype(int)\n",
        "\n",
        "    out = {TIMESTAMP_COLUMN: times}\n",
        "    for col in track.columns:\n",
        "        # We handle 'track' column in the outer function, so we skip it here\n",
        "        if col == TIMESTAMP_COLUMN or col == 'track':\n",
        "            continue\n",
        "        elif col in COLUMNS_TO_INTERPOLATE:\n",
        "            out[col] = INTERP_FUNC(times, track[TIMESTAMP_COLUMN], track[col])\n",
        "        elif col in COLUMNS_TO_USE_MOST_RECENT:\n",
        "            out[col] = track[col].iloc[idx].values\n",
        "        elif col in STABLE_COLUMNS:\n",
        "            out[col] = np.repeat(track[col].iat[0], len(times))\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    # add year/month as separate columns\n",
        "    dt = pd.to_datetime(times * 1e9)\n",
        "    out['year'] = dt.year.astype('int16')\n",
        "    out['month'] = dt.month.astype('byte')\n",
        "\n",
        "    # Important: Do not return the 'track' column here. It will be part of the index.\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "\n",
        "def interpolate_partition(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Interpolates a single Dask partition by applying interpolation\n",
        "    to each track. This function is mapped over each partition of the Dask DataFrame.\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Group by 'track' and apply the interpolation function.\n",
        "    # This results in a MultiIndex with levels ['track', 'original_index'].\n",
        "    result = df.groupby('track').apply(interpolate_track)\n",
        "\n",
        "    # Reset the index, turning the MultiIndex levels into columns.\n",
        "    # This creates columns 'track' and 'level_1'.\n",
        "    result = result.reset_index()\n",
        "\n",
        "    # Drop the spurious 'level_1' column.\n",
        "    if 'level_1' in result.columns:\n",
        "        result = result.drop('level_1', axis=1)\n",
        "\n",
        "    # Set the 'track' column back as the index for the final result.\n",
        "    return result.set_index('track')\n",
        "\n",
        "# ------------------- Mapping ------------------- #\n",
        "def interpolate_datasets(ds: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Applies the interpolation logic to all Dask datasets (train, valid, test).\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for name, ddf in ds.items():\n",
        "        # convert timestamp to seconds\n",
        "        ddf = ddf.assign(base_datetime=(ddf['base_datetime'].astype('int64') // 10**9))\n",
        "        # bootstrap meta from first partition to avoid Dask meta errors\n",
        "        first = ddf.partitions[0].compute()\n",
        "        meta = interpolate_partition(first)\n",
        "        out[name] = ddf.map_partitions(interpolate_partition, meta=meta)\n",
        "    return out\n",
        "\n",
        "# ------------------- Saving ------------------- #\n",
        "def save_datasets(ds_out: dict, dst: str) -> None:\n",
        "    \"\"\"Saves the interpolated Dask DataFrames to parquet files.\"\"\"\n",
        "    clear_path(dst)\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    for name, ddf in ds_out.items():\n",
        "        path = os.path.join(dst, f\"{name}.parquet\")\n",
        "        counts = ddf.map_partitions(len, meta=('n', 'int64')).compute()\n",
        "        if counts.sum() == 0:\n",
        "            pd.DataFrame(columns=ddf._meta.columns).to_parquet(\n",
        "                path, engine=PARQUET_ENGINE, index=False\n",
        "            )\n",
        "        else:\n",
        "            ddf = ddf.clear_divisions()\n",
        "            if PARQUET_ENGINE == 'fastparquet':\n",
        "                ddf.to_parquet(path, engine=PARQUET_ENGINE, compute_divisions=False)\n",
        "            else:\n",
        "                ddf.to_parquet(path, engine=PARQUET_ENGINE, write_metadata_file=False)\n",
        "        print(f\"Saved {name} -> {path}\")\n",
        "\n",
        "# ------------------- Main ------------------- #\n",
        "def main(dataset_name: str, debug: bool, loglvl: int) -> None:\n",
        "    \"\"\"Main function to run the interpolation pipeline.\"\"\"\n",
        "    config.set_log_level(loglvl)\n",
        "    config.dataset_config = data_config.datasets[dataset_name]\n",
        "    src, dst = get_paths(dataset_name, debug)\n",
        "    ds = load_datasets(src)\n",
        "    interp = interpolate_datasets(ds)\n",
        "    save_datasets(interp, dst)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('dataset_name', choices=['new_york'])\n",
        "    p.add_argument('--debug', action='store_true')\n",
        "    p.add_argument('--log_level', type=int, default=2)\n",
        "    args = p.parse_args()\n",
        "\n",
        "    dask.config.set(\n",
        "        scheduler='single-threaded' if args.debug else 'processes'\n",
        "    )\n",
        "    main(args.dataset_name, args.debug, args.log_level)\n"
      ],
      "metadata": {
        "id": "L8D57GaTlec3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python interpolator.py new_york --debug --log_level 3\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZCVzsO9Wxhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Zip the interpolated_debug directory\n",
        "!zip -r /content/interpolated_debug.zip /content/data/new_york/interpolated_debug\n",
        "\n",
        "# 2. Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. Copy the ZIP into your MyDrive\n",
        "!cp /content/interpolated_debug.zip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "NvZn6PI7daEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 2. Load with pandas\n",
        "import pandas as pd\n",
        "\n",
        "# replace with your actual path\n",
        "path = \"/content/data/new_york/interpolated_debug/test.parquet/part.0.parquet\"\n",
        "\n",
        "# Read the whole file (or just the first N rows)\n",
        "df = pd.read_parquet(path, engine=\"pyarrow\")   # or engine=\"fastparquet\"\n",
        "print(df.shape)\n",
        "df.head()   # show first 5 rows\n",
        "\n",
        "# 3. Get a quick summary\n",
        "df.info()\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "Nhm2m0yEX_Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "933f1d2c"
      },
      "source": [
        "%%writefile sliding_window.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "SlidingWindow pipeline using Dask for large-scale AIS data.\n",
        "Splits long tracks into overlapping (train/valid) or nonoverlapping (test) subtracks.\n",
        "\"\"\"\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "\n",
        "import config\n",
        "import data_config\n",
        "from utils import clear_path\n",
        "\n",
        "#  Slidingwindow functions  #\n",
        "\n",
        "def window_track_long_term_train(track: pd.DataFrame, movement: int) -> pd.DataFrame:\n",
        "    \"\"\"Breaks a single track into overlapping windows for training/validation.\"\"\"\n",
        "    H = config.length_of_history\n",
        "    F = config.length_into_the_future\n",
        "    step = int(movement / config.interpolation_time_gap)\n",
        "    n = len(track) - (H + F)\n",
        "    idxs = [np.arange(i, i + H + F + 1) for i in range(0, n, step)]\n",
        "    if not idxs:  # Handle empty list of indices\n",
        "        return pd.DataFrame(columns=track.columns) # Return empty DataFrame with original columns\n",
        "    return pd.concat(\n",
        "        [track.iloc[ix].reset_index(drop=True) for ix in idxs],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "\n",
        "def window_track_long_term_test(track: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Breaks a single track into non-overlapping windows for testing.\"\"\"\n",
        "    H = config.length_of_history\n",
        "    F = config.length_into_the_future\n",
        "    n = len(track) - (H + F)\n",
        "    idxs = [np.arange(i, i + H + F + 1) for i in range(0, n, H)]\n",
        "    if not idxs: # Handle empty list of indices\n",
        "         return pd.DataFrame(columns=track.columns) # Return empty DataFrame with original columns\n",
        "    return pd.concat(\n",
        "        [track.iloc[ix].reset_index(drop=True) for ix in idxs],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "\n",
        "def window_partition(df: pd.DataFrame, fn, *args) -> pd.DataFrame:\n",
        "    \"\"\"Apply sliding-window fn per track, and explicitly build the output DataFrame.\"\"\"\n",
        "    results = []\n",
        "    # Iterate through groups and apply the windowing function\n",
        "    for track_id, group_df in df.groupby('track'):\n",
        "        windowed_df = fn(group_df, *args)\n",
        "        if not windowed_df.empty:\n",
        "            # Add 'track' as a column to the windowed DataFrame\n",
        "            windowed_df['track'] = track_id\n",
        "            results.append(windowed_df)\n",
        "\n",
        "    if not results:\n",
        "        # Return an empty DataFrame with expected columns if no windows were created\n",
        "        # We need to infer the columns from the original DataFrame and the added 'track'\n",
        "        # This might be tricky with Dask metadata, but for a partition, we can attempt\n",
        "        # to get columns from the input or a sample. Let's use input columns + 'track'.\n",
        "        cols = list(df.columns) + ['track']\n",
        "        # Assuming the data types for the original columns are preserved,\n",
        "        # and 'track' is the same dtype as the group key.\n",
        "        # This is a simplified meta; might need refinement based on actual data types.\n",
        "        # For now, let's return an empty DataFrame that *should* be compatible.\n",
        "        # This might require more robust metadata handling for production use with Dask.\n",
        "        # However, for debugging the 'level_1' error, let's see if this structure helps.\n",
        "        return pd.DataFrame(columns=cols).set_index('track')\n",
        "\n",
        "\n",
        "    # Concatenate the results from all tracks in this partition\n",
        "    partition_result_df = pd.concat(results, ignore_index=True)\n",
        "\n",
        "    # Set 'track' as the index\n",
        "    partition_result_df = partition_result_df.set_index('track')\n",
        "\n",
        "    # The index name might be None after set_index if the original 'track' column had no name.\n",
        "    # Ensure the index is named 'track'\n",
        "    partition_result_df.index.name = 'track'\n",
        "\n",
        "\n",
        "    # Drop any residual 'level_0' or 'level_1' that might have been implicitly created\n",
        "    # during the concatenation or index operations, though the manual construction\n",
        "    # should ideally prevent 'level_1' conflicts from reset_index.\n",
        "    partition_result_df = partition_result_df.drop(columns=['level_0', 'level_1'], errors='ignore')\n",
        "\n",
        "\n",
        "    return partition_result_df\n",
        "\n",
        "\n",
        "#  I/O helpers  #\n",
        "\n",
        "def get_paths(dataset_name: str, debug: bool = False):\n",
        "    \"\"\"Determine source and destination directories for input/output.\"\"\"\n",
        "    base = os.path.join(config.data_directory, dataset_name)\n",
        "    src_dbg = os.path.join(base, 'interpolated_debug')\n",
        "    src_std = os.path.join(base, 'interpolated')\n",
        "    if debug and os.path.isdir(src_dbg):\n",
        "        src = src_dbg\n",
        "    elif os.path.isdir(src_std):\n",
        "        src = src_std\n",
        "    elif os.path.isdir(src_dbg):\n",
        "        src = src_dbg\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Neither '{src_std}' nor '{src_dbg}' exists\")\n",
        "    dst = os.path.join(base, 'windowed_debug' if debug else 'windowed')\n",
        "    os.makedirs(dst, exist_ok=True)\n",
        "    return src, dst\n",
        "\n",
        "\n",
        "def load_split(src: str, split: str) -> dd.DataFrame:\n",
        "    \"\"\"Load a split (train/valid/test) from parquet and ensure track index.\"\"\"\n",
        "    path = os.path.join(src, f\"{split}.parquet\")\n",
        "    ddf = dd.read_parquet(path)\n",
        "    if ddf.index.name != 'track':\n",
        "        ddf = ddf.map_partitions(\n",
        "            lambda df: df.set_index(pd.Index(df.index, name='track')),\n",
        "            meta=ddf._meta\n",
        "        )\n",
        "    return ddf\n",
        "\n",
        "\n",
        "def save_split(ddf: dd.DataFrame, dst: str, name: str):\n",
        "    \"\"\"Write a windowed split to parquet with clear logging.\"\"\"\n",
        "    out = os.path.join(dst, f\"{name}.parquet\")\n",
        "    clear_path(out)\n",
        "    ddf.to_parquet(out, schema='infer')\n",
        "    print(f\" Saved {name} to {out}\")\n",
        "\n",
        "#  Main  #\n",
        "\n",
        "def main(dataset_name: str, debug: bool, loglvl: int):\n",
        "    config.set_log_level(loglvl)\n",
        "    ds_cfg = data_config.datasets[dataset_name]\n",
        "\n",
        "    src, dst = get_paths(dataset_name, debug)\n",
        "    splits = {\n",
        "        'train': load_split(src, 'train'),\n",
        "        'valid': load_split(src, 'valid'),\n",
        "        'test':  load_split(src, 'test'),\n",
        "    }\n",
        "\n",
        "    outputs = {}\n",
        "    # Overlapping windows for train & valid\n",
        "    for split in ('train', 'valid'):\n",
        "        outputs[f\"{split}_long_term_train\"] = splits[split].map_partitions(\n",
        "            window_partition,\n",
        "            window_track_long_term_train,\n",
        "            ds_cfg.sliding_window_movement,\n",
        "            meta=splits[split]._meta\n",
        "        )\n",
        "    # Non-overlapping windows for test & valid\n",
        "    for split in ('test', 'valid'):\n",
        "        outputs[f\"{split}_long_term_test\"] = splits[split].map_partitions(\n",
        "            window_partition,\n",
        "            window_track_long_term_test,\n",
        "            meta=splits[split]._meta\n",
        "        )\n",
        "\n",
        "    for name, ddf in outputs.items():\n",
        "        save_split(ddf, dst, name)\n",
        "\n",
        "# Check if running in an interactive environment (like Colab)\n",
        "def is_interactive():\n",
        "    import __main__ as main\n",
        "    return not hasattr(main, '__file__')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # In interactive mode, directly call main with hardcoded values from the !python command\n",
        "    main('new_york', debug=True, loglvl=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dask.dataframe as dd\n",
        "import pandas as pd # Import pandas to create metadata DataFrame\n",
        "import shutil # Import shutil for removing directories\n",
        "from utils import clear_path\n",
        "\n",
        "# Import necessary functions from the sliding_window script\n",
        "from sliding_window import (\n",
        "    get_paths,\n",
        "    load_split,\n",
        "    window_partition,\n",
        "    window_track_long_term_train,\n",
        "    window_track_long_term_test,\n",
        "    save_split\n",
        ")\n",
        "\n",
        "# Import config and data_config\n",
        "import config\n",
        "import data_config\n",
        "\n",
        "# Hardcoded values from the previous execution attempt\n",
        "dataset_name = 'new_york'\n",
        "debug = True\n",
        "loglvl = 3\n",
        "\n",
        "# Set log level\n",
        "config.set_log_level(loglvl)\n",
        "\n",
        "# Get source and destination paths\n",
        "src, dst = get_paths(dataset_name, debug)\n",
        "\n",
        "# Load the interpolated data splits\n",
        "splits = {\n",
        "    'train': load_split(src, 'train'),\n",
        "    'valid': load_split(src, 'valid'),\n",
        "    'test':  load_split(src, 'test'),\n",
        "}\n",
        "\n",
        "outputs = {}\n",
        "ds_cfg = data_config.datasets[dataset_name]\n",
        "\n",
        "# --- Explicitly define metadata for map_partitions ---\n",
        "# Get columns and dtypes from one of the input splits\n",
        "# Assuming all splits have the same column structure\n",
        "input_meta = splits['train']._meta\n",
        "\n",
        "# Define the expected columns and dtypes after window_partition\n",
        "# This should include the original columns (excluding the index if it was not a column)\n",
        "# and potentially 'track' as the index.\n",
        "# The original columns in the interpolated data were:\n",
        "# 'base_datetime', 'mmsi', 'lat', 'lon', 'sog', 'cog', 'heading', 'status', 'year', 'month', 'track' (index)\n",
        "# The window_partition function is expected to return a DataFrame with 'track' as index\n",
        "# and these columns as data columns.\n",
        "# We need to create a pandas DataFrame metadata object that reflects this structure.\n",
        "\n",
        "# Get data columns from input meta (excluding the index)\n",
        "data_cols_meta = input_meta.reset_index().drop(columns=['track', 'level_0', 'level_1'], errors='ignore')\n",
        "\n",
        "# Create a new metadata DataFrame with 'track' as the index and the data columns\n",
        "# The dtype of 'track' should match the original index dtype\n",
        "track_dtype = input_meta.index.dtype\n",
        "\n",
        "# Create an empty pandas DataFrame with the desired column structure and dtypes\n",
        "# The index will be named 'track' with the correct dtype\n",
        "# The columns will be the data columns with their original dtypes\n",
        "expected_columns = data_cols_meta.columns\n",
        "expected_dtypes = data_cols_meta.dtypes\n",
        "\n",
        "# Create an empty DataFrame with the correct structure\n",
        "output_meta_dict = {col: pd.Series(dtype=dtype) for col, dtype in zip(expected_columns, expected_dtypes)}\n",
        "output_meta = pd.DataFrame(output_meta_dict)\n",
        "output_meta.index = pd.Index([], dtype=track_dtype, name='track')\n",
        "\n",
        "# Ensure 'base_datetime' is treated as object/int64 before conversion back to datetime if needed later\n",
        "if 'base_datetime' in output_meta.columns:\n",
        "    output_meta['base_datetime'] = output_meta['base_datetime'].astype('int64')\n",
        "\n",
        "\n",
        "# Apply overlapping windows for train & valid\n",
        "for split in ('train', 'valid'):\n",
        "    outputs[f\"{split}_long_term_train\"] = splits[split].map_partitions(\n",
        "        window_partition,\n",
        "        window_track_long_term_train,\n",
        "        ds_cfg.sliding_window_movement,\n",
        "        meta=output_meta # Pass explicit metadata\n",
        "    )\n",
        "\n",
        "# Apply non-overlapping windows for test & valid\n",
        "for split in ('test', 'valid'):\n",
        "    outputs[f\"{split}_long_term_test\"] = splits[split].map_partitions(\n",
        "        window_partition,\n",
        "        window_track_long_term_test,\n",
        "        meta=output_meta # Pass explicit metadata\n",
        "    )\n",
        "\n",
        "# Save the windowed datasets\n",
        "# Modified save_split function with checks\n",
        "def save_split_with_checks(ddf: dd.DataFrame, dst: str, name: str):\n",
        "    \"\"\"Write a windowed split to parquet with clear logging and checks.\"\"\"\n",
        "    out = os.path.join(dst, f\"{name}.parquet\")\n",
        "    clear_path(out)\n",
        "    print(f\"Attempting to save {name} to {out}\")\n",
        "    try:\n",
        "        ddf.to_parquet(out, schema='infer')\n",
        "        print(f\"Save operation for {out} completed.\")\n",
        "\n",
        "        # Add checks after saving\n",
        "        print(f\"Checking saved location: {out}\")\n",
        "        if not os.path.exists(out):\n",
        "            print(f\"Error: Output directory {out} was not created after saving.\")\n",
        "            raise FileNotFoundError(f\"Output directory {out} was not created after saving.\")\n",
        "        if not os.path.isdir(out):\n",
        "             print(f\"Error: Output path {out} is not a directory after saving.\")\n",
        "             raise NotADirectoryError(f\"Output path {out} is not a directory after saving.\")\n",
        "\n",
        "        output_dir_contents = os.listdir(out)\n",
        "        print(f\"Contents of output directory {out}: {output_dir_contents}\")\n",
        "\n",
        "        parquet_files_found = any(f.endswith('.parquet') for f in output_dir_contents)\n",
        "        if not parquet_files_found:\n",
        "             print(f\"Error: Output directory {out} is empty or does not contain .parquet files after saving.\")\n",
        "             raise FileNotFoundError(f\"Output directory {out} is empty or does not contain .parquet files after saving.\")\n",
        "        print(f\"Found .parquet files in output directory {out}.\")\n",
        "\n",
        "        print(f\" Saved {name} successfully to {out}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during save or check of output location {out}: {e}\")\n",
        "        # Clean up the incomplete output directory\n",
        "        if os.path.exists(out):\n",
        "             print(f\"Cleaning up incomplete output directory: {out}\")\n",
        "             if os.path.isdir(out):\n",
        "                 shutil.rmtree(out)\n",
        "             else:\n",
        "                 os.remove(out)\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "\n",
        "for name, ddf in outputs.items():\n",
        "    save_split_with_checks(ddf, dst, name)\n",
        "\n",
        "print(\"Sliding window processing complete.\")"
      ],
      "metadata": {
        "id": "pjHwQeSb2qiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 2. Load with pandas\n",
        "import pandas as pd\n",
        "\n",
        "# replace with your actual path\n",
        "path = \"/content/data/new_york/windowed_debug/test_long_term_test.parquet\"\n",
        "\n",
        "# Read the whole file (or just the first N rows)\n",
        "df = pd.read_parquet(path, engine=\"pyarrow\")   # or engine=\"fastparquet\"\n",
        "print(df.shape)\n",
        "df.head()   # show first 5 rows\n",
        "\n",
        "# 3. Get a quick summary\n",
        "df.info()\n",
        "df.describe()\n"
      ],
      "metadata": {
        "id": "v6PeDTaQhpec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Zip the windowed_debug directory\n",
        "!zip -r /content/windowed_debug.zip /content/data/new_york/windowed_debug\n",
        "\n",
        "# 2. Mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. Copy the ZIP into your MyDrive\n",
        "!cp /content/windowed_debug.zip /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "5zFK6YzCxBgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade dask-ml scikit-learn dask pandas pyarrow fastparquet"
      ],
      "metadata": {
        "id": "PljtBX911spP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile formatter.py\n",
        "# formatter.py\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import shutil\n",
        "from typing import Optional, Any\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "import json # Added for saving all_categories\n",
        "\n",
        "# Assuming config and utils are in the same directory or accessible via PYTHONPATH\n",
        "import config\n",
        "from data_config import datasets\n",
        "import utils\n",
        "\n",
        "# Initialize OneHotEncoder type hint to Any if dask_ml is not available\n",
        "OneHotEncoderType = Any\n",
        "try:\n",
        "    from dask_ml.preprocessing import OneHotEncoder\n",
        "    use_ml = True\n",
        "    OneHotEncoderType = OneHotEncoder\n",
        "except ImportError:\n",
        "    use_ml = False\n",
        "    logging.warning(\"dask-ml not found. Falling back to dd.get_dummies, which may lead to \"\n",
        "                    \"inconsistent feature sets if categories vary across data splits. \"\n",
        "                    \"It is highly recommended to install dask-ml for robust one-hot encoding.\")\n",
        "\n",
        "\n",
        "engine = None\n",
        "try:\n",
        "    import pyarrow  # noqa: F401\n",
        "    engine = 'pyarrow'\n",
        "except ImportError:\n",
        "    try:\n",
        "        import fastparquet  # noqa: F401\n",
        "        engine = 'fastparquet'\n",
        "    except ImportError:\n",
        "        sys.exit(\"Missing dependency 'pyarrow' or 'fastparquet'.\")\n",
        "\n",
        "\n",
        "def setup_logging(save_log: bool, level: int):\n",
        "    fmt = '%(asctime)s %(levelname)s %(message)s'\n",
        "    if save_log:\n",
        "        logging.basicConfig(filename='format.log', level=level, format=fmt)\n",
        "    else:\n",
        "        logging.basicConfig(level=level, format=fmt)\n",
        "\n",
        "\n",
        "def repartition_if_needed(df: dd.DataFrame, name: str, base: str) -> dd.DataFrame:\n",
        "    tmp = os.path.join(base, f'.tmp_{name}.parquet')\n",
        "    path = os.path.join(base, f'{name}.parquet')\n",
        "\n",
        "    if os.path.exists(tmp):\n",
        "        shutil.rmtree(tmp) if os.path.isdir(tmp) else os.remove(tmp)\n",
        "\n",
        "    original_ddf = dd.read_parquet(path, engine=engine)\n",
        "    df = original_ddf.repartition(partition_size='4000MB', force=True)\n",
        "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
        "    dd.to_parquet(df, tmp, engine=engine, schema='infer', write_metadata_file=True)\n",
        "    return dd.read_parquet(tmp, engine=engine)\n",
        "\n",
        "\n",
        "def load_windowed(base: str, names: list, engine: str) -> dict:\n",
        "    out = {}\n",
        "    for n in names:\n",
        "        original_path = os.path.join(base, f'{n}.parquet')\n",
        "        raw = dd.read_parquet(original_path, engine=engine)\n",
        "        out[n] = raw\n",
        "    return out\n",
        "\n",
        "\n",
        "def one_hot_encode(df: dd.DataFrame, categorical_columns: list,\n",
        "                   fitted_encoder: Optional[OneHotEncoderType] = None,\n",
        "                   all_known_categories: Optional[dict] = None) -> (dd.DataFrame, Optional[OneHotEncoderType]):\n",
        "    cols_to_encode = [c for c in categorical_columns if c in df.columns]\n",
        "\n",
        "    if not cols_to_encode:\n",
        "        return df, fitted_encoder\n",
        "\n",
        "    if all_known_categories:\n",
        "        logging.info(f\"Applying global known categories to {cols_to_encode} for this DataFrame.\")\n",
        "        for col in cols_to_encode:\n",
        "            if col in all_known_categories:\n",
        "                df[col] = df[col].astype(pd.CategoricalDtype(categories=all_known_categories[col]))\n",
        "            else:\n",
        "                logging.warning(f\"Categorical column '{col}' missing from 'all_known_categories'. \"\n",
        "                                \"Categories will be inferred locally for this column.\")\n",
        "                df[col] = df[col].astype('category') # Fallback to infer locally\n",
        "    else:\n",
        "        logging.warning(\"No global known categories provided. Inferring categories locally. \"\n",
        "                        \"This may lead to inconsistent features if categories differ across splits.\")\n",
        "        df = df.categorize(columns=cols_to_encode)\n",
        "\n",
        "    if use_ml:\n",
        "        if fitted_encoder is None:\n",
        "            logging.info(f\"Fitting OneHotEncoder on categorical columns: {cols_to_encode}\")\n",
        "            fitted_encoder = OneHotEncoder(sparse_output=False)\n",
        "            fitted_encoder.fit(df[cols_to_encode])\n",
        "\n",
        "        transformed_df = fitted_encoder.transform(df[cols_to_encode])\n",
        "\n",
        "        df = df.drop(cols_to_encode, axis=1)\n",
        "        df = dd.concat([df, transformed_df], axis=1)\n",
        "\n",
        "    else: # Fallback when dask_ml is not available, using dd.get_dummies\n",
        "        logging.warning(\"Using dd.get_dummies without a pre-fitted encoder. \"\n",
        "                        \"Feature consistency relies on all_known_categories being pre-applied and fillna(0).\")\n",
        "        df = dd.get_dummies(df, columns=cols_to_encode, dtype='bool')\n",
        "\n",
        "    df = df.map_partitions(lambda pdf: pdf.apply(pd.to_numeric, errors='coerce').astype('float32'))\n",
        "    return df, fitted_encoder\n",
        "\n",
        "\n",
        "def reshape_partition(part: pd.DataFrame, ts: int) -> np.ndarray:\n",
        "    part = part.replace('undefined', np.nan)\n",
        "    part = part.apply(pd.to_numeric, errors='coerce').astype('float32')\n",
        "\n",
        "    if part.empty:\n",
        "        return np.empty((0, ts, part.shape[1]), dtype='float32')\n",
        "\n",
        "    arr = part.to_numpy()\n",
        "    num_windows = len(arr) // ts\n",
        "    if num_windows == 0:\n",
        "        return np.empty((0, ts, part.shape[1]), dtype='float32')\n",
        "\n",
        "    rows = [np.arange(i * ts, (i + 1) * ts) for i in range(num_windows)]\n",
        "\n",
        "    return np.stack([arr[r] for r in rows])\n",
        "\n",
        "\n",
        "def format_and_save(dfs: dict, out: str, conserve: bool = True, engine: str = None, feature_cols_map: dict = None):\n",
        "    utils.clear_path(out)\n",
        "    os.makedirs(out, exist_ok=True)\n",
        "\n",
        "    if not dfs or all(ddf.npartitions == 0 for ddf in dfs.values()):\n",
        "        # Ensure features.csv is still created, even if empty, for DataLoader to not fail\n",
        "        pd.DataFrame(columns=['FeatureName', 'dtype', 'IsTarget']).to_csv(os.path.join(out, 'features.csv'), index=False)\n",
        "        logging.warning(\"No dataframes or all dataframes are empty. Exiting format_and_save.\")\n",
        "        return\n",
        "\n",
        "    if feature_cols_map is None:\n",
        "        logging.error(\"feature_cols_map is None. Cannot ensure consistent columns for saving NumPy arrays. Exiting.\")\n",
        "        pd.DataFrame(columns=['FeatureName', 'dtype', 'IsTarget']).to_csv(os.path.join(out, 'features.csv'), index=False)\n",
        "        return\n",
        "\n",
        "    # Determine latitude and longitude indices (using 'lat' and 'lon' as per data)\n",
        "    lat_idx = feature_cols_map.get('lat')\n",
        "    lon_idx = feature_cols_map.get('lon')\n",
        "\n",
        "    if lat_idx is None or lon_idx is None:\n",
        "        logging.error(\"Latitude or Longitude column (named 'lat' or 'lon') not found in feature_cols_map. Cannot generate Y data correctly. Exiting.\")\n",
        "        pd.DataFrame(columns=['FeatureName', 'dtype', 'IsTarget']).to_csv(os.path.join(out, 'features.csv'), index=False)\n",
        "        return\n",
        "\n",
        "    # Define the indices for the Y data (latitude, longitude)\n",
        "    y_feature_indices = [lat_idx, lon_idx] # This maintains the order [lat, lon]\n",
        "\n",
        "    print(f\"\\n--- DEBUG: Y Feature Indices for Target ---\")\n",
        "    print(f\"  'lat' found at index: {lat_idx}\")\n",
        "    print(f\"  'lon' found at index: {lon_idx}\")\n",
        "    print(f\"  Y data will be sliced using indices: {y_feature_indices}\")\n",
        "    print(f\"  Corresponding feature names: {[list(feature_cols_map.keys())[i] for i in y_feature_indices]}\")\n",
        "    print(f\"-------------------------------------------\\n\")\n",
        "\n",
        "\n",
        "    # Store feature names for X and Y in features.csv\n",
        "    feats_df_data = []\n",
        "    for col_name, idx in feature_cols_map.items():\n",
        "        is_target = col_name in ['lat', 'lon']\n",
        "        feats_df_data.append({'FeatureName': col_name, 'dtype': 'float32', 'IsTarget': is_target})\n",
        "\n",
        "    feats_df = pd.DataFrame(feats_df_data)\n",
        "    # Sort by the original order to ensure consistent indexing in the CSV\n",
        "    feats_df['Order'] = feats_df['FeatureName'].map(feature_cols_map)\n",
        "    feats_df = feats_df.sort_values(by='Order').drop(columns=['Order'])\n",
        "\n",
        "    feats_df.to_csv(os.path.join(out, 'features.csv'), index=False)\n",
        "    logging.info(f\"Saved features metadata to {os.path.join(out, 'features.csv')}\")\n",
        "\n",
        "    tsf = config.length_of_history + config.length_into_the_future + 1\n",
        "\n",
        "    for nm, ddf in dfs.items():\n",
        "        sd = os.path.join(out, nm)\n",
        "        os.makedirs(sd, exist_ok=True)\n",
        "        logging.info(f\"Processing dataset: {nm}\")\n",
        "\n",
        "        if ddf.npartitions == 0:\n",
        "            logging.warning(f\"Dataset {nm} has no partitions. Creating empty directories.\")\n",
        "            for gap in config.time_gaps:\n",
        "                mins = int(gap / 60)\n",
        "                os.makedirs(os.path.join(sd, f'{mins}_min_time_gap_x'), exist_ok=True)\n",
        "                os.makedirs(os.path.join(sd, f'{mins}_min_time_gap_y'), exist_ok=True)\n",
        "            continue\n",
        "\n",
        "        logging.info(f\"Aligning columns for {nm} to master feature set.\")\n",
        "        aligned_ddf = ddf[list(feature_cols_map.keys())].fillna(0.0)\n",
        "\n",
        "        for gap in config.time_gaps:\n",
        "            step = int(gap / config.interpolation_time_gap)\n",
        "            xi = np.arange(0, config.length_of_history)\n",
        "            yi = np.arange(config.length_of_history, tsf, step)\n",
        "            xi = xi[xi < tsf]; yi = yi[yi < tsf]\n",
        "\n",
        "            mins = int(gap / 60)\n",
        "            xdir = os.path.join(sd, f'{mins}_min_time_gap_x'); os.makedirs(xdir, exist_ok=True)\n",
        "            ydir = os.path.join(sd, f'{mins}_min_time_gap_y'); os.makedirs(ydir, exist_ok=True)\n",
        "            logging.info(f\"    Processing time gap {mins} min for {nm}\")\n",
        "\n",
        "            if conserve:\n",
        "                delayed_saves = []\n",
        "                for i in range(aligned_ddf.npartitions):\n",
        "                    pdf = aligned_ddf.partitions[i].compute()\n",
        "                    arr = reshape_partition(pdf, tsf)\n",
        "\n",
        "                    x_arr = arr[:, xi, :] # Keep all features for X\n",
        "                    # Corrected for IndexError: shape mismatch\n",
        "                    y_arr = arr[:, yi][:, :, y_feature_indices] # Select lat/lon for Y\n",
        "\n",
        "                    xp, yp = os.path.join(xdir, f'{i}.npy'), os.path.join(ydir, f'{i}.npy')\n",
        "\n",
        "                    def save_arrays(xa, ya, xp, yp):\n",
        "                        np.save(xp, xa); np.save(yp, ya)\n",
        "                        return xa.shape[0], ya.shape[0]\n",
        "\n",
        "                    delayed_saves.append(dask.delayed(save_arrays)(x_arr, y_arr, xp, yp))\n",
        "\n",
        "                dask.compute(*delayed_saves)\n",
        "                logging.info(f\"    Saved {len(delayed_saves)} partitions for {nm} and {mins} min gap (conserve mode).\")\n",
        "            else:\n",
        "                full = aligned_ddf.compute()\n",
        "                arr = reshape_partition(full, tsf)\n",
        "                x_arr = arr[:, xi, :] # Keep all features for X\n",
        "                # Corrected for IndexError: shape mismatch\n",
        "                y_arr = arr[:, yi][:, :, y_feature_indices] # Select lat/lon for Y\n",
        "\n",
        "                np.save(os.path.join(sd, f'{mins}_min_time_gap_x.npy'), x_arr)\n",
        "                np.save(os.path.join(sd, f'{mins}_min_time_gap_y.npy'), y_arr)\n",
        "                logging.info(f\"    Saved full arrays for {nm} and {mins} min gap (non-conserve mode).\")\n",
        "                del full; gc.collect()\n",
        "\n",
        "    print(\"Formatting process complete.\")\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('dataset_name', choices=datasets.keys())\n",
        "    parser.add_argument('-l','--log_level', type=int, default=2, choices=[0,1,2,3,4])\n",
        "    parser.add_argument('-s','--save_log', action='store_true')\n",
        "    parser.add_argument('--debug', action='store_true')\n",
        "\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    if unknown:\n",
        "        logging.warning(f\"Found unknown arguments: {unknown}. These will be ignored.\")\n",
        "        logging.warning(\"If these include a kernel JSON path, ensure you're calling the script correctly.\")\n",
        "\n",
        "\n",
        "    config.set_log_level(args.log_level)\n",
        "    setup_logging(args.save_log, args.log_level)\n",
        "\n",
        "    base = f\"/content/data/{args.dataset_name}\"\n",
        "    wdir = os.path.join(base, 'windowed' + ('_debug' if args.debug else ''))\n",
        "    odir = os.path.join(base, 'formatted' + ('_debug' if args.debug else ''))\n",
        "    names = [\n",
        "        'train_long_term_train',\n",
        "        'test_long_term_test',\n",
        "        'valid_long_term_train',\n",
        "        'valid_long_term_test'\n",
        "    ]\n",
        "\n",
        "    dfs = load_windowed(wdir, names, engine)\n",
        "\n",
        "    fitted_ohe = None\n",
        "    feature_columns_after_ohe = None\n",
        "\n",
        "    # --- Step 1: Collect ALL unique categories from ALL data splits ---\n",
        "    all_categories = {}\n",
        "    logging.info(\"Collecting global unique categories for categorical columns across all datasets.\")\n",
        "    for col in config.categorical_columns:\n",
        "        if any(col in ddf.columns for ddf in dfs.values()):\n",
        "            combined_series_list = [ddf[col] for ddf in dfs.values() if col in ddf.columns]\n",
        "\n",
        "            combined_series = dd.concat(combined_series_list)\n",
        "\n",
        "            unique_cats = combined_series.astype('category').cat.as_known().cat.categories\n",
        "\n",
        "            all_categories[col] = list(unique_cats)\n",
        "    logging.info(f\"Collected global categories: {all_categories}\")\n",
        "\n",
        "    # 2. Process training data first to fit the OneHotEncoder\n",
        "    train_name = 'train_long_term_train'\n",
        "    if train_name in dfs:\n",
        "        logging.info(f\"Processing training data ({train_name}) to fit OneHotEncoder.\")\n",
        "        dfs[train_name], fitted_ohe = one_hot_encode(\n",
        "            dfs[train_name], config.categorical_columns, fitted_ohe,\n",
        "            all_known_categories=all_categories\n",
        "        )\n",
        "        feature_columns_after_ohe = dfs[train_name].columns.tolist()\n",
        "        logging.info(f\"Training data columns after OHE: {feature_columns_after_ohe}\")\n",
        "    else:\n",
        "        logging.error(f\"Training data '{train_name}' not found. Cannot fit OneHotEncoder.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "    # 3. Process all other datasets using the fitted encoder\n",
        "    for n in names:\n",
        "        if n == train_name:\n",
        "            continue\n",
        "        if n in dfs:\n",
        "            logging.info(f\"Processing data split: {n} (transforming using fitted encoder).\")\n",
        "            dfs[n], _ = one_hot_encode(\n",
        "                dfs[n], config.categorical_columns, fitted_ohe,\n",
        "                all_known_categories=all_categories\n",
        "            )\n",
        "            logging.info(f\"    Columns for {n} after OHE: {dfs[n].columns.tolist()}\")\n",
        "        else:\n",
        "            logging.warning(f\"Data split '{n}' not found. Skipping.\")\n",
        "\n",
        "\n",
        "    # 4. Pass the master feature columns to format_and_save\n",
        "    if feature_columns_after_ohe is None:\n",
        "        logging.error(\"No master feature columns defined after processing training data. Exiting.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    master_feature_cols_map = {col: i for i, col in enumerate(feature_columns_after_ohe)}\n",
        "\n",
        "    format_and_save(dfs, odir, conserve=True, engine=engine, feature_cols_map=master_feature_cols_map)\n",
        "\n",
        "    # --- MOVED: Save all_categories for consistent one-hot encoding during prediction ---\n",
        "    all_categories_path = os.path.join(odir, 'all_categories.json')\n",
        "    with open(all_categories_path, 'w') as f:\n",
        "        json.dump(all_categories, f, indent=4)\n",
        "    logging.info(f\"Saved all_categories to {all_categories_path}\")\n",
        "    # --- END MOVED SECTION ---\n",
        "\n",
        "    logging.info(\"Formatter script finished successfully.\")\n"
      ],
      "metadata": {
        "id": "YmXGMO5dKRf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python formatter.py new_york --debug -l 3 -s"
      ],
      "metadata": {
        "id": "Rj8HvCwELBMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# adjust this path to wherever your formatted files live\n",
        "x = np.load('/content/data/new_york/formatted_debug/test_long_term_test/30_min_time_gap_x/0.npy')\n",
        "y = np.load('/content/data/new_york/formatted_debug/test_long_term_test/30_min_time_gap_y/0.npy')\n",
        "\n",
        "print(\"X shape:\", x.shape)\n",
        "print(\"Y shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "v9km2-XO23rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define source folder\n",
        "src = \"/content/data/new_york/formatted_debug\"\n",
        "\n",
        "# 3. Define output .zip path in MyDrive\n",
        "zip_path = \"/content/drive/MyDrive/formatted_debug.zip\"\n",
        "\n",
        "# 4. Create the zip\n",
        "print(f\"Zipping {src}  {zip_path}\")\n",
        "shutil.make_archive(base_name=zip_path.replace('.zip',''),\n",
        "                    format='zip',\n",
        "                    root_dir=src)\n",
        "\n",
        "print(\"Done!  Youll find formatted_debug.zip in the root of your MyDrive.\")"
      ],
      "metadata": {
        "id": "6vq-rrrPVmBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # grant Colab access to your Drive\n",
        "\n",
        "# then copy the file into the working directory\n",
        "!cp \"/content/drive/MyDrive/formatted_debug.zip\" .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q6SEnpZ7a0F",
        "outputId": "ffcaae5b-5c0b-43c1-bd3f-8100116713ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/data/new_york/formatted_debug"
      ],
      "metadata": {
        "id": "ZSxJb28l9lAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip formatted_debug.zip -d formatted_debug\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5d7zMRm8T3x",
        "outputId": "f4e46a30-0270-44aa-874f-60daf9547bcb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  formatted_debug.zip\n",
            "   creating: formatted_debug/test_long_term_test/\n",
            "   creating: formatted_debug/train_long_term_train/\n",
            "   creating: formatted_debug/valid_long_term_test/\n",
            "   creating: formatted_debug/valid_long_term_train/\n",
            "  inflating: formatted_debug/features.csv  \n",
            "  inflating: formatted_debug/all_categories.json  \n",
            "   creating: formatted_debug/test_long_term_test/30_min_time_gap_x/\n",
            "   creating: formatted_debug/test_long_term_test/30_min_time_gap_y/\n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/53.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/62.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/50.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/88.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/68.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/22.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/99.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/66.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/4.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/3.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/48.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/41.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/26.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/121.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/89.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/111.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/59.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/1.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/101.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/86.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/90.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/51.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/93.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/76.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/103.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/112.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/64.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/104.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/2.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/119.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/85.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/118.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/82.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/9.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/23.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/80.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/12.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/96.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/5.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/69.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/83.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/97.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/74.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/10.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/37.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/110.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/15.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/47.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/63.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/8.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/7.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/92.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/114.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/115.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/98.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/102.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/25.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/55.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/35.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/0.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/77.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/73.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/107.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/16.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/100.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/6.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/106.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/58.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/38.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/87.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/36.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/34.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/70.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/42.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/67.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/49.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/20.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/18.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/13.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/113.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/24.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/78.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/57.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/11.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/27.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/95.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/46.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/71.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/108.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/17.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/61.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/33.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/32.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/21.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/28.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/14.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/84.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/43.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/109.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/65.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/40.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/116.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/120.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/31.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/105.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/56.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/60.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/75.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/72.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/81.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/91.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/39.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/54.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/30.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/52.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/94.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/117.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/79.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/29.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/44.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/19.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_x/45.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/53.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/62.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/50.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/88.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/68.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/22.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/99.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/66.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/4.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/3.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/48.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/41.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/26.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/121.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/89.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/111.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/59.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/1.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/101.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/86.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/90.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/51.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/93.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/76.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/103.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/112.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/64.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/104.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/2.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/119.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/85.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/118.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/82.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/9.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/23.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/80.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/12.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/96.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/5.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/69.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/83.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/97.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/74.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/10.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/37.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/110.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/15.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/47.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/63.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/8.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/7.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/92.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/114.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/115.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/98.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/102.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/25.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/55.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/35.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/0.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/77.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/73.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/107.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/16.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/100.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/6.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/106.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/58.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/38.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/87.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/36.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/34.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/70.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/42.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/67.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/49.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/20.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/18.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/13.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/113.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/24.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/78.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/57.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/11.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/27.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/95.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/46.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/71.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/108.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/17.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/61.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/33.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/32.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/21.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/28.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/14.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/84.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/43.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/109.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/65.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/40.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/116.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/120.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/31.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/105.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/56.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/60.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/75.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/72.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/81.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/91.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/39.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/54.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/30.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/52.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/94.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/117.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/79.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/29.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/44.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/19.npy  \n",
            "  inflating: formatted_debug/test_long_term_test/30_min_time_gap_y/45.npy  \n",
            "   creating: formatted_debug/train_long_term_train/30_min_time_gap_x/\n",
            "   creating: formatted_debug/train_long_term_train/30_min_time_gap_y/\n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/53.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/222.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/158.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/62.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/127.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/50.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/235.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/201.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/224.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/156.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/159.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/236.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/195.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/88.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/153.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/68.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/233.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/22.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/200.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/186.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/166.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/99.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/150.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/66.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/4.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/3.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/48.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/41.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/26.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/121.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/149.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/144.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/89.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/111.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/241.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/238.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/179.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/59.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/193.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/126.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/1.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/101.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/135.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/86.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/90.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/189.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/154.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/51.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/93.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/76.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/103.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/168.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/137.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/223.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/112.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/205.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/232.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/64.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/104.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/2.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/176.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/171.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/128.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/191.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/119.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/190.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/204.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/85.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/118.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/134.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/82.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/161.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/9.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/198.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/23.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/173.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/80.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/12.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/96.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/194.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/5.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/141.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/211.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/69.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/136.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/228.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/182.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/83.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/97.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/74.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/157.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/239.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/10.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/169.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/37.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/110.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/15.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/148.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/122.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/47.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/178.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/63.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/162.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/203.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/8.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/7.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/92.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/114.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/115.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/98.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/140.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/221.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/206.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/167.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/102.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/218.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/25.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/230.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/212.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/55.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/35.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/0.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/240.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/160.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/124.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/227.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/142.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/77.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/73.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/107.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/181.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/123.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/16.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/180.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/100.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/6.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/170.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/196.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/106.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/219.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/165.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/58.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/234.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/38.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/220.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/164.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/87.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/155.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/36.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/34.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/174.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/146.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/131.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/70.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/229.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/216.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/42.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/67.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/132.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/172.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/130.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/202.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/49.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/20.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/18.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/13.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/113.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/24.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/78.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/129.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/57.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/187.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/11.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/27.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/95.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/46.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/71.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/210.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/108.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/145.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/17.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/61.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/209.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/33.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/138.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/163.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/237.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/32.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/213.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/21.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/28.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/175.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/133.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/14.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/177.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/226.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/188.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/84.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/43.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/109.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/65.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/207.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/40.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/116.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/197.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/120.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/225.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/199.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/31.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/105.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/139.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/143.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/152.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/125.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/183.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/151.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/56.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/217.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/192.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/184.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/60.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/75.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/72.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/81.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/208.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/91.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/39.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/242.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/147.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/185.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/54.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/214.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/30.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/52.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/94.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/117.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/79.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/231.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/29.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/44.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/215.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/19.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_x/45.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/53.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/222.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/158.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/62.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/127.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/50.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/235.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/201.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/224.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/156.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/159.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/236.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/195.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/88.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/153.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/68.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/233.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/22.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/200.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/186.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/166.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/99.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/150.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/66.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/4.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/3.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/48.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/41.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/26.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/121.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/149.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/144.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/89.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/111.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/241.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/238.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/179.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/59.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/193.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/126.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/1.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/101.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/135.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/86.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/90.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/189.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/154.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/51.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/93.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/76.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/103.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/168.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/137.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/223.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/112.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/205.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/232.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/64.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/104.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/2.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/176.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/171.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/128.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/191.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/119.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/190.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/204.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/85.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/118.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/134.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/82.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/161.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/9.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/198.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/23.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/173.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/80.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/12.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/96.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/194.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/5.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/141.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/211.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/69.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/136.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/228.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/182.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/83.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/97.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/74.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/157.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/239.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/10.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/169.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/37.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/110.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/15.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/148.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/122.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/47.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/178.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/63.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/162.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/203.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/8.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/7.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/92.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/114.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/115.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/98.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/140.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/221.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/206.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/167.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/102.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/218.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/25.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/230.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/212.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/55.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/35.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/0.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/240.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/160.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/124.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/227.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/142.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/77.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/73.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/107.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/181.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/123.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/16.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/180.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/100.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/6.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/170.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/196.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/106.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/219.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/165.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/58.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/234.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/38.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/220.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/164.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/87.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/155.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/36.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/34.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/174.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/146.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/131.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/70.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/229.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/216.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/42.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/67.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/132.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/172.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/130.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/202.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/49.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/20.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/18.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/13.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/113.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/24.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/78.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/129.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/57.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/187.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/11.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/27.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/95.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/46.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/71.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/210.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/108.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/145.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/17.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/61.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/209.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/33.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/138.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/163.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/237.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/32.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/213.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/21.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/28.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/175.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/133.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/14.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/177.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/226.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/188.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/84.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/43.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/109.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/65.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/207.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/40.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/116.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/197.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/120.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/225.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/199.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/31.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/105.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/139.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/143.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/152.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/125.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/183.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/151.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/56.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/217.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/192.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/184.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/60.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/75.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/72.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/81.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/208.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/91.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/39.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/242.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/147.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/185.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/54.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/214.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/30.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/52.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/94.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/117.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/79.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/231.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/29.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/44.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/215.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/19.npy  \n",
            "  inflating: formatted_debug/train_long_term_train/30_min_time_gap_y/45.npy  \n",
            "   creating: formatted_debug/valid_long_term_train/30_min_time_gap_x/\n",
            "   creating: formatted_debug/valid_long_term_train/30_min_time_gap_y/\n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/53.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/222.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/158.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/62.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/127.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/50.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/235.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/201.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/224.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/156.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/159.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/236.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/195.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/88.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/153.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/68.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/233.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/22.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/200.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/186.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/166.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/99.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/150.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/66.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/4.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/3.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/48.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/41.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/26.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/121.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/149.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/144.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/89.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/111.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/241.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/238.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/179.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/59.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/193.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/126.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/1.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/101.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/135.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/86.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/90.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/189.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/154.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/51.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/93.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/76.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/103.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/168.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/137.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/223.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/112.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/205.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/232.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/64.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/104.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/2.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/176.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/171.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/128.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/191.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/119.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/190.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/204.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/85.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/118.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/134.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/82.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/161.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/9.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/198.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/23.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/173.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/80.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/12.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/96.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/194.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/5.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/141.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/211.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/69.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/136.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/228.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/182.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/83.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/97.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/74.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/157.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/239.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/10.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/169.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/37.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/110.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/15.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/148.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/122.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/47.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/178.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/63.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/162.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/203.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/8.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/7.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/92.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/114.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/115.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/98.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/140.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/221.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/206.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/167.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/102.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/218.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/25.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/230.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/212.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/55.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/35.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/0.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/240.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/160.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/124.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/227.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/142.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/77.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/73.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/107.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/181.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/123.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/16.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/180.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/100.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/6.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/170.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/196.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/106.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/219.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/165.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/58.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/234.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/38.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/220.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/164.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/87.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/155.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/36.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/34.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/174.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/146.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/131.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/70.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/229.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/216.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/42.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/67.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/132.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/172.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/130.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/202.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/49.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/20.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/18.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/13.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/113.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/24.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/78.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/129.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/57.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/187.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/11.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/27.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/95.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/46.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/71.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/210.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/108.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/145.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/17.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/61.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/209.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/33.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/138.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/163.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/237.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/32.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/213.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/21.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/28.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/175.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/133.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/14.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/177.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/226.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/188.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/84.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/43.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/109.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/65.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/207.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/40.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/116.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/197.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/120.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/225.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/199.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/31.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/105.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/139.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/143.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/152.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/125.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/183.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/151.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/56.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/217.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/192.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/184.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/60.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/75.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/72.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/81.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/208.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/91.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/39.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/242.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/147.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/185.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/54.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/214.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/30.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/52.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/94.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/117.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/79.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/231.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/29.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/44.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/215.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/19.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_x/45.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/53.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/222.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/158.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/62.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/127.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/50.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/235.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/201.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/224.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/156.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/159.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/236.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/195.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/88.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/153.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/68.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/233.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/22.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/200.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/186.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/166.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/99.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/150.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/66.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/4.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/3.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/48.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/41.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/26.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/121.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/149.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/144.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/89.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/111.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/241.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/238.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/179.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/59.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/193.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/126.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/1.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/101.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/135.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/86.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/90.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/189.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/154.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/51.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/93.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/76.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/103.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/168.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/137.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/223.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/112.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/205.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/232.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/64.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/104.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/2.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/176.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/171.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/128.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/191.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/119.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/190.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/204.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/85.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/118.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/134.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/82.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/161.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/9.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/198.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/23.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/173.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/80.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/12.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/96.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/194.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/5.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/141.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/211.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/69.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/136.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/228.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/182.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/83.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/97.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/74.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/157.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/239.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/10.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/169.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/37.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/110.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/15.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/148.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/122.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/47.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/178.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/63.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/162.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/203.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/8.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/7.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/92.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/114.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/115.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/98.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/140.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/221.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/206.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/167.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/102.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/218.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/25.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/230.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/212.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/55.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/35.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/0.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/240.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/160.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/124.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/227.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/142.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/77.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/73.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/107.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/181.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/123.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/16.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/180.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/100.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/6.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/170.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/196.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/106.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/219.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/165.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/58.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/234.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/38.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/220.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/164.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/87.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/155.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/36.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/34.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/174.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/146.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/131.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/70.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/229.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/216.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/42.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/67.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/132.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/172.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/130.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/202.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/49.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/20.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/18.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/13.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/113.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/24.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/78.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/129.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/57.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/187.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/11.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/27.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/95.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/46.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/71.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/210.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/108.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/145.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/17.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/61.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/209.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/33.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/138.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/163.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/237.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/32.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/213.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/21.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/28.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/175.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/133.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/14.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/177.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/226.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/188.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/84.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/43.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/109.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/65.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/207.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/40.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/116.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/197.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/120.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/225.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/199.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/31.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/105.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/139.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/143.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/152.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/125.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/183.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/151.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/56.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/217.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/192.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/184.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/60.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/75.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/72.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/81.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/208.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/91.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/39.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/242.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/147.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/185.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/54.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/214.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/30.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/52.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/94.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/117.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/79.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/231.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/29.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/44.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/215.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/19.npy  \n",
            "  inflating: formatted_debug/valid_long_term_train/30_min_time_gap_y/45.npy  \n",
            "   creating: formatted_debug/valid_long_term_test/30_min_time_gap_x/\n",
            "   creating: formatted_debug/valid_long_term_test/30_min_time_gap_y/\n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/53.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/222.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/158.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/62.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/127.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/50.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/235.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/201.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/224.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/156.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/159.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/236.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/195.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/88.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/153.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/68.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/233.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/22.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/200.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/186.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/166.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/99.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/150.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/66.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/4.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/3.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/48.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/41.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/26.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/121.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/149.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/144.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/89.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/111.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/241.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/238.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/179.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/59.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/193.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/126.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/1.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/101.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/135.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/86.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/90.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/189.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/154.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/51.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/93.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/76.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/103.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/168.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/137.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/223.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/112.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/205.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/232.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/64.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/104.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/2.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/176.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/171.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/128.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/191.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/119.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/190.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/204.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/85.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/118.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/134.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/82.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/161.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/9.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/198.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/23.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/173.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/80.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/12.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/96.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/194.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/5.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/141.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/211.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/69.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/136.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/228.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/182.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/83.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/97.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/74.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/157.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/239.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/10.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/169.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/37.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/110.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/15.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/148.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/122.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/47.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/178.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/63.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/162.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/203.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/8.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/7.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/92.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/114.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/115.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/98.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/140.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/221.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/206.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/167.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/102.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/218.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/25.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/230.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/212.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/55.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/35.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/0.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/240.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/160.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/124.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/227.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/142.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/77.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/73.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/107.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/181.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/123.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/16.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/180.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/100.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/6.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/170.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/196.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/106.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/219.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/165.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/58.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/234.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/38.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/220.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/164.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/87.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/155.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/36.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/34.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/174.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/146.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/131.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/70.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/229.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/216.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/42.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/67.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/132.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/172.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/130.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/202.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/49.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/20.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/18.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/13.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/113.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/24.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/78.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/129.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/57.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/187.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/11.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/27.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/95.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/46.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/71.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/210.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/108.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/145.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/17.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/61.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/209.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/33.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/138.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/163.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/237.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/32.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/213.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/21.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/28.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/175.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/133.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/14.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/177.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/226.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/188.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/84.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/43.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/109.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/65.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/207.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/40.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/116.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/197.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/120.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/225.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/199.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/31.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/105.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/139.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/143.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/152.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/125.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/183.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/151.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/56.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/217.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/192.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/184.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/60.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/75.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/72.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/81.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/208.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/91.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/39.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/242.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/147.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/185.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/54.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/214.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/30.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/52.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/94.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/117.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/79.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/231.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/29.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/44.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/215.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/19.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_x/45.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/53.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/222.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/158.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/62.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/127.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/50.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/235.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/201.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/224.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/156.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/159.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/236.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/195.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/88.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/153.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/68.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/233.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/22.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/200.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/186.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/166.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/99.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/150.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/66.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/4.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/3.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/48.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/41.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/26.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/121.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/149.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/144.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/89.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/111.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/241.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/238.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/179.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/59.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/193.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/126.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/1.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/101.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/135.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/86.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/90.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/189.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/154.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/51.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/93.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/76.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/103.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/168.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/137.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/223.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/112.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/205.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/232.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/64.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/104.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/2.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/176.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/171.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/128.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/191.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/119.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/190.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/204.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/85.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/118.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/134.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/82.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/161.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/9.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/198.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/23.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/173.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/80.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/12.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/96.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/194.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/5.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/141.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/211.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/69.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/136.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/228.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/182.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/83.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/97.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/74.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/157.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/239.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/10.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/169.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/37.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/110.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/15.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/148.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/122.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/47.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/178.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/63.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/162.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/203.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/8.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/7.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/92.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/114.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/115.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/98.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/140.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/221.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/206.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/167.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/102.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/218.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/25.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/230.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/212.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/55.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/35.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/0.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/240.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/160.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/124.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/227.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/142.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/77.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/73.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/107.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/181.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/123.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/16.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/180.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/100.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/6.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/170.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/196.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/106.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/219.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/165.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/58.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/234.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/38.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/220.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/164.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/87.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/155.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/36.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/34.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/174.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/146.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/131.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/70.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/229.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/216.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/42.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/67.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/132.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/172.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/130.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/202.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/49.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/20.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/18.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/13.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/113.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/24.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/78.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/129.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/57.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/187.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/11.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/27.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/95.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/46.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/71.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/210.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/108.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/145.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/17.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/61.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/209.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/33.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/138.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/163.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/237.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/32.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/213.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/21.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/28.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/175.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/133.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/14.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/177.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/226.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/188.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/84.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/43.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/109.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/65.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/207.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/40.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/116.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/197.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/120.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/225.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/199.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/31.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/105.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/139.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/143.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/152.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/125.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/183.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/151.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/56.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/217.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/192.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/184.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/60.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/75.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/72.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/81.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/208.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/91.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/39.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/242.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/147.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/185.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/54.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/214.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/30.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/52.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/94.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/117.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/79.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/231.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/29.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/44.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/215.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/19.npy  \n",
            "  inflating: formatted_debug/valid_long_term_test/30_min_time_gap_y/45.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Path to your .npy file\n",
        "path = \"/content/data/new_york/formatted_debug/test_long_term_test/30_min_time_gap_x/0.npy\"\n",
        "\n",
        "# Load the array\n",
        "arr = np.load(path, allow_pickle=True)   # set allow_pickle=True only if you know it contains pickled objects\n",
        "\n",
        "# Inspect shape and dtype\n",
        "print(\"Array shape:\", arr.shape)\n",
        "print(\"Array dtype:\", arr.dtype)\n",
        "\n",
        "# Peek at the first few entries\n",
        "print(\"First 5 elements:\\n\", arr[:5])\n"
      ],
      "metadata": {
        "id": "tQPUhSH7lKu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/data/new_york/formatted_debug"
      ],
      "metadata": {
        "id": "FP8ULt7INoF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your feature definitions\n",
        "features = pd.read_csv('/content/data/new_york/formatted_debug/features.csv')\n",
        "features.head(13)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NLcIoOATHKVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile loading_utils.py\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Set relevant random seeds\n",
        "\n",
        "    :param seed:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "def clear_path(path):\n",
        "    \"\"\"\n",
        "    Delete any files or directories from a path\n",
        "\n",
        "    :param path: Path to remove\n",
        "    :type path: str\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    if os.path.exists(path):\n",
        "        if os.path.isfile(path):\n",
        "            os.remove(path)\n",
        "        else:\n",
        "            shutil.rmtree(path)\n",
        "\n",
        "def total_system_ram():\n",
        "    \"\"\"\n",
        "    Get total system ram, in bytes\n",
        "\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    return os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')"
      ],
      "metadata": {
        "id": "ZLsZB2iUBdt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python loading_utils.py"
      ],
      "metadata": {
        "id": "QLaQALqescb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile normalizer.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Any, List, Optional\n",
        "\n",
        "class Normalizer:\n",
        "    \"\"\"\n",
        "    Handles data normalization (e.g., Min-Max scaling).\n",
        "    Can calculate factors from a DataFrame and apply normalization to new data.\n",
        "    \"\"\"\n",
        "    def __init__(self, method: str = 'min_max'):\n",
        "        if method not in ['min_max']: # Can extend to 'z_score' if needed\n",
        "            raise ValueError(f\"Normalization method '{method}' not supported.\")\n",
        "        self.method = method\n",
        "        self.normalization_factors: Optional[Dict[str, Dict[str, float]]] = None\n",
        "\n",
        "    def get_normalization_factors(self, df: pd.DataFrame, features: List[str]) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Calculates normalization factors (min/max) for specified features from a DataFrame.\n",
        "        This method is typically called only on the training data.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): The DataFrame to calculate factors from.\n",
        "            features (List[str]): List of column names (features) to calculate factors for.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Dict[str, float]]: A dictionary where keys are feature names and values\n",
        "                                         are dictionaries containing 'min' and 'max' values.\n",
        "        \"\"\"\n",
        "        factors = {}\n",
        "        for feature in features:\n",
        "            if feature in df.columns and pd.api.types.is_numeric_dtype(df[feature]):\n",
        "                min_val = df[feature].min()\n",
        "                max_val = df[feature].max()\n",
        "                if max_val == min_val: # Avoid division by zero\n",
        "                    min_val = min_val - 1e-6\n",
        "                    max_val = max_val + 1e-6\n",
        "                factors[feature] = {'min': float(min_val), 'max': float(max_val)}\n",
        "            else:\n",
        "                print(f\"Warning: Feature '{feature}' not found or not numeric in DataFrame for factor calculation.\")\n",
        "        self.normalization_factors = factors\n",
        "        return factors\n",
        "\n",
        "    def normalize_data(self, data: np.ndarray, normalization_factors: Dict[str, Dict[str, float]],\n",
        "                       feature_mapping: Dict[str, int]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Applies normalization to a numpy array using provided normalization factors.\n",
        "        Assumes `data` is 3D (batch, sequence_length, features).\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): The numpy array to normalize.\n",
        "            normalization_factors (Dict[str, Dict[str, float]]): Factors to use for normalization.\n",
        "            feature_mapping (Dict[str, int]): A dictionary mapping feature names to their\n",
        "                                              index in the last dimension of the numpy array.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The normalized numpy array.\n",
        "        \"\"\"\n",
        "        if not normalization_factors:\n",
        "            print(\"Warning: No normalization factors provided. Returning data unnormalized.\")\n",
        "            return data\n",
        "\n",
        "        normalized_data = data.copy()\n",
        "\n",
        "        for feature_name, factors in normalization_factors.items():\n",
        "            if feature_name in feature_mapping:\n",
        "                col_idx = feature_mapping[feature_name]\n",
        "                min_val = factors['min']\n",
        "                max_val = factors['max']\n",
        "\n",
        "                if self.method == 'min_max':\n",
        "                    # Apply Min-Max scaling: normalized = (original - min) / (max - min)\n",
        "                    # Handle the case where max_val == min_val to prevent division by zero\n",
        "                    if (max_val - min_val) != 0:\n",
        "                        normalized_data[:, :, col_idx] = (normalized_data[:, :, col_idx] - min_val) / (max_val - min_val)\n",
        "                    else:\n",
        "                        # If min == max, set to 0.5 (midpoint) or 0 if a specific behavior is desired\n",
        "                        normalized_data[:, :, col_idx] = 0.5 # or 0.0 depending on convention\n",
        "            else:\n",
        "                # This could happen if a factor exists for a feature not present in the current 'data' array\n",
        "                print(f\"Warning: Feature '{feature_name}' from normalization factors not found in provided data mapping. Skipping.\")\n",
        "        return normalized_data\n",
        "\n",
        "    def unnormalize_data(self, data: np.ndarray, normalization_factors: Dict[str, Dict[str, float]],\n",
        "                         feature_mapping: Dict[str, int]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Applies inverse normalization (un-normalizes) to a numpy array.\n",
        "        Assumes `data` is 3D (batch, sequence_length, features).\n",
        "\n",
        "        Args:\n",
        "            data (np.ndarray): The numpy array to unnormalize.\n",
        "            normalization_factors (Dict[str, Dict[str, float]]): Factors to use for unnormalization.\n",
        "            feature_mapping (Dict[str, int]): A dictionary mapping feature names to their\n",
        "                                              index in the last dimension of the numpy array.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The unnormalized numpy array.\n",
        "        \"\"\"\n",
        "        if not normalization_factors:\n",
        "            print(\"Warning: No normalization factors provided. Returning data as is.\")\n",
        "            return data\n",
        "\n",
        "        unnormalized_data = data.copy()\n",
        "\n",
        "        for feature_name, factors in normalization_factors.items():\n",
        "            if feature_name in feature_mapping:\n",
        "                col_idx = feature_mapping[feature_name]\n",
        "                min_val = factors['min']\n",
        "                max_val = factors['max']\n",
        "\n",
        "                if self.method == 'min_max':\n",
        "                    # Apply inverse Min-Max scaling: original = normalized * (max - min) + min\n",
        "                    unnormalized_data[:, :, col_idx] = unnormalized_data[:, :, col_idx] * (max_val - min_val) + min_val\n",
        "            else:\n",
        "                # This could happen if a factor exists for a feature not present in the current 'data' array\n",
        "                pass\n",
        "        return unnormalized_data\n",
        "\n",
        "# Self-test for Normalizer\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running self-test for normalizer.py...\")\n",
        "\n",
        "    # Create dummy data\n",
        "    data = np.random.rand(10, 5, 3) # 10 samples, 5 timesteps, 3 features\n",
        "\n",
        "    # Simulate features.csv column names and their mapping to indices\n",
        "    # This is what formatter.py's features.csv would tell us.\n",
        "    feature_names = ['feature_A', 'feature_B', 'feature_C']\n",
        "    feature_mapping = {name: idx for idx, name in enumerate(feature_names)}\n",
        "\n",
        "    # Create a DataFrame for factor calculation (only training data should be used for this)\n",
        "    # Let's assume a simplified 2D DataFrame for this, representing the 'raw' features before windowing\n",
        "    df_for_factors = pd.DataFrame({\n",
        "        'feature_A': np.random.uniform(10, 20, 100),\n",
        "        'feature_B': np.random.uniform(0, 1, 100),\n",
        "        'feature_C': np.random.uniform(-5, 5, 100),\n",
        "        'categorical_feature': ['X', 'Y', 'Z'] * 33 + ['X']\n",
        "    })\n",
        "\n",
        "    normalizer = Normalizer(method='min_max')\n",
        "\n",
        "    # Test factor calculation\n",
        "    print(\"\\nTesting factor calculation...\")\n",
        "    numeric_features_for_factors = ['feature_A', 'feature_B', 'feature_C']\n",
        "    factors = normalizer.get_normalization_factors(df_for_factors, numeric_features_for_factors)\n",
        "    print(\"Calculated Factors:\", factors)\n",
        "\n",
        "    assert 'feature_A' in factors and 'feature_B' in factors and 'feature_C' in factors\n",
        "    assert factors['feature_A']['min'] >= 10 and factors['feature_A']['max'] <= 20\n",
        "    assert factors['feature_B']['min'] >= 0 and factors['feature_B']['max'] <= 1\n",
        "    assert factors['feature_C']['min'] >= -5 and factors['feature_C']['max'] <= 5\n",
        "\n",
        "    # Test normalization\n",
        "    print(\"\\nTesting normalization...\")\n",
        "    original_data = np.array([\n",
        "        [[15.0, 0.5, 0.0], [12.0, 0.2, -3.0]], # Sample 1\n",
        "        [[18.0, 0.8, 4.0], [10.0, 0.0, -5.0]]  # Sample 2\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    # Make a 3D array with matching feature columns for test\n",
        "    # This simulates the X array from formatter.py, with known values for `feature_A`, `feature_B`, `feature_C`\n",
        "    test_X_data = np.zeros((2, 2, len(feature_names)), dtype=np.float32)\n",
        "    test_X_data[0,0, feature_mapping['feature_A']] = 15.0\n",
        "    test_X_data[0,0, feature_mapping['feature_B']] = 0.5\n",
        "    test_X_data[0,0, feature_mapping['feature_C']] = 0.0\n",
        "    test_X_data[0,1, feature_mapping['feature_A']] = 12.0\n",
        "    test_X_data[0,1, feature_mapping['feature_B']] = 0.2\n",
        "    test_X_data[0,1, feature_mapping['feature_C']] = -3.0\n",
        "\n",
        "    test_X_data[1,0, feature_mapping['feature_A']] = 18.0\n",
        "    test_X_data[1,0, feature_mapping['feature_B']] = 0.8\n",
        "    test_X_data[1,0, feature_mapping['feature_C']] = 4.0\n",
        "    test_X_data[1,1, feature_mapping['feature_A']] = 10.0\n",
        "    test_X_data[1,1, feature_mapping['feature_B']] = 0.0\n",
        "    test_X_data[1,1, feature_mapping['feature_C']] = -5.0\n",
        "\n",
        "    # Example factors to use for normalization\n",
        "    example_factors = {\n",
        "        'feature_A': {'min': 10.0, 'max': 20.0},\n",
        "        'feature_B': {'min': 0.0, 'max': 1.0},\n",
        "        'feature_C': {'min': -5.0, 'max': 5.0},\n",
        "    }\n",
        "\n",
        "    normalized_data = normalizer.normalize_data(test_X_data, example_factors, feature_mapping)\n",
        "    print(\"Normalized Data (first 2 samples, 2 timesteps):\")\n",
        "    print(normalized_data[:, :2, :]) # Print first 2 timesteps and all features\n",
        "\n",
        "    # Verify values are between 0 and 1\n",
        "    assert np.all((normalized_data >= -1e-6) & (normalized_data <= 1 + 1e-6))\n",
        "\n",
        "    # Test unnormalization\n",
        "    print(\"\\nTesting unnormalization...\")\n",
        "    unnormalized_data = normalizer.unnormalize_data(normalized_data, example_factors, feature_mapping)\n",
        "    print(\"Unnormalized Data (first 2 samples, 2 timesteps):\")\n",
        "    print(unnormalized_data[:, :2, :])\n",
        "\n",
        "    # Verify unnormalized data is close to original_data\n",
        "    assert np.allclose(unnormalized_data[:, :, feature_mapping['feature_A']], test_X_data[:, :, feature_mapping['feature_A']], atol=1e-5)\n",
        "    assert np.allclose(unnormalized_data[:, :, feature_mapping['feature_B']], test_X_data[:, :, feature_mapping['feature_B']], atol=1e-5)\n",
        "    assert np.allclose(unnormalized_data[:, :, feature_mapping['feature_C']], test_X_data[:, :, feature_mapping['feature_C']], atol=1e-5)\n",
        "\n",
        "    print(\"\\nNormalizer self-test PASSED!\")"
      ],
      "metadata": {
        "id": "OX5OnH4jtiIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e4eb5d-97f1-4193-c920-73e3ba889c5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing normalizer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python normalizer.py"
      ],
      "metadata": {
        "id": "WID6j_uyyTnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile loading.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "\n",
        "def read_ts_data(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reads time series data from a parquet file.\n",
        "    Placeholder function. In a real scenario, this might handle errors,\n",
        "    or convert specific column types.\n",
        "    \"\"\"\n",
        "    print(f\"Reading data from {path}\")\n",
        "    try:\n",
        "        return pd.read_parquet(path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading parquet from {path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Helper function for Haversine distance\n",
        "def _haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate the Haversine distance between two points on Earth (specified in decimal degrees).\n",
        "    Returns distance in meters.\n",
        "    \"\"\"\n",
        "    R = 6371000  # Radius of Earth in meters\n",
        "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
        "    delta_phi = np.radians(lat2 - lat1)\n",
        "    delta_lambda = np.radians(lon2 - lon1)\n",
        "\n",
        "    a = np.sin(delta_phi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "# Helper function to calculate initial bearing\n",
        "def _calculate_initial_bearing(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate the initial bearing from point 1 to point 2.\n",
        "    Returns bearing in degrees (0-360).\n",
        "    \"\"\"\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "    delta_lon = lon2 - lon1\n",
        "\n",
        "    x = np.sin(delta_lon) * np.cos(lat2)\n",
        "    y = np.cos(lat1) * np.sin(lat2) - (np.sin(lat1) * np.cos(lat2) * np.cos(delta_lon))\n",
        "\n",
        "    initial_bearing = np.arctan2(x, y)\n",
        "\n",
        "    # Convert to degrees and normalize to 0-360\n",
        "    initial_bearing = np.degrees(initial_bearing)\n",
        "    initial_bearing = (initial_bearing + 360) % 360\n",
        "\n",
        "    return initial_bearing\n",
        "\n",
        "def apply_transformations(df: pd.DataFrame, latitude_col: str = 'latitude',\n",
        "                          longitude_col: str = 'longitude', timestamp_col: str = 'timestamp',\n",
        "                          vessel_id_col: str = 'vessel_id') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies initial feature engineering transformations to the DataFrame for vessel trajectories.\n",
        "    These transformations should *not* include normalization or one-hot encoding,\n",
        "    as those are handled by formatter.py and dataloader.py respectively.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame. Expected to have 'latitude', 'longitude', 'timestamp', and 'vessel_id' columns.\n",
        "        latitude_col (str): Name of the latitude column.\n",
        "        longitude_col (str): Name of the longitude column.\n",
        "        timestamp_col (str): Name of the timestamp column.\n",
        "        vessel_id_col (str): Name of the vessel ID column.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with engineered features.\n",
        "    \"\"\"\n",
        "    print(\"Applying initial feature engineering transformations...\")\n",
        "\n",
        "    # Ensure timestamp is datetime\n",
        "    if timestamp_col in df.columns:\n",
        "        df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')\n",
        "        df = df.dropna(subset=[timestamp_col]) # Drop rows with invalid timestamps\n",
        "        df = df.sort_values(by=[vessel_id_col, timestamp_col]).reset_index(drop=True)\n",
        "    else:\n",
        "        print(f\"Warning: '{timestamp_col}' column not found. Cannot create time-based features or order data.\")\n",
        "        return df # Or raise error depending on strictness\n",
        "\n",
        "    # Handle vessel-specific calculations\n",
        "    if vessel_id_col in df.columns:\n",
        "        # Group by vessel_id for calculations that should not cross vessel boundaries\n",
        "        grouped = df.groupby(vessel_id_col)\n",
        "\n",
        "        # Calculate time difference (in seconds)\n",
        "        df['time_diff_sec'] = grouped[timestamp_col].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "        # Calculate previous lat/lon for distance and bearing\n",
        "        df['prev_latitude'] = grouped[latitude_col].shift(1)\n",
        "        df['prev_longitude'] = grouped[longitude_col].shift(1)\n",
        "\n",
        "        # Calculate segment distance (meters)\n",
        "        # Use .apply(lambda x: ...) with a Series for performance on grouped data, if possible\n",
        "        # Otherwise, fillna with values that result in 0 distance for the first point\n",
        "        df['segment_distance_m'] = _haversine_distance(\n",
        "            df['prev_latitude'].fillna(df[latitude_col]), # Use current lat for first point (distance 0)\n",
        "            df['prev_longitude'].fillna(df[longitude_col]), # Use current lon for first point (distance 0)\n",
        "            df[latitude_col],\n",
        "            df[longitude_col]\n",
        "        )\n",
        "\n",
        "        # Calculate speed (meters/second and knots)\n",
        "        df['speed_mps'] = df['segment_distance_m'] / df['time_diff_sec']\n",
        "        # Handle division by zero for speed if time_diff_sec is 0\n",
        "        df['speed_mps'] = df['speed_mps'].replace([np.inf, -np.inf], np.nan).fillna(0) # Fill nan after inf/nan conversion\n",
        "\n",
        "        # 1 knot = 0.514444 meters/second\n",
        "        df['speed_knots'] = df['speed_mps'] / 0.514444\n",
        "        df['speed_knots'] = df['speed_knots'].fillna(0)\n",
        "\n",
        "        # Calculate course/bearing (degrees)\n",
        "        # For first point of a track, bearing is NaN, fill with 0 or last known.\n",
        "        df['course_deg'] = _calculate_initial_bearing(\n",
        "            df['prev_latitude'].fillna(df[latitude_col]), # Use current lat for first point\n",
        "            df['prev_longitude'].fillna(df[longitude_col]), # Use current lon for first point\n",
        "            df[latitude_col],\n",
        "            df[longitude_col]\n",
        "        )\n",
        "        # The bearing for the first point will be NaN or inaccurate, fill with 0 or a sensible default\n",
        "        df['course_deg'] = df['course_deg'].fillna(0)\n",
        "\n",
        "\n",
        "        # Calculate change in speed and course (derivative features)\n",
        "        df['speed_change_knots'] = grouped['speed_knots'].diff().fillna(0)\n",
        "        # For course change, handle wrap-around (e.g., 350 to 10 deg is +20, not -340)\n",
        "        df['course_change_deg'] = df['course_deg'] - grouped['course_deg'].shift(1)\n",
        "        df['course_change_deg'] = df['course_change_deg'].apply(lambda x: (x + 180) % 360 - 180 if pd.notna(x) else 0) # Wrap-around\n",
        "        df['course_change_deg'] = df['course_change_deg'].fillna(0)\n",
        "\n",
        "\n",
        "        # Drop intermediate columns used for calculation\n",
        "        df = df.drop(columns=['prev_latitude', 'prev_longitude'], errors='ignore')\n",
        "\n",
        "    else:\n",
        "        print(f\"Warning: '{vessel_id_col}' column not found. Calculations for speed, distance, course will be global, not per-vessel.\")\n",
        "        # If no vessel_id_col, you'd apply global diff/shift which is usually incorrect for trajectories.\n",
        "        # For a real pipeline, vessel_id_col would be mandatory.\n",
        "        df['time_diff_sec'] = df[timestamp_col].diff().dt.total_seconds().fillna(0)\n",
        "        df['segment_distance_m'] = _haversine_distance(df[latitude_col].shift(1).fillna(df[latitude_col]),\n",
        "                                                     df[longitude_col].shift(1).fillna(df[longitude_col]),\n",
        "                                                     df[latitude_col],\n",
        "                                                     df[longitude_col])\n",
        "        df['speed_knots'] = (df['segment_distance_m'] / df['time_diff_sec']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        df['course_deg'] = _calculate_initial_bearing(df[latitude_col].shift(1).fillna(df[latitude_col]),\n",
        "                                                    df[longitude_col].shift(1).fillna(df[longitude_col]),\n",
        "                                                    df[latitude_col],\n",
        "                                                    df[longitude_col]).fillna(0)\n",
        "        df['speed_change_knots'] = df['speed_knots'].diff().fillna(0)\n",
        "        df['course_change_deg'] = df['course_deg'] - df['course_deg'].shift(1)\n",
        "        df['course_change_deg'] = df['course_change_deg'].apply(lambda x: (x + 180) % 360 - 180 if pd.notna(x) else 0)\n",
        "        df['course_change_deg'] = df['course_change_deg'].fillna(0)\n",
        "\n",
        "\n",
        "    # Add more time-based features\n",
        "    df['hour_of_day'] = df[timestamp_col].dt.hour\n",
        "    df['day_of_week'] = df[timestamp_col].dt.dayofweek\n",
        "    df['day_of_year'] = df[timestamp_col].dt.dayofyear\n",
        "    df['month_of_year'] = df[timestamp_col].dt.month\n",
        "    df['quarter'] = df[timestamp_col].dt.quarter\n",
        "    df['is_weekend'] = df[timestamp_col].dt.dayofweek.isin([5, 6]).astype(int)\n",
        "\n",
        "    print(f\"Applied transformations. New columns: {list(df.columns)}\")\n",
        "    return df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"Running self-test for loading.py...\")\n",
        "    # Create a dummy parquet file with multiple vessels and some NaN/inf cases\n",
        "    test_df = pd.DataFrame({\n",
        "        'vessel_id': [1, 1, 1, 1, 2, 2, 2],\n",
        "        'timestamp': pd.to_datetime([\n",
        "            '2023-01-01 10:00:00', '2023-01-01 10:01:00', '2023-01-01 10:02:00', '2023-01-01 10:03:00',\n",
        "            '2023-01-01 10:00:00', '2023-01-01 10:02:00', '2023-01-01 10:03:00'\n",
        "        ]),\n",
        "        'latitude': [40.0, 40.005, 40.010, 40.015, 30.0, 30.01, 30.02],\n",
        "        'longitude': [-74.0, -74.005, -74.010, -74.015, -80.0, -80.01, -80.02],\n",
        "        'value': [10, 20, 30, 40, 50, 60, 70] # Just some arbitrary feature\n",
        "    })\n",
        "    test_path = \"test_loading_data.parquet\"\n",
        "    test_df.to_parquet(test_path, index=False)\n",
        "\n",
        "    loaded_df = read_ts_data(test_path)\n",
        "    print(\"Loaded DataFrame (first 5 rows):\\n\", loaded_df.head())\n",
        "\n",
        "    transformed_df = apply_transformations(loaded_df.copy())\n",
        "    print(\"\\nTransformed DataFrame (first 5 rows):\\n\", transformed_df.head())\n",
        "    print(\"\\nTransformed DataFrame (last 5 rows):\\n\", transformed_df.tail())\n",
        "\n",
        "\n",
        "    # Basic assertions\n",
        "    assert 'time_diff_sec' in transformed_df.columns\n",
        "    assert 'segment_distance_m' in transformed_df.columns\n",
        "    assert 'speed_knots' in transformed_df.columns\n",
        "    assert 'course_deg' in transformed_df.columns\n",
        "    assert 'speed_change_knots' in transformed_df.columns\n",
        "    assert 'course_change_deg' in transformed_df.columns\n",
        "    assert 'hour_of_day' in transformed_df.columns\n",
        "    assert 'is_weekend' in transformed_df.columns\n",
        "\n",
        "    # Check for NaNs that should have been filled (e.g., first row of each group)\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 1, 'time_diff_sec'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 2, 'time_diff_sec'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 1, 'segment_distance_m'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 2, 'segment_distance_m'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 1, 'speed_knots'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 2, 'speed_knots'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 1, 'course_deg'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 2, 'course_deg'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 1, 'speed_change_knots'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 2, 'speed_change_knots'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 1, 'course_change_deg'].iloc[0] == 0\n",
        "    assert transformed_df.loc[transformed_df['vessel_id'] == 2, 'course_change_deg'].iloc[0] == 0\n",
        "\n",
        "    print(\"\\nloading.py self-test PASSED!\")\n",
        "    import os\n",
        "    os.remove(test_path)"
      ],
      "metadata": {
        "id": "a5jv3lMlFkQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f57f9c97-c8a5-49bc-f692-106ddea009c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing loading.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python loading.py"
      ],
      "metadata": {
        "id": "DonimOu_-OL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_loader.py\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Union, Tuple, Optional\n",
        "import hashlib\n",
        "import traceback\n",
        "\n",
        "from normalizer import Normalizer # Assuming this is a separate file you have\n",
        "\n",
        "class RunConfig:\n",
        "    \"\"\"\n",
        "    Stores the configuration relevant to data loading, including:\n",
        "    - Features DataFrame (from formatter.py's output features.csv)\n",
        "    - Normalization factors (calculated by dataloader.py)\n",
        "    - CLI flags that affect data loading/preprocessing done by formatter.py and dataloader.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, cli_args: Dict[str, Any]):\n",
        "        self.cli_args = cli_args\n",
        "        self.features_df: Optional[pd.DataFrame] = None # From formatter.py's features.csv\n",
        "        self.normalization_factors: Optional[Dict[str, Dict[str, float]]] = None # Calculated by DataLoader\n",
        "        # Add other relevant cli_args to settings for hashing\n",
        "        self.settings = {\n",
        "            k: v for k, v in cli_args.items() if k not in ['cache_base_dir', 'features_csv_path', 'base_data_path', 'force_rebuild']\n",
        "        }\n",
        "\n",
        "    def to_hashable(self) -> str:\n",
        "        \"\"\"Generates a hashable string from the relevant configuration.\"\"\"\n",
        "        hash_input = {\n",
        "            'settings': self.settings,\n",
        "            # IMPORTANT: Convert DataFrame to a stable, hashable representation\n",
        "            # For features_df, sorting by 'FeatureName' ensures consistent order\n",
        "            'features': (self.features_df.sort_values('FeatureName').to_dict('records')\n",
        "                         if self.features_df is not None else None),\n",
        "            'normalization_factors': self.normalization_factors\n",
        "        }\n",
        "        # Use a more robust JSON dump for hashing (separators, sorted keys)\n",
        "        hash_string = json.dumps(hash_input, sort_keys=True, separators=(',', ':')).encode('utf-8')\n",
        "        return hashlib.md5(hash_string).hexdigest()\n",
        "\n",
        "    def load_from_cache_dir(self, cache_dir: Path):\n",
        "        \"\"\"Loads features_df and normalization factors from the cache directory.\"\"\"\n",
        "        features_path = cache_dir / \"features.csv\"\n",
        "        normalization_factors_path = cache_dir / \"normalization_factors.json\"\n",
        "\n",
        "        if features_path.exists():\n",
        "            self.features_df = pd.read_csv(features_path)\n",
        "            # print(f\"Loaded features_df from cache: {features_path}\") # Removed verbose print\n",
        "        else:\n",
        "            print(f\"Warning: features.csv not found in cache at {features_path}.\")\n",
        "\n",
        "        if normalization_factors_path.exists():\n",
        "            with open(normalization_factors_path, 'r') as f:\n",
        "                self.normalization_factors = json.load(f)\n",
        "            # print(f\"Loaded normalization factors from cache: {normalization_factors_path}\") # Removed verbose print\n",
        "        else:\n",
        "            print(f\"Warning: normalization_factors.json not found in cache at {normalization_factors_path}.\")\n",
        "\n",
        "    def save_to_cache_dir(self, cache_dir: Path):\n",
        "        \"\"\"Saves features_df and normalization factors to the cache directory.\"\"\"\n",
        "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if self.features_df is not None:\n",
        "            self.features_df.to_csv(cache_dir / \"features.csv\", index=False)\n",
        "            # print(f\"Saved features_df to cache: {cache_dir / 'features.csv'}\") # Removed verbose print\n",
        "\n",
        "        if self.normalization_factors is not None:\n",
        "            with open(cache_dir / \"normalization_factors.json\", 'w') as f:\n",
        "                json.dump(self.normalization_factors, f, indent=4)\n",
        "            # print(f\"Saved normalization factors to cache: {cache_dir / 'normalization_factors.json'}\") # Removed verbose print\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"\n",
        "    Loads pre-processed data (X, Y numpy arrays) from formatter.py's output,\n",
        "    calculates and applies normalization, and manages caching.\n",
        "    \"\"\"\n",
        "    def __init__(self, cli_args: Dict[str, Any]):\n",
        "        self.cli_args = cli_args\n",
        "        self.base_data_path = Path(cli_args['base_data_path']) # Path to where formatter.py outputs (e.g., .../formatted_debug/)\n",
        "        self.cache_base_dir = Path(cli_args['cache_base_dir'])\n",
        "        self.time_gap = cli_args['time_gap'] # e.g., '30' for 30_min_time_gap\n",
        "        self.is_data_chunked = cli_args.get('is_data_chunked', True) # formatter.py usually chunks data\n",
        "        self.force_rebuild = cli_args.get('force_rebuild', False)\n",
        "        self.normalization_method = cli_args.get('normalization_method', 'min_max')\n",
        "\n",
        "        self.data: Dict[str, Tuple[np.ndarray, np.ndarray]] = {} # Stores (X, Y) numpy arrays (after normalization)\n",
        "        self.run_config = RunConfig(cli_args) # Initialize RunConfig\n",
        "        self.normalizer = Normalizer(method=self.normalization_method)\n",
        "\n",
        "        self.cache_dir: Optional[Path] = None # Will be set once and reused\n",
        "\n",
        "        # --- NEW LOGIC: Load features.csv and attempt to load normalization factors once at init ---\n",
        "        # This ensures the RunConfig is fully populated before any hashing is done.\n",
        "        features_csv_path = self.base_data_path / 'features.csv'\n",
        "        if features_csv_path.exists():\n",
        "            self.run_config.features_df = pd.read_csv(features_csv_path)\n",
        "            print(f\"DataLoader: Loaded features metadata from {features_csv_path}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Error: features.csv not found at {features_csv_path}. \"\n",
        "                                    \"Please ensure formatter.py has run successfully and outputs to base_data_path.\")\n",
        "\n",
        "        # Determine the cache directory hash based on initial config (including features_df)\n",
        "        self.cache_dir = self._get_cache_dir() # Determine cache_dir once\n",
        "\n",
        "        # Attempt to load normalization factors from this determined cache_dir\n",
        "        # This is crucial for 'val' and 'test' to use the 'train' factors\n",
        "        norm_factors_path = self.cache_dir / \"normalization_factors.json\"\n",
        "        if norm_factors_path.exists():\n",
        "            with open(norm_factors_path, 'r') as f:\n",
        "                self.normalizer.normalization_factors = json.load(f)\n",
        "                self.run_config.normalization_factors = self.normalizer.normalization_factors # Keep run_config in sync\n",
        "            print(f\"DataLoader: Loaded normalization factors from cache: {norm_factors_path}\")\n",
        "        else:\n",
        "            print(\"DataLoader: Normalization factors not found in cache. Will calculate on first 'train' load.\")\n",
        "\n",
        "        print(f\"DataLoader: Determined consistent cache directory: {self.cache_dir}\")\n",
        "\n",
        "\n",
        "    def _get_cache_dir(self) -> Path:\n",
        "        \"\"\"Determines and creates the cache directory based on RunConfig hash.\"\"\"\n",
        "        run_hash = self.run_config.to_hashable()\n",
        "        cache_path = self.cache_base_dir / run_hash\n",
        "        cache_path.mkdir(parents=True, exist_ok=True)\n",
        "        return cache_path\n",
        "\n",
        "    def _load_raw_formatter_output(self, dataset_type: str) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Loads the unnormalized X and Y numpy arrays directly from formatter.py's output.\n",
        "        \"\"\"\n",
        "        formatter_output_name_map = {\n",
        "            'train': 'train_long_term_train',\n",
        "            'val': 'valid_long_term_train', # Keep this as per your directory structure\n",
        "            'test': 'test_long_term_test',\n",
        "            'probe': 'probe_data' # Special case for probe data\n",
        "        }\n",
        "\n",
        "        if dataset_type == 'probe':\n",
        "            probe_path = Path(self.cli_args.get('probe_data_path'))\n",
        "            if not probe_path.exists():\n",
        "                print(f\"Warning: Probe data not found at {probe_path}. Returning empty arrays.\")\n",
        "                return (np.array([]), None) # Y is None for probe data\n",
        "            X_probe = np.load(probe_path)\n",
        "            return (X_probe, None)\n",
        "\n",
        "        formatter_dir_name = formatter_output_name_map.get(dataset_type)\n",
        "        if not formatter_dir_name:\n",
        "            raise ValueError(f\"Unknown dataset_type: {dataset_type}\")\n",
        "\n",
        "        base_data_set_path = self.base_data_path / formatter_dir_name\n",
        "        x_dir = base_data_set_path / f\"{self.time_gap}_min_time_gap_x\"\n",
        "        y_dir = base_data_set_path / f\"{self.time_gap}_min_time_gap_y\"\n",
        "\n",
        "        if not x_dir.exists() or not y_dir.exists():\n",
        "            print(f\"    Warning: Data directories not found for {dataset_type} at {x_dir} and {y_dir}. Returning empty arrays.\")\n",
        "            return (np.array([]), np.array([]))\n",
        "\n",
        "        X_parts = []\n",
        "        Y_parts = []\n",
        "\n",
        "        if self.is_data_chunked:\n",
        "            x_files = sorted(x_dir.glob('*.npy'), key=lambda p: int(p.stem))\n",
        "            y_files = sorted(y_dir.glob('*.npy'), key=lambda p: int(p.stem))\n",
        "\n",
        "            if not x_files or not y_files:\n",
        "                print(f\"    Warning: No .npy chunks found in {x_dir} or {y_dir}. Returning empty arrays.\")\n",
        "                return (np.array([]), np.array([]))\n",
        "\n",
        "            if len(x_files) != len(y_files):\n",
        "                raise ValueError(f\"Mismatch in number of X and Y chunks for {dataset_type} ({len(x_files)} vs {len(y_files)})\")\n",
        "\n",
        "            for x_file, y_file in zip(x_files, y_files):\n",
        "                try:\n",
        "                    X_parts.append(np.load(x_file))\n",
        "                    y_array = np.load(y_file)\n",
        "                    Y_parts.append(y_array)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading chunk {x_file} or {y_file}: {e}\")\n",
        "                    traceback.print_exc() # Print full traceback for debugging\n",
        "                    continue # Skip problematic chunks for now\n",
        "\n",
        "            if not X_parts:\n",
        "                print(f\"    Warning: No valid chunks loaded for {dataset_type}. Returning empty arrays.\")\n",
        "                return (np.array([]), np.array([]))\n",
        "\n",
        "            X_raw = np.concatenate(X_parts, axis=0)\n",
        "            Y_raw = np.concatenate(Y_parts, axis=0)\n",
        "\n",
        "        else: # Load full files if not chunked\n",
        "            x_file = x_dir.parent / f\"{formatter_dir_name}_{self.time_gap}_min_time_gap_x.npy\"\n",
        "            y_file = y_dir.parent / f\"{formatter_dir_name}_{self.time_gap}_min_time_gap_y.npy\"\n",
        "            if not x_file.exists() or not y_file.exists():\n",
        "                    print(f\"    Warning: Full data files not found for {dataset_type} at {x_file} and {y_file}. Returning empty arrays.\")\n",
        "                    return (np.array([]), np.array([]))\n",
        "            try:\n",
        "                X_raw = np.load(x_file)\n",
        "                Y_raw = np.load(y_file)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading full file {x_file} or {y_file}: {e}\")\n",
        "                traceback.print_exc() # Print full traceback for debugging\n",
        "                return (np.array([]), np.array([]))\n",
        "\n",
        "        print(f\"    Loaded raw data for {dataset_type}: X shape {X_raw.shape}, Y shape {Y_raw.shape}\")\n",
        "        return X_raw, Y_raw\n",
        "\n",
        "    def load_data(self, dataset_type: str, force_rebuild: Optional[bool] = None):\n",
        "        \"\"\"\n",
        "        Loads, normalizes, and caches data for a given dataset type (train, val, test, probe).\n",
        "        \"\"\"\n",
        "        if force_rebuild is None:\n",
        "            force_rebuild = self.force_rebuild\n",
        "\n",
        "        print(f\"\\nLoading data for '{dataset_type}' (force_rebuild: {force_rebuild})...\")\n",
        "\n",
        "        # Use the cache_dir that was determined at initialization\n",
        "        cached_x_path = self.cache_dir / f\"{dataset_type}_X.npy\"\n",
        "        cached_y_path = self.cache_dir / f\"{dataset_type}_Y.npy\"\n",
        "        norm_factors_path_in_cache = self.cache_dir / \"normalization_factors.json\" # Redundant with self.normalizer, but good for saving\n",
        "\n",
        "        # Attempt to load from cache\n",
        "        if not force_rebuild and cached_x_path.exists() and (cached_y_path.exists() or dataset_type == 'probe') and norm_factors_path_in_cache.exists():\n",
        "            try:\n",
        "                print(f\"    Attempting to load from cache: {cached_x_path}\")\n",
        "                X = np.load(cached_x_path)\n",
        "                Y = np.load(cached_y_path) if dataset_type != 'probe' else None\n",
        "\n",
        "                # Normalization factors are loaded at init, but re-verify\n",
        "                if self.normalizer.normalization_factors is None:\n",
        "                    # This should theoretically not happen if norm_factors_path_in_cache exists\n",
        "                    with open(norm_factors_path_in_cache, 'r') as f:\n",
        "                        self.normalizer.normalization_factors = json.load(f)\n",
        "                        self.run_config.normalization_factors = self.normalizer.normalization_factors\n",
        "                    print(f\"    Loaded normalization factors (secondary check) from cache: {norm_factors_path_in_cache}\")\n",
        "\n",
        "\n",
        "                print(f\"    Successfully loaded '{dataset_type}' data from cache.\")\n",
        "                self.data[dataset_type] = (X, Y)\n",
        "                return\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    Error loading from cache: {e}. Rebuilding data.\")\n",
        "                traceback.print_exc()\n",
        "                # Clean up potentially corrupted cache files if an error occurs\n",
        "                if cached_x_path.exists(): os.remove(cached_x_path)\n",
        "                if cached_y_path.exists() and dataset_type != 'probe': os.remove(cached_y_path)\n",
        "                if norm_factors_path_in_cache.exists(): os.remove(norm_factors_path_in_cache)\n",
        "\n",
        "\n",
        "        # Load raw data from formatter output\n",
        "        X_raw, Y_raw = self._load_raw_formatter_output(dataset_type)\n",
        "\n",
        "        if X_raw.size == 0:\n",
        "            print(f\"    Raw data for '{dataset_type}' is empty. Skipping further processing.\")\n",
        "            self.data[dataset_type] = (np.array([]), np.array([]))\n",
        "            return\n",
        "\n",
        "        # Get feature names and their order from features.csv\n",
        "        num_features_in_current_split = X_raw.shape[-1]\n",
        "\n",
        "        # Use the features_df loaded during __init__\n",
        "        if self.run_config.features_df is None:\n",
        "            # This should have been caught in __init__\n",
        "            raise RuntimeError(\"features_df is None. This indicates an issue during DataLoader initialization.\")\n",
        "\n",
        "        # Filter features based on what's actually present in X_raw and numeric_dtypes in features.csv\n",
        "        numeric_features_df = self.run_config.features_df[\n",
        "            self.run_config.features_df['FeatureName'].isin(self.run_config.features_df['FeatureName'].tolist()[:num_features_in_current_split]) &\n",
        "            self.run_config.features_df['dtype'].isin(['float32', 'float64', 'int32', 'int64', 'int16', 'int8', 'uint8', 'bool'])\n",
        "        ]\n",
        "        features_to_normalize = numeric_features_df['FeatureName'].tolist()\n",
        "        feature_names_in_X = self.run_config.features_df['FeatureName'].tolist()[:num_features_in_current_split]\n",
        "        feature_mapping = {name: idx for idx, name in enumerate(feature_names_in_X)}\n",
        "\n",
        "        excluded_from_normalization = ['mmsi', 'base_datetime', 'year', 'month', 'day_of_year', 'day_of_week', 'hour_of_day', 'quarter', 'is_weekend']\n",
        "        features_to_normalize = [f for f in features_to_normalize if f not in excluded_from_normalization]\n",
        "\n",
        "        # Calculate normalization factors ONLY if for 'train' data and factors are not already loaded/calculated\n",
        "        if dataset_type == 'train' and self.normalizer.normalization_factors is None:\n",
        "            X_raw_flat = X_raw.reshape(-1, X_raw.shape[-1])\n",
        "            temp_df_for_factors = pd.DataFrame(X_raw_flat, columns=feature_names_in_X)\n",
        "\n",
        "            print(\"    Calculating normalization factors from training data...\")\n",
        "            normalization_factors = self.normalizer.get_normalization_factors(temp_df_for_factors, features_to_normalize)\n",
        "            self.normalizer.normalization_factors = normalization_factors\n",
        "            self.run_config.normalization_factors = normalization_factors # Store in run_config for hashing consistency\n",
        "\n",
        "            # Save normalization factors to the cache directory determined at init\n",
        "            self.run_config.save_to_cache_dir(self.cache_dir)\n",
        "            print(f\"    Saved newly calculated normalization factors to cache: {norm_factors_path_in_cache}\")\n",
        "        elif self.normalizer.normalization_factors is None and dataset_type != 'train':\n",
        "             # This means we are loading val/test before train, or norm factors failed to load from cache\n",
        "             # This is a critical error, as val/test data needs to be normalized with train factors.\n",
        "             raise RuntimeError(f\"Normalization factors are not available for '{dataset_type}' data. \"\n",
        "                                \"Please ensure 'train' data is loaded first to calculate/load factors, \"\n",
        "                                \"or ensure normalization_factors.json exists in cache.\")\n",
        "\n",
        "\n",
        "        # Apply normalization using the loaded/calculated factors\n",
        "        if self.normalizer.normalization_factors:\n",
        "            print(f\"    Applying {self.normalizer.method} normalization to '{dataset_type}' data...\")\n",
        "            X_normalized = self.normalizer.normalize_data(X_raw,\n",
        "                                                            self.normalizer.normalization_factors,\n",
        "                                                            feature_mapping)\n",
        "            print(f\"    Normalization applied to X data for '{dataset_type}'.\")\n",
        "            X = X_normalized\n",
        "            # Y data (latitude, longitude) should typically NOT be normalized for prediction\n",
        "            Y = Y_raw\n",
        "            print(f\"    Y data is kept unnormalized as it represents target coordinates.\")\n",
        "        else:\n",
        "            print(\"    Normalization factors not available. Skipping normalization.\")\n",
        "            X = X_raw\n",
        "            Y = Y_raw\n",
        "\n",
        "        # Store processed data\n",
        "        self.data[dataset_type] = (X, Y)\n",
        "\n",
        "        # Cache the processed data (X, Y)\n",
        "        if self.cache_dir:\n",
        "            try:\n",
        "                print(f\"    Caching processed data for '{dataset_type}' to {self.cache_dir}...\")\n",
        "                np.save(cached_x_path, X)\n",
        "                if dataset_type != 'probe':\n",
        "                    np.save(cached_y_path, Y)\n",
        "                print(f\"    Successfully cached '{dataset_type}' data.\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error caching data for '{dataset_type}': {e}\")\n",
        "\n",
        "\n",
        "    def get_data(self, dataset_type: str) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
        "        \"\"\"Retrieves loaded data for a given dataset type.\"\"\"\n",
        "        if dataset_type not in self.data:\n",
        "            raise ValueError(f\"Data for '{dataset_type}' not loaded yet. Call load_data() first.\")\n",
        "        return self.data[dataset_type]\n",
        "\n",
        "    # Add a method to get the normalizer, e.g., for use in `predict.py`\n",
        "    def get_normalizer(self) -> Normalizer:\n",
        "        if self.normalizer.normalization_factors is None:\n",
        "            raise RuntimeError(\"Normalization factors are not set. Load 'train' data first.\")\n",
        "        return self.normalizer\n",
        "\n",
        "# --- Configuration for direct script execution ---\n",
        "CLI_ARGS = {\n",
        "    'cache_base_dir': '/content/data/new_york/dataloader_cache', # Example cache directory\n",
        "    'base_data_path': '/content/data/new_york/formatted_debug',   # Path to formatter.py output\n",
        "    'time_gap': '30', # Must match time_gaps in config.py and formatter.py\n",
        "    'is_data_chunked': True, # True if formatter.py saved chunks\n",
        "    'force_rebuild': False, # Set to True to ignore cache - generally false for cache\n",
        "    'normalization_method': 'min_max', # Must match Normalizer initialization\n",
        "    'input_sequence_length': 7, # Added for completeness as it affects RunConfig hash\n",
        "    'output_sequence_length': 6, # Added for completeness\n",
        "    'num_features': 51, # Added for completeness\n",
        "    # 'probe_data_path': './temp_data_for_dl_test/probe_data.npy', # Example for probe data\n",
        "}\n",
        "\n",
        "# --- Main execution block ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running data_loader.py as a script...\")\n",
        "    try:\n",
        "        # Initialize and run the DataLoader\n",
        "        data_loader = DataLoader(CLI_ARGS)\n",
        "\n",
        "        # Force rebuild for this test run to ensure new cache generation if needed\n",
        "        # In a real scenario, you'd typically set force_rebuild: False in CLI_ARGS\n",
        "        # unless you change raw data or config.\n",
        "        # However, for testing this fix, it's good to force it.\n",
        "\n",
        "        data_loader.load_data(dataset_type='train', force_rebuild=True)\n",
        "        data_loader.load_data(dataset_type='val', force_rebuild=True)\n",
        "        data_loader.load_data(dataset_type='test', force_rebuild=True)\n",
        "\n",
        "        print(\"\\nDataLoader self-test PASSED!\")\n",
        "        X_train, Y_train = data_loader.get_data('train')\n",
        "        X_val, Y_val = data_loader.get_data('val')\n",
        "        X_test, Y_test = data_loader.get_data('test')\n",
        "\n",
        "        print(f\"Final loaded train shapes: X={X_train.shape}, Y={Y_train.shape}\")\n",
        "        print(f\"Final loaded val shapes: X={X_val.shape}, Y={Y_val.shape}\")\n",
        "        print(f\"Final loaded test shapes: X={X_test.shape}, Y={X_test.shape}\") # Should be Y_test.shape\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data loading: {e}\")\n",
        "        traceback.print_exc() # Print full traceback for debugging"
      ],
      "metadata": {
        "id": "ZNy3J9PbDdqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76c87bd-b3db-4f34-dd61-96b547855f82"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_loader.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wv5Ji6er-Zi",
        "outputId": "ccd860b9-774f-49e0-a5df-b71dc1414a60"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running data_loader.py as a script...\n",
            "DataLoader: Loaded features metadata from /content/data/new_york/formatted_debug/features.csv\n",
            "DataLoader: Normalization factors not found in cache. Will calculate on first 'train' load.\n",
            "DataLoader: Determined consistent cache directory: /content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9\n",
            "\n",
            "Loading data for 'train' (force_rebuild: True)...\n",
            "    Loaded raw data for train: X shape (1084072, 37, 51), Y shape (1084072, 6, 2)\n",
            "    Calculating normalization factors from training data...\n",
            "    Saved newly calculated normalization factors to cache: /content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9/normalization_factors.json\n",
            "    Applying min_max normalization to 'train' data...\n",
            "    Normalization applied to X data for 'train'.\n",
            "    Y data is kept unnormalized as it represents target coordinates.\n",
            "    Caching processed data for 'train' to /content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9...\n",
            "    Successfully cached 'train' data.\n",
            "\n",
            "Loading data for 'val' (force_rebuild: True)...\n",
            "    Loaded raw data for val: X shape (277431, 37, 51), Y shape (277431, 6, 2)\n",
            "    Applying min_max normalization to 'val' data...\n",
            "    Normalization applied to X data for 'val'.\n",
            "    Y data is kept unnormalized as it represents target coordinates.\n",
            "    Caching processed data for 'val' to /content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9...\n",
            "    Successfully cached 'val' data.\n",
            "\n",
            "Loading data for 'test' (force_rebuild: True)...\n",
            "    Loaded raw data for test: X shape (232197, 37, 51), Y shape (232197, 6, 2)\n",
            "    Applying min_max normalization to 'test' data...\n",
            "    Normalization applied to X data for 'test'.\n",
            "    Y data is kept unnormalized as it represents target coordinates.\n",
            "    Caching processed data for 'test' to /content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9...\n",
            "    Successfully cached 'test' data.\n",
            "\n",
            "DataLoader self-test PASSED!\n",
            "Final loaded train shapes: X=(1084072, 37, 51), Y=(1084072, 6, 2)\n",
            "Final loaded val shapes: X=(277431, 37, 51), Y=(277431, 6, 2)\n",
            "Final loaded test shapes: X=(232197, 37, 51), Y=(232197, 37, 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generator.py\n",
        "import numpy as np\n",
        "from typing import Tuple, Generator\n",
        "\n",
        "class DataGenerator:\n",
        "    \"\"\"\n",
        "    A simple data generator for Keras/TensorFlow models.\n",
        "    Assumes X and Y are already loaded and normalized by the DataLoader.\n",
        "    \"\"\"\n",
        "    def __init__(self, X: np.ndarray, Y: np.ndarray, batch_size: int):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.batch_size = batch_size\n",
        "        self.num_samples = X.shape[0]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Denotes the number of batches per epoch.\"\"\"\n",
        "        return int(np.ceil(self.num_samples / self.batch_size))\n",
        "\n",
        "    def generate(self) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
        "        \"\"\"Generates batches of data.\"\"\"\n",
        "        indices = np.arange(self.num_samples)\n",
        "        np.random.shuffle(indices) # Shuffle data for each epoch\n",
        "\n",
        "        for start_idx in range(0, self.num_samples, self.batch_size):\n",
        "            batch_indices = indices[start_idx : start_idx + self.batch_size]\n",
        "\n",
        "            batch_X = self.X[batch_indices]\n",
        "            batch_Y = self.Y[batch_indices]\n",
        "\n",
        "            yield batch_X, batch_Y\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running self-test for data_generator.py...\")\n",
        "\n",
        "    # Create dummy normalized data (as if from DataLoader)\n",
        "    dummy_X = np.random.rand(100, 7, 44).astype(np.float32) # 100 samples, 7 timesteps, 44 features\n",
        "    dummy_Y = np.random.rand(100, 2).astype(np.float32)    # 100 samples, 2 prediction targets\n",
        "\n",
        "    batch_size = 32\n",
        "    generator = DataGenerator(dummy_X, dummy_Y, batch_size)\n",
        "\n",
        "    # Test length\n",
        "    expected_len = int(np.ceil(100 / 32))\n",
        "    assert len(generator) == expected_len, f\"Expected length {expected_len}, got {len(generator)}\"\n",
        "    print(f\"Generator length: {len(generator)} batches.\")\n",
        "\n",
        "    # Test generation\n",
        "    for i, (batch_X, batch_Y) in enumerate(generator.generate()):\n",
        "        print(f\"Batch {i+1}: X shape {batch_X.shape}, Y shape {batch_Y.shape}\")\n",
        "        assert batch_X.shape[0] <= batch_size\n",
        "        assert batch_X.shape[1:] == (7, 44)\n",
        "        assert batch_Y.shape[0] <= batch_size\n",
        "        assert batch_Y.shape[1:] == (2,)\n",
        "\n",
        "    print(\"\\nDataGenerator self-test PASSED!\")"
      ],
      "metadata": {
        "id": "JUFzCLGTOztH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f20c4701-69d9-4b9b-8664-a6a8a3cf021b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing generator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python generator.py"
      ],
      "metadata": {
        "id": "pX0lfj7JPu6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras-tuner numpy pandas scikit-learn"
      ],
      "metadata": {
        "id": "146XodP1WHK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca2147f-8844-4ca6-f022-f7542086bd4f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile metrics_losses.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.metrics import Metric\n",
        "import numpy as np\n",
        "\n",
        "# Constants for Haversine calculation\n",
        "R_EARTH = 6371.0 # Radius of Earth in kilometers\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate the haversine distance between two sets of (latitude, longitude) coordinates.\n",
        "    This function is vectorized to handle batches of data.\n",
        "\n",
        "    Args:\n",
        "        lat1, lon1: TensorFlow tensors of shape (batch_size, sequence_length).\n",
        "        lat2, lon2: TensorFlow tensors of shape (batch_size, sequence_length).\n",
        "\n",
        "    Returns:\n",
        "        A tensor of haversine distances in kilometers.\n",
        "    \"\"\"\n",
        "    # Use TensorFlow's native math functions for consistency and reliability\n",
        "    lat1_rad = tf.experimental.numpy.deg2rad(lat1)\n",
        "    lon1_rad = tf.experimental.numpy.deg2rad(lon1)\n",
        "    lat2_rad = tf.experimental.numpy.deg2rad(lat2)\n",
        "    lon2_rad = tf.experimental.numpy.deg2rad(lon2)\n",
        "\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "\n",
        "    # Haversine formula implementation\n",
        "    a = tf.sin(dlat / 2.0)**2 + tf.cos(lat1_rad) * tf.cos(lat2_rad) * tf.sin(dlon / 2.0)**2\n",
        "\n",
        "    # Clip 'a' to ensure input to atan2 is in valid range [0, 1] due to\n",
        "    # floating-point inaccuracies.\n",
        "    a_clipped = tf.clip_by_value(a, 0.0, 1.0)\n",
        "    c = 2.0 * tf.math.atan2(tf.sqrt(a_clipped), tf.sqrt(1.0 - a_clipped))\n",
        "\n",
        "    distance = R_EARTH * c\n",
        "    # Return mean distance, ensuring any residual nans are zeroed out\n",
        "    distance = tf.where(tf.math.is_nan(distance), 0.0, distance)\n",
        "    return distance\n",
        "\n",
        "def haversine_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates the mean haversine distance between predicted and true coordinates.\n",
        "    The inputs are expected to have a shape of (batch_size, timesteps, 2),\n",
        "    where the last dimension represents [latitude, longitude].\n",
        "    \"\"\"\n",
        "    y_true_lat = y_true[..., 0]\n",
        "    y_true_lon = y_true[..., 1]\n",
        "    y_pred_lat = y_pred[..., 0]\n",
        "    y_pred_lon = y_pred[..., 1]\n",
        "\n",
        "    distance = haversine_distance(y_true_lat, y_true_lon, y_pred_lat, y_pred_lon)\n",
        "\n",
        "    # Return the mean distance per batch\n",
        "    return tf.reduce_mean(distance)\n",
        "\n",
        "\n",
        "class HaversineMetric(Metric):\n",
        "    \"\"\"\n",
        "    Custom Keras metric to compute the mean Haversine distance,\n",
        "    with clipping safeguards for numerical stability.\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"haversine_metric\", **kwargs):\n",
        "        super(HaversineMetric, self).__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Updates the state with a new batch of predictions and true values.\n",
        "        \"\"\"\n",
        "        y_true_lat = y_true[..., 0]\n",
        "        y_true_lon = y_true[..., 1]\n",
        "        y_pred_lat = y_pred[..., 0]\n",
        "        y_pred_lon = y_pred[..., 1]\n",
        "\n",
        "        # Calculate haversine distance for each sample\n",
        "        distances = haversine_distance(y_true_lat, y_true_lon, y_pred_lat, y_pred_lon)\n",
        "\n",
        "        # Update the state variables\n",
        "        self.total.assign_add(tf.reduce_sum(distances))\n",
        "        self.count.assign_add(tf.cast(tf.size(distances), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        \"\"\"\n",
        "        Returns the mean Haversine distance over all processed batches.\n",
        "        \"\"\"\n",
        "        return self.total / self.count\n",
        "\n",
        "    def reset_state(self):\n",
        "        \"\"\"\n",
        "        Resets the state of the metric.\n",
        "        \"\"\"\n",
        "        self.total.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "\n",
        "# --- Self-testing block for metrics_losses.py ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running self-test for metrics_losses.py...\")\n",
        "\n",
        "    # Create mock true and predicted data\n",
        "    y_true_mock = np.array([\n",
        "        [[34.0522, -118.2437], [34.0522, -118.2437]],  # Los Angeles\n",
        "        [[40.7128, -74.0060], [40.7128, -74.0060]]   # New York\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    # Test case 1: Identical predictions, distance should be close to 0\n",
        "    y_pred_mock_1 = np.array([\n",
        "        [[34.0522, -118.2437], [34.0522, -118.2437]],\n",
        "        [[40.7128, -74.0060], [40.7128, -74.0060]]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    # Test haversine_loss\n",
        "    loss_1 = haversine_loss(y_true_mock, y_pred_mock_1)\n",
        "    print(f\"Loss with identical predictions: {loss_1.numpy():.4f} km (should be ~0)\")\n",
        "    assert np.isclose(loss_1.numpy(), 0.0), \"Haversine loss with identical predictions failed.\"\n",
        "\n",
        "    # Test HaversineMetric\n",
        "    metric_1 = HaversineMetric()\n",
        "    metric_1.update_state(y_true_mock, y_pred_mock_1)\n",
        "    result_1 = metric_1.result()\n",
        "    print(f\"Metric with identical predictions: {result_1.numpy():.4f} km (should be ~0)\")\n",
        "    assert np.isclose(result_1.numpy(), 0.0), \"Haversine metric with identical predictions failed.\"\n",
        "    metric_1.reset_state()\n",
        "\n",
        "    # Test case 2: Different predictions, distance should be non-zero\n",
        "    y_pred_mock_2 = np.array([\n",
        "        [[34.0522, -118.2437], [35.0522, -119.2437]],  # Los Angeles to slightly different location\n",
        "        [[40.7128, -74.0060], [41.7128, -75.0060]]     # New York to slightly different location\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "    # Test haversine_loss\n",
        "    loss_2 = haversine_loss(y_true_mock, y_pred_mock_2)\n",
        "    print(f\"Loss with different predictions: {loss_2.numpy():.4f} km (should be > 0)\")\n",
        "    assert loss_2.numpy() > 0.0, \"Haversine loss with different predictions failed.\"\n",
        "\n",
        "    # Test HaversineMetric\n",
        "    metric_2 = HaversineMetric()\n",
        "    metric_2.update_state(y_true_mock, y_pred_mock_2)\n",
        "    result_2 = metric_2.result()\n",
        "    print(f\"Metric with different predictions: {result_2.numpy():.4f} km (should be > 0)\")\n",
        "    assert result_2.numpy() > 0.0, \"Haversine metric with different predictions failed.\"\n",
        "\n",
        "    print(\"\\nAll metrics_losses.py self-tests PASSED!\")\n"
      ],
      "metadata": {
        "id": "r-sJzdqpRYus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbed6ae7-9cac-4da3-8ea7-f8ce9944edd8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting metrics_losses.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python metrics_losses.py"
      ],
      "metadata": {
        "id": "SpZ3ig1dVrll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae51bf6-c94b-4039-d4bb-fa4aa1edce96"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-04 18:56:05.153822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754333765.174168   60628 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754333765.180352   60628 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Running self-test for metrics_losses.py...\n",
            "2025-08-04 18:56:09.676041: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1754333769.676188   60628 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13838 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "Loss with identical predictions: 0.0000 km (should be ~0)\n",
            "Metric with identical predictions: 0.0000 km (should be ~0)\n",
            "Loss with different predictions: 70.7986 km (should be > 0)\n",
            "Metric with different predictions: 70.7986 km (should be > 0)\n",
            "\n",
            "All metrics_losses.py self-tests PASSED!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras-tuner numpy pandas scikit-learn"
      ],
      "metadata": {
        "id": "5dQK1IbAwfdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Attention, Input, Reshape, Permute, Multiply, Concatenate, Add, Lambda\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Assuming haversine_loss and HaversineMetric are defined elsewhere.\n",
        "# For the purpose of this example and to make the self-test runnable,\n",
        "# we'll define simple mock versions.\n",
        "def haversine_loss(y_true, y_pred):\n",
        "    # This is a placeholder. You should use your actual loss function here.\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "class HaversineMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='haversine_metric', **kwargs):\n",
        "        super(HaversineMetric, self).__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name='total', initializer='zeros')\n",
        "        self.count = self.add_weight(name='count', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # This is a placeholder. You should use your actual metric logic here.\n",
        "        values = tf.abs(y_true - y_pred)\n",
        "        self.total.assign_add(tf.reduce_sum(values))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), dtype=tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.total.assign(0.0)\n",
        "        self.count.assign(0.0)\n",
        "\n",
        "def build_lstm_model(hp, input_shape, output_dim):\n",
        "    \"\"\"Builds an LSTM model with tunable hyperparameters.\"\"\"\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    # Layers: search from 15\n",
        "    num_lstm_layers = hp.Int('num_lstm_layers', min_value=1, max_value=5, step=1)\n",
        "    for i in range(num_lstm_layers):\n",
        "        # LSTM units: use wider ranges (e.g., 32512)\n",
        "        lstm_units = hp.Int(f'lstm_units_{i}', min_value=32, max_value=512, step=32)\n",
        "        return_sequences = True if i < num_lstm_layers - 1 else False\n",
        "        x = LSTM(lstm_units, return_sequences=return_sequences)(x)\n",
        "        # Dropout: 0.10.5\n",
        "        x = Dropout(hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
        "    #residual skip: project the inputs global summary into the same dim as x\n",
        "    skip = Dense(K.int_shape(x)[-1])(\n",
        "        Lambda(lambda z: K.mean(z, axis=1))(model_input)\n",
        "    )\n",
        "    x = Add()([x, skip])\n",
        "\n",
        "    output_timesteps = output_dim // 2\n",
        "    # The final Dense layer's output is now constrained to [-1, 1] using tanh.\n",
        "    x = Dense(output_dim, activation='tanh')(x)\n",
        "    # This Lambda layer scales the tanh output to the correct lat/lon ranges.\n",
        "    # We create a scaling tensor and multiply the output by it.\n",
        "    scaling_factors = tf.constant([90.0, 180.0] * output_timesteps, dtype=tf.float32)\n",
        "    x = x * scaling_factors\n",
        "    # The output is then reshaped to match the target shape (timesteps, 2)\n",
        "    model_output = Reshape((output_timesteps, 2))(x)\n",
        "\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=1.0)\n",
        "    model.compile(optimizer=optimizer, loss=haversine_loss, metrics=[HaversineMetric()])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_gru_model(hp, input_shape, output_dim):\n",
        "    \"\"\"Builds a GRU model with tunable hyperparameters.\"\"\"\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    # Layers: search from 15\n",
        "    num_gru_layers = hp.Int('num_gru_layers', min_value=1, max_value=5, step=1)\n",
        "    for i in range(num_gru_layers):\n",
        "        # GRU units: use wider ranges (e.g., 32512)\n",
        "        gru_units = hp.Int(f'gru_units_{i}', min_value=32, max_value=512, step=32)\n",
        "        return_sequences = True if i < num_gru_layers - 1 else False\n",
        "        x = GRU(gru_units, return_sequences=return_sequences)(x)\n",
        "        # Dropout: 0.10.5\n",
        "        x = Dropout(hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
        "    #residual skip: project the inputs global summary into the same dim as x\n",
        "    skip = Dense(K.int_shape(x)[-1])(\n",
        "        Lambda(lambda z: K.mean(z, axis=1))(model_input)\n",
        "    )\n",
        "    x = Add()([x, skip])\n",
        "\n",
        "    output_timesteps = output_dim // 2\n",
        "    x = Dense(output_dim, activation='tanh')(x)\n",
        "    scaling_factors = tf.constant([90.0, 180.0] * output_timesteps, dtype=tf.float32)\n",
        "    x = x * scaling_factors\n",
        "    model_output = Reshape((output_timesteps, 2))(x)\n",
        "\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=1.0)\n",
        "    model.compile(optimizer=optimizer, loss=haversine_loss, metrics=[HaversineMetric()])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_bilstm_model(hp, input_shape, output_dim):\n",
        "    \"\"\"Builds a Bidirectional LSTM model with tunable hyperparameters.\"\"\"\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    # Layers: search from 15\n",
        "    num_bilstm_layers = hp.Int('num_bilstm_layers', min_value=1, max_value=5, step=1)\n",
        "    for i in range(num_bilstm_layers):\n",
        "        # BiLSTM units: use wider ranges (e.g., 32512)\n",
        "        bilstm_units = hp.Int(f'bilstm_units_{i}', min_value=32, max_value=512, step=32)\n",
        "        return_sequences = True if i < num_bilstm_layers - 1 else False\n",
        "        x = Bidirectional(LSTM(bilstm_units, return_sequences=return_sequences))(x)\n",
        "        # Dropout: 0.10.5\n",
        "        x = Dropout(hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
        "    #residual skip: project the inputs global summary into the same dim as x\n",
        "    skip = Dense(K.int_shape(x)[-1])(\n",
        "        Lambda(lambda z: K.mean(z, axis=1))(model_input)\n",
        "    )\n",
        "    x = Add()([x, skip])\n",
        "\n",
        "    output_timesteps = output_dim // 2\n",
        "    x = Dense(output_dim, activation='tanh')(x)\n",
        "    scaling_factors = tf.constant([90.0, 180.0] * output_timesteps, dtype=tf.float32)\n",
        "    x = x * scaling_factors\n",
        "    model_output = Reshape((output_timesteps, 2))(x)\n",
        "\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=1.0)\n",
        "    model.compile(optimizer=optimizer, loss=haversine_loss, metrics=[HaversineMetric()])\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_bilstm_attention_model(hp, input_shape, output_dim):\n",
        "    \"\"\"Builds a Bidirectional LSTM model with Attention mechanism.\"\"\"\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    # Layers: search from 15\n",
        "    num_bilstm_layers = hp.Int('num_bilstm_attention_layers', min_value=1, max_value=5, step=1)\n",
        "    for i in range(num_bilstm_layers):\n",
        "        # BiLSTM units: use wider ranges (e.g., 32512)\n",
        "        bilstm_units = hp.Int(f'bilstm_attention_units_{i}', min_value=32, max_value=512, step=32)\n",
        "        # Always return sequences for attention layer\n",
        "        x = Bidirectional(LSTM(bilstm_units, return_sequences=True))(x) # Output: (batch_size, timesteps, units*2)\n",
        "        # Dropout: 0.10.5\n",
        "        x = Dropout(hp.Float(f'dropout_attention_{i}', min_value=0.1, max_value=0.5, step=0.1))(x)\n",
        "\n",
        "    # Attention Mechanism\n",
        "    attention_scores = Dense(1, activation='tanh', name='attention_scores')(x)\n",
        "    attention_weights = layers.Activation('softmax', name='attention_weights')(attention_scores)\n",
        "    weighted_sequence = Multiply(name='weighted_sequence')([x, attention_weights])\n",
        "    context_vector = layers.Lambda(lambda z: tf.reduce_sum(z, axis=1), name='context_vector')(weighted_sequence)\n",
        "\n",
        "    #  Residual skip: project the inputs global summary into the same dim as context_vector \n",
        "    skip = Dense(K.int_shape(context_vector)[-1])(\n",
        "        Lambda(lambda z: K.mean(z, axis=1))(model_input))\n",
        "    merged = Add()([context_vector, skip])\n",
        "\n",
        "    output_timesteps = output_dim // 2\n",
        "    x = Dense(output_dim, activation='tanh')(merged)\n",
        "    scaling_factors = tf.constant([90.0, 180.0] * output_timesteps, dtype=tf.float32)\n",
        "    x = x * scaling_factors\n",
        "    model_output = Reshape((output_timesteps, 2))(x)\n",
        "\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipvalue=1.0)\n",
        "    model.compile(optimizer=optimizer, loss=haversine_loss, metrics=[HaversineMetric()])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- Self-testing block for models.py ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Running self-test for models.py...\")\n",
        "\n",
        "    class MockHyperParameters:\n",
        "        def Int(self, name, min_value, max_value, step):\n",
        "            if 'units' in name: return 64\n",
        "            if 'layers' in name: return 1\n",
        "            if 'batch_size' in name: return 32\n",
        "            return min_value\n",
        "        def Float(self, name, min_value, max_value, step=None, sampling=None):\n",
        "            if 'learning_rate' in name: return 0.001\n",
        "            return 0.2\n",
        "        def Choice(self, name, values, default=None):\n",
        "            return values[0]\n",
        "\n",
        "    hp = MockHyperParameters()\n",
        "    input_shape = (10, 7)\n",
        "    output_dim = 2 * 2\n",
        "\n",
        "    # Test LSTM Model\n",
        "    print(\"\\nTesting LSTM Model...\")\n",
        "    lstm_model = build_lstm_model(hp, input_shape, output_dim)\n",
        "    lstm_model.summary()\n",
        "    assert lstm_model.output_shape == (None, output_dim // 2, 2), f\"LSTM output shape mismatch: {lstm_model.output_shape}\"\n",
        "    print(\"LSTM Model test PASSED.\")\n",
        "\n",
        "    # Test GRU Model\n",
        "    print(\"\\nTesting GRU Model...\")\n",
        "    gru_model = build_gru_model(hp, input_shape, output_dim)\n",
        "    gru_model.summary()\n",
        "    assert gru_model.output_shape == (None, output_dim // 2, 2), f\"GRU output shape mismatch: {gru_model.output_shape}\"\n",
        "    print(\"GRU Model test PASSED.\")\n",
        "\n",
        "    # Test BiLSTM Model\n",
        "    print(\"\\nTesting BiLSTM Model...\")\n",
        "    bilstm_model = build_bilstm_model(hp, input_shape, output_dim)\n",
        "    bilstm_model.summary()\n",
        "    assert bilstm_model.output_shape == (None, output_dim // 2, 2), f\"BiLSTM output shape mismatch: {bilstm_model.output_shape}\"\n",
        "    print(\"BiLSTM Model test PASSED.\")\n",
        "\n",
        "    # Test BiLSTM with Attention Model\n",
        "    print(\"\\nTesting BiLSTM with Attention Model...\")\n",
        "    bilstm_attention_model = build_bilstm_attention_model(hp, input_shape, output_dim)\n",
        "    bilstm_attention_model.summary()\n",
        "    assert bilstm_attention_model.output_shape == (None, output_dim // 2, 2), f\"BiLSTM Attention output shape mismatch: {bilstm_attention_model.output_shape}\"\n",
        "    print(\"BiLSTM with Attention Model test PASSED.\")\n",
        "\n",
        "    print(\"\\nAll models.py self-tests PASSED!\")\n"
      ],
      "metadata": {
        "id": "U-c4MlZ9Upk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98329de-259b-435a-9231-a5c408070c81"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python models.py"
      ],
      "metadata": {
        "id": "GYGxHI5mYzpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b30f88f-30f0-447f-ff29-e1324d3b7d0b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-04 18:56:27.958372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754333787.976908   60867 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754333787.982476   60867 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Running self-test for models.py...\n",
            "\n",
            "Testing LSTM Model...\n",
            "2025-08-04 18:56:32.158839: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1754333792.158982   60867 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13838 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\u001b[1mModel: \"functional\"\u001b[0m\n",
            "\n",
            "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
            "\n",
            " input_layer          (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m7\u001b[0m)               \u001b[32m0\u001b[0m  -                 \n",
            " (\u001b[94mInputLayer\u001b[0m)                                                          \n",
            "\n",
            " lstm (\u001b[94mLSTM\u001b[0m)          (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)             \u001b[32m18,432\u001b[0m  input_layer[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m] \n",
            "\n",
            " lambda (\u001b[94mLambda\u001b[0m)      (\u001b[96mNone\u001b[0m, \u001b[32m7\u001b[0m)                   \u001b[32m0\u001b[0m  input_layer[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m] \n",
            "\n",
            " dropout (\u001b[94mDropout\u001b[0m)    (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)                  \u001b[32m0\u001b[0m  lstm[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]        \n",
            "\n",
            " dense (\u001b[94mDense\u001b[0m)        (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)                \u001b[32m512\u001b[0m  lambda[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]      \n",
            "\n",
            " add (\u001b[94mAdd\u001b[0m)            (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)                  \u001b[32m0\u001b[0m  dropout[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],    \n",
            "                                                     dense[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       \n",
            "\n",
            " dense_1 (\u001b[94mDense\u001b[0m)      (\u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m)                 \u001b[32m260\u001b[0m  add[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]         \n",
            "\n",
            " multiply (\u001b[94mMultiply\u001b[0m)  (\u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m)                   \u001b[32m0\u001b[0m  dense_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     \n",
            "\n",
            " reshape (\u001b[94mReshape\u001b[0m)    (\u001b[96mNone\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2\u001b[0m)                \u001b[32m0\u001b[0m  multiply[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    \n",
            "\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m19,204\u001b[0m (75.02 KB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m19,204\u001b[0m (75.02 KB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
            "LSTM Model test PASSED.\n",
            "\n",
            "Testing GRU Model...\n",
            "\u001b[1mModel: \"functional_1\"\u001b[0m\n",
            "\n",
            "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
            "\n",
            " input_layer_1        (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m7\u001b[0m)               \u001b[32m0\u001b[0m  -                 \n",
            " (\u001b[94mInputLayer\u001b[0m)                                                          \n",
            "\n",
            " gru (\u001b[94mGRU\u001b[0m)            (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)             \u001b[32m14,016\u001b[0m  input_layer_1[\u001b[32m0\u001b[0m] \n",
            "\n",
            " lambda_1 (\u001b[94mLambda\u001b[0m)    (\u001b[96mNone\u001b[0m, \u001b[32m7\u001b[0m)                   \u001b[32m0\u001b[0m  input_layer_1[\u001b[32m0\u001b[0m] \n",
            "\n",
            " dropout_1 (\u001b[94mDropout\u001b[0m)  (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)                  \u001b[32m0\u001b[0m  gru[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]         \n",
            "\n",
            " dense_2 (\u001b[94mDense\u001b[0m)      (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)                \u001b[32m512\u001b[0m  lambda_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    \n",
            "\n",
            " add_1 (\u001b[94mAdd\u001b[0m)          (\u001b[96mNone\u001b[0m, \u001b[32m64\u001b[0m)                  \u001b[32m0\u001b[0m  dropout_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],  \n",
            "                                                     dense_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     \n",
            "\n",
            " dense_3 (\u001b[94mDense\u001b[0m)      (\u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m)                 \u001b[32m260\u001b[0m  add_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       \n",
            "\n",
            " multiply_1           (\u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m)                   \u001b[32m0\u001b[0m  dense_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     \n",
            " (\u001b[94mMultiply\u001b[0m)                                                            \n",
            "\n",
            " reshape_1 (\u001b[94mReshape\u001b[0m)  (\u001b[96mNone\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2\u001b[0m)                \u001b[32m0\u001b[0m  multiply_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]  \n",
            "\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m14,788\u001b[0m (57.77 KB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m14,788\u001b[0m (57.77 KB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
            "GRU Model test PASSED.\n",
            "\n",
            "Testing BiLSTM Model...\n",
            "\u001b[1mModel: \"functional_2\"\u001b[0m\n",
            "\n",
            "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
            "\n",
            " input_layer_2        (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m7\u001b[0m)               \u001b[32m0\u001b[0m  -                 \n",
            " (\u001b[94mInputLayer\u001b[0m)                                                          \n",
            "\n",
            " bidirectional        (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)            \u001b[32m36,864\u001b[0m  input_layer_2[\u001b[32m0\u001b[0m] \n",
            " (\u001b[94mBidirectional\u001b[0m)                                                       \n",
            "\n",
            " lambda_2 (\u001b[94mLambda\u001b[0m)    (\u001b[96mNone\u001b[0m, \u001b[32m7\u001b[0m)                   \u001b[32m0\u001b[0m  input_layer_2[\u001b[32m0\u001b[0m] \n",
            "\n",
            " dropout_2 (\u001b[94mDropout\u001b[0m)  (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)                 \u001b[32m0\u001b[0m  bidirectional[\u001b[32m0\u001b[0m] \n",
            "\n",
            " dense_4 (\u001b[94mDense\u001b[0m)      (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)             \u001b[32m1,024\u001b[0m  lambda_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    \n",
            "\n",
            " add_2 (\u001b[94mAdd\u001b[0m)          (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)                 \u001b[32m0\u001b[0m  dropout_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],  \n",
            "                                                     dense_4[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     \n",
            "\n",
            " dense_5 (\u001b[94mDense\u001b[0m)      (\u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m)                 \u001b[32m516\u001b[0m  add_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       \n",
            "\n",
            " multiply_2           (\u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m)                   \u001b[32m0\u001b[0m  dense_5[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     \n",
            " (\u001b[94mMultiply\u001b[0m)                                                            \n",
            "\n",
            " reshape_2 (\u001b[94mReshape\u001b[0m)  (\u001b[96mNone\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2\u001b[0m)                \u001b[32m0\u001b[0m  multiply_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]  \n",
            "\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m38,404\u001b[0m (150.02 KB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m38,404\u001b[0m (150.02 KB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
            "BiLSTM Model test PASSED.\n",
            "\n",
            "Testing BiLSTM with Attention Model...\n",
            "\u001b[1mModel: \"functional_3\"\u001b[0m\n",
            "\n",
            "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
            "\n",
            " input_layer_3        (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m7\u001b[0m)               \u001b[32m0\u001b[0m  -                 \n",
            " (\u001b[94mInputLayer\u001b[0m)                                                          \n",
            "\n",
            " bidirectional_1      (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m128\u001b[0m)        \u001b[32m36,864\u001b[0m  input_layer_3[\u001b[32m0\u001b[0m] \n",
            " (\u001b[94mBidirectional\u001b[0m)                                                       \n",
            "\n",
            " dropout_3 (\u001b[94mDropout\u001b[0m)  (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m128\u001b[0m)             \u001b[32m0\u001b[0m  bidirectional_1[\u001b[32m\u001b[0m \n",
            "\n",
            " attention_scores     (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m1\u001b[0m)             \u001b[32m129\u001b[0m  dropout_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]   \n",
            " (\u001b[94mDense\u001b[0m)                                                               \n",
            "\n",
            " attention_weights    (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m1\u001b[0m)               \u001b[32m0\u001b[0m  attention_scores \n",
            " (\u001b[94mActivation\u001b[0m)                                                          \n",
            "\n",
            " weighted_sequence    (\u001b[96mNone\u001b[0m, \u001b[32m10\u001b[0m, \u001b[32m128\u001b[0m)             \u001b[32m0\u001b[0m  dropout_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],  \n",
            " (\u001b[94mMultiply\u001b[0m)                                          attention_weight \n",
            "\n",
            " lambda_3 (\u001b[94mLambda\u001b[0m)    (\u001b[96mNone\u001b[0m, \u001b[32m7\u001b[0m)                   \u001b[32m0\u001b[0m  input_layer_3[\u001b[32m0\u001b[0m] \n",
            "\n",
            " context_vector       (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)                 \u001b[32m0\u001b[0m  weighted_sequenc \n",
            " (\u001b[94mLambda\u001b[0m)                                                              \n",
            "\n",
            " dense_6 (\u001b[94mDense\u001b[0m)      (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)             \u001b[32m1,024\u001b[0m  lambda_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    \n",
            "\n",
            " add_3 (\u001b[94mAdd\u001b[0m)          (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)                 \u001b[32m0\u001b[0m  context_vector[\u001b[32m0\u001b[0m \n",
            "                                                     dense_6[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     \n",
            "\n",
            " dense_7 (\u001b[94mDense\u001b[0m)      (\u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m)                 \u001b[32m516\u001b[0m  add_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       \n",
            "\n",
            " multiply_3           (\u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m)                   \u001b[32m0\u001b[0m  dense_7[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     \n",
            " (\u001b[94mMultiply\u001b[0m)                                                            \n",
            "\n",
            " reshape_3 (\u001b[94mReshape\u001b[0m)  (\u001b[96mNone\u001b[0m, \u001b[32m2\u001b[0m, \u001b[32m2\u001b[0m)                \u001b[32m0\u001b[0m  multiply_3[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]  \n",
            "\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m38,533\u001b[0m (150.52 KB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m38,533\u001b[0m (150.52 KB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
            "BiLSTM with Attention Model test PASSED.\n",
            "\n",
            "All models.py self-tests PASSED!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hyperparameter_tuner.py\n",
        "# hyperparameter_tuner.py\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Ensure these imports correctly point to your modules\n",
        "from data_loader import DataLoader\n",
        "from metrics_losses import haversine_loss, HaversineMetric\n",
        "from models import (\n",
        "    build_lstm_model,\n",
        "    build_gru_model,\n",
        "    build_bilstm_model,\n",
        "    build_bilstm_attention_model\n",
        ")\n",
        "\n",
        "class HyperparamTuner:\n",
        "    \"\"\"\n",
        "    Performs hyperparameter tuning for LSTM/GRU models using KerasTuner.\n",
        "    It now dynamically computes sequence lengths and feature counts based on the\n",
        "    shape of the loaded data, preventing shape mismatch errors.\n",
        "    \"\"\"\n",
        "    def __init__(self, cli_args: Dict[str, Any]):\n",
        "        self.cli_args = cli_args\n",
        "        self.project_name = cli_args['tuning_project_name']\n",
        "        self.log_dir = Path(cli_args['tuning_results_dir'])\n",
        "\n",
        "        # NOTE: We no longer pre-calculate the shapes here.\n",
        "        # Instead, we will determine them dynamically once the data is loaded.\n",
        "        self.input_shape = None\n",
        "        self.output_dim = None\n",
        "\n",
        "        # Prepare logging directories\n",
        "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.all_trials_csv = self.log_dir / f\"{self.project_name}_all_trials.csv\"\n",
        "        self.best_params_json = self.log_dir / f\"{self.project_name}_best_params.json\"\n",
        "\n",
        "        print(f\"Tuner initialized for project: {self.project_name}\")\n",
        "        print(f\"Results will be saved to: {self.log_dir}\")\n",
        "        print(\"Model input/output shapes will be determined from loaded data.\")\n",
        "\n",
        "    def model_builder(self, hp: kt.HyperParameters) -> tf.keras.Model:\n",
        "        # Determine model type (fixed or sampled)\n",
        "        model_type = (\n",
        "            self.cli_args.get('fixed_model_type')\n",
        "            if 'fixed_model_type' in self.cli_args\n",
        "            else hp.Choice(\n",
        "                'model_type', ['lstm', 'gru', 'bilstm', 'bilstm_attention'], default='bilstm_attention'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Build selected architecture\n",
        "        if model_type == 'lstm':\n",
        "            model = build_lstm_model(hp, self.input_shape, self.output_dim)\n",
        "        elif model_type == 'gru':\n",
        "            model = build_gru_model(hp, self.input_shape, self.output_dim)\n",
        "        elif model_type == 'bilstm':\n",
        "            model = build_bilstm_model(hp, self.input_shape, self.output_dim)\n",
        "        else:\n",
        "            model = build_bilstm_attention_model(hp, self.input_shape, self.output_dim)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(\n",
        "                learning_rate=hp.Float('learning_rate', 1e-5, 1e-2, sampling='LOG', default=1e-3)\n",
        "            ),\n",
        "            loss=haversine_loss,\n",
        "            metrics=[HaversineMetric(name='haversine_metric')]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def run_tuning(self):\n",
        "        # Load and normalize data\n",
        "        print(\"Loading data for tuning...\")\n",
        "        dl = DataLoader(self.cli_args)\n",
        "        dl.load_data('train')\n",
        "        dl.load_data('val')\n",
        "        X_train, Y_train = dl.get_data('train')\n",
        "        X_val, Y_val     = dl.get_data('val')\n",
        "\n",
        "        # Dynamically set the input and output shapes based on the loaded data\n",
        "        self.input_shape = X_train.shape[1:]\n",
        "        self.output_dim = Y_train.shape[1] * Y_train.shape[2]\n",
        "\n",
        "        print(f\"Training data shape: X={X_train.shape}, Y={Y_train.shape}\")\n",
        "        print(f\"Validation data shape: X={X_val.shape}, Y={Y_val.shape}\")\n",
        "\n",
        "        # Final check for empty data after loading\n",
        "        if X_train.shape[0] == 0 or X_val.shape[0] == 0:\n",
        "            print(\"Error: Empty data. Aborting tuning.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Tuner is now configured with Input Shape: {self.input_shape}\")\n",
        "        print(f\"Tuner is now configured with Output Dimension: {self.output_dim}\")\n",
        "\n",
        "        all_models = ['lstm', 'gru', 'bilstm', 'bilstm_attention']\n",
        "        all_records = []\n",
        "\n",
        "        for arch in all_models:\n",
        "            print(f\"\\n=== Tuning {arch} ===\")\n",
        "            self.cli_args['fixed_model_type'] = arch\n",
        "            tuner = kt.RandomSearch(\n",
        "                self.model_builder,\n",
        "                objective=kt.Objective('val_haversine_metric', 'min'),\n",
        "                max_trials=self.cli_args.get('trials_per_arch', 5),\n",
        "                executions_per_trial=self.cli_args.get('executions_per_trial', 1),\n",
        "                directory=str(self.log_dir),\n",
        "                project_name=f\"{self.project_name}_{arch}\",\n",
        "                overwrite=False\n",
        "            )\n",
        "            tuner.search(\n",
        "                X_train, Y_train,\n",
        "                epochs=self.cli_args.get('epochs', 10),\n",
        "                validation_data=(X_val, Y_val),\n",
        "                batch_size=self.cli_args.get('batch_size', 32),\n",
        "                callbacks=[EarlyStopping(\n",
        "                    monitor='val_haversine_metric',\n",
        "                    patience=self.cli_args.get('patience', 5),\n",
        "                    mode='min',\n",
        "                    restore_best_weights=True\n",
        "                )],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Collect trial results\n",
        "            trials = tuner.oracle.get_best_trials(num_trials=len(tuner.oracle.trials))\n",
        "            for t in trials:\n",
        "                if t.score is not None:\n",
        "                    all_records.append({\n",
        "                        'model_type': arch,\n",
        "                        'trial_id': t.trial_id,\n",
        "                        **t.hyperparameters.values,\n",
        "                        'val_haversine_metric': t.score\n",
        "                    })\n",
        "            best_hp = tuner.get_best_hyperparameters(num_trials=1)\n",
        "            if best_hp:\n",
        "                print(f\"Best for {arch}: {best_hp[0].values}\")\n",
        "\n",
        "        if not all_records:\n",
        "            print(\"No trials recorded. Exiting.\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(all_records).sort_values('val_haversine_metric')\n",
        "        print(\"\\nAll Trials Summary:\")\n",
        "        print(df.to_string(index=False))\n",
        "        df.to_csv(self.all_trials_csv, index=False)\n",
        "        print(f\"Saved trials to {self.all_trials_csv}\")\n",
        "\n",
        "        #  NEW: pick the best hyperparameters *per* architecture \n",
        "        best_per_arch = {}\n",
        "        for arch in all_models:\n",
        "            arch_df = df[df['model_type'] == arch].sort_values('val_haversine_metric')\n",
        "            if not arch_df.empty:\n",
        "                row = arch_df.iloc[0]\n",
        "                # drop the columns we don't need downstream\n",
        "                params = row.drop(['model_type', 'trial_id', 'val_haversine_metric']).to_dict()\n",
        "                best_per_arch[arch] = params\n",
        "\n",
        "        # overwrite the JSON with a dict of {arch: best_params}\n",
        "        with open(self.best_params_json, 'w') as f:\n",
        "            json.dump(best_per_arch, f, indent=2)\n",
        "        print(f\"Saved best params for each arch to {self.best_params_json}\")\n",
        "\n",
        "        # Verify tuner directories\n",
        "        print(\"--- Verify tuner directories ---\")\n",
        "        for arch in all_models:\n",
        "            proj = self.log_dir / f\"{self.project_name}_{arch}\"\n",
        "            if proj.is_dir():\n",
        "                trials = [d for d in os.listdir(proj) if d.startswith('trial_')]\n",
        "                print(f\"{arch} trials: {trials}\")\n",
        "            else:\n",
        "                print(f\"Missing directory for {arch}: {proj}\")\n",
        "        print(\"--- Done ---\")\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    args = {\n",
        "        'base_data_path': '/content/data/new_york/formatted_debug',\n",
        "        'cache_base_dir': '/content/data/new_york/dataloader_cache',\n",
        "        'tuning_results_dir': './tuner_logs',\n",
        "        'tuning_project_name': 'ship_pred_tuning',\n",
        "        'time_gap': '30',\n",
        "        'interpolation_time_gap_minutes': '5',\n",
        "        #'num_features': 49, # This will be overridden dynamically\n",
        "        'trials_per_arch': 5,\n",
        "        'executions_per_trial': 1,\n",
        "        'epochs': 15,\n",
        "        #'batch_size': 16,\n",
        "        'patience': 5\n",
        "    }\n",
        "\n",
        "    print(\"Initializing Hyperparameter Tuner...\")\n",
        "    tuner = HyperparamTuner(args)\n",
        "    tuner.run_tuning()\n"
      ],
      "metadata": {
        "id": "-uREGfriU7Nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0d9760-5119-40e6-b919-147b4dbca2ea"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hyperparameter_tuner.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the tuner_logs directory\n",
        "!zip -r tuner_logs.zip tuner_logs\n",
        "\n",
        "# Zip the dataloader_cache directory.\n",
        "!zip -r dataloader_cache.zip /content/data/new_york/dataloader_cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLoD9DlJpYgF",
        "outputId": "9d4841e7-8a22-4a26-f5ab-d2d999768de9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: tuner_logs\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r tuner_logs.zip . -i tuner_logs)\n",
            "  adding: content/data/new_york/dataloader_cache/ (stored 0%)\n",
            "  adding: content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9/ (stored 0%)\n",
            "  adding: content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9/normalization_factors.json (deflated 92%)\n",
            "  adding: content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9/val_Y.npy (deflated 78%)\n",
            "  adding: content/data/new_york/dataloader_cache/cb5170adc6753993f2abbdd88ef376a9/train_Y.npy\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python hyperparameter_tuner.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOoxW8hYkkzL",
        "outputId": "48afc94a-776c-4eb9-a7e8-9fff73493c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-04 18:56:43.454980: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754333803.476425   61009 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754333803.482752   61009 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Initializing Hyperparameter Tuner...\n",
            "Tuner initialized for project: ship_pred_tuning\n",
            "Results will be saved to: tuner_logs\n",
            "Model input/output shapes will be determined from loaded data.\n",
            "Loading data for tuning...\n",
            "DataLoader: Loaded features metadata from /content/data/new_york/formatted_debug/features.csv\n",
            "DataLoader: Loaded normalization factors from cache: /content/data/new_york/dataloader_cache/6b1eca834eee711d9ac8147edb69fcff/normalization_factors.json\n",
            "DataLoader: Determined consistent cache directory: /content/data/new_york/dataloader_cache/6b1eca834eee711d9ac8147edb69fcff\n",
            "\n",
            "Loading data for 'train' (force_rebuild: False)...\n",
            "    Attempting to load from cache: /content/data/new_york/dataloader_cache/6b1eca834eee711d9ac8147edb69fcff/train_X.npy\n",
            "    Successfully loaded 'train' data from cache.\n",
            "\n",
            "Loading data for 'val' (force_rebuild: False)...\n",
            "    Attempting to load from cache: /content/data/new_york/dataloader_cache/6b1eca834eee711d9ac8147edb69fcff/val_X.npy\n",
            "    Successfully loaded 'val' data from cache.\n",
            "Training data shape: X=(1084072, 37, 51), Y=(1084072, 6, 2)\n",
            "Validation data shape: X=(277431, 37, 51), Y=(277431, 6, 2)\n",
            "Tuner is now configured with Input Shape: (37, 51)\n",
            "Tuner is now configured with Output Dimension: 12\n",
            "\n",
            "=== Tuning lstm ===\n",
            "Reloading Tuner from tuner_logs/ship_pred_tuning_lstm/tuner0.json\n",
            "\n",
            "Search: Running Trial #2\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "3                 |4                 |num_lstm_layers\n",
            "512               |256               |lstm_units_0\n",
            "0.2               |0.3               |dropout_0\n",
            "0.00056897        |0.00016724        |learning_rate\n",
            "448               |32                |lstm_units_1\n",
            "0.5               |0.1               |dropout_1\n",
            "288               |32                |lstm_units_2\n",
            "0.1               |0.1               |dropout_2\n",
            "32                |32                |lstm_units_3\n",
            "0.2               |0.1               |dropout_3\n",
            "\n",
            "2025-08-04 18:56:57.532147: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "I0000 00:00:1754333817.532290   61009 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13838 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2025-08-04 18:57:01.366117: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 8182575456 exceeds 10% of free system memory.\n",
            "2025-08-04 18:57:08.805624: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 8182575456 exceeds 10% of free system memory.\n",
            "Epoch 1/15\n",
            "I0000 00:00:1754333837.569427   61132 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
            "\u001b[1m11869/33878\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6:35\u001b[0m 18ms/step - haversine_metric: 8498.8848 - loss: 8498.8662"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil, os\n",
        "\n",
        "# 1 Mount Google Drive (remount if needed)\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# 2 Define source folder and desired zip name\n",
        "src_path = '/content/tuner_logs'\n",
        "zip_name = 'tuner_logs'   # resulting archive will be zip_name.zip\n",
        "\n",
        "# 3 Set destination directory in your Drive\n",
        "dst_dir = '/content/drive/MyDrive'\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "dst_base = os.path.join(dst_dir, zip_name)\n",
        "\n",
        "# 4 Create the zip archive\n",
        "shutil.make_archive(base_name=dst_base, format='zip', root_dir=src_path)\n",
        "\n",
        "print(f\" Zipped {src_path}  {dst_base}.zip in your MyDrive.\")\n"
      ],
      "metadata": {
        "id": "D7eCAsnrkt2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_and_evaluate.py\n",
        "# train_and_evaluate.py - Final version with corrected model-building and loss logic\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Bidirectional, Layer, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import time # Import the time module\n",
        "\n",
        "# Assume these are available in the working directory\n",
        "from data_loader import DataLoader\n",
        "from metrics_losses import haversine_loss, HaversineMetric, haversine_distance\n",
        "\n",
        "# --- Corrected Model-Building Functions ---\n",
        "# These functions now accept a dictionary `hps` and access values directly.\n",
        "\n",
        "def build_lstm_model(hps: Dict[str, Any], input_shape: tuple, output_dim: int) -> tf.keras.Model:\n",
        "    \"\"\"Builds an LSTM model using hyperparameters from a dictionary.\"\"\"\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    num_lstm_layers = int(hps.get('num_lstm_layers', 1))\n",
        "    for i in range(num_lstm_layers):\n",
        "        lstm_units = int(hps.get(f'lstm_units_{i}', 32))\n",
        "        dropout_rate = float(hps.get(f'dropout_{i}', 0.0))\n",
        "        return_sequences = (i < num_lstm_layers - 1)\n",
        "        x = LSTM(units=lstm_units, return_sequences=return_sequences)(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    model_output = Dense(output_dim, activation='linear')(x)\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "    return model\n",
        "\n",
        "def build_gru_model(hps: Dict[str, Any], input_shape: tuple, output_dim: int) -> tf.keras.Model:\n",
        "    \"\"\"Builds a GRU model using hyperparameters from a dictionary.\"\"\"\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    num_gru_layers = int(hps.get('num_gru_layers', 1))\n",
        "    for i in range(num_gru_layers):\n",
        "        gru_units = int(hps.get(f'gru_units_{i}', 32))\n",
        "        dropout_rate = float(hps.get(f'dropout_{i}', 0.0))\n",
        "        return_sequences = (i < num_gru_layers - 1)\n",
        "        x = GRU(units=gru_units, return_sequences=return_sequences)(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    model_output = Dense(output_dim, activation='linear')(x)\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "    return model\n",
        "\n",
        "def build_bilstm_model(hps: Dict[str, Any], input_shape: tuple, output_dim: int) -> tf.keras.Model:\n",
        "    \"\"\"Builds a Bidirectional LSTM model using hyperparameters from a dictionary.\"\"\"\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    num_bilstm_layers = int(hps.get('num_bilstm_layers', 1))\n",
        "    for i in range(num_bilstm_layers):\n",
        "        bilstm_units = int(hps.get(f'bilstm_units_{i}', 32))\n",
        "        dropout_rate = float(hps.get(f'dropout_{i}', 0.0))\n",
        "        return_sequences = (i < num_bilstm_layers - 1)\n",
        "        x = Bidirectional(LSTM(units=bilstm_units, return_sequences=return_sequences))(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    model_output = Dense(output_dim, activation='linear')(x)\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "    return model\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    Custom attention layer for the BiLSTM with Attention model.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], input_shape[-1]),\n",
        "                                     initializer=\"glorot_uniform\", trainable=True)\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-1],),\n",
        "                                     initializer=\"zeros\", trainable=True)\n",
        "        self.u = self.add_weight(name=\"att_context\", shape=(input_shape[-1],),\n",
        "                                     initializer=\"glorot_uniform\", trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        u_prime = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        alpha = K.softmax(K.dot(u_prime, K.expand_dims(self.u)))\n",
        "        output = K.sum(x * alpha, axis=1)\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "def build_bilstm_attention_model(hps: Dict[str, Any], input_shape: tuple, output_dim: int) -> tf.keras.Model:\n",
        "    \"\"\"Builds a Bidirectional LSTM with Attention model using hyperparameters from a dictionary.\"\"\"\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    num_bilstm_layers = int(hps.get('num_bilstm_attention_layers', 1))\n",
        "    for i in range(num_bilstm_layers):\n",
        "        bilstm_units = int(hps.get(f'bilstm_attention_units_{i}', 32))\n",
        "        return_sequences = (i < num_bilstm_layers - 1) or True # The last BiLSTM must return sequences for the attention layer\n",
        "        x = Bidirectional(LSTM(units=bilstm_units, return_sequences=return_sequences))(x)\n",
        "        dropout_rate = float(hps.get(f'dropout_attention_{i}', 0.0))\n",
        "        if dropout_rate > 0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    attention_output = AttentionLayer()(x)\n",
        "    model_output = Dense(output_dim, activation='linear')(attention_output)\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "    return model\n",
        "\n",
        "# Custom objects dictionary for loading models\n",
        "custom_objects = {\n",
        "    'haversine_loss': haversine_loss,\n",
        "    'HaversineMetric': HaversineMetric,\n",
        "    'AttentionLayer': AttentionLayer,\n",
        "}\n",
        "\n",
        "\n",
        "class TrainAndEvaluateAllModels:\n",
        "    \"\"\"\n",
        "    A class to train and evaluate all model types (LSTM, GRU, BiLSTM, BiLSTM_Attention)\n",
        "    using their individually best hyperparameters found by the hyperparameter tuner.\n",
        "    \"\"\"\n",
        "    def __init__(self, cli_args: Dict[str, Any]):\n",
        "        self.cli_args = cli_args\n",
        "\n",
        "        # Define paths for loading and saving\n",
        "        self.tuning_results_dir = Path(cli_args['tuning_results_dir'])\n",
        "        self.best_params_path = self.tuning_results_dir / f\"{cli_args['tuning_project_name']}_best_params.json\"\n",
        "\n",
        "        self.model_save_dir = Path(cli_args['model_save_dir'])\n",
        "        self.model_save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.evaluation_report_dir = Path(cli_args['evaluation_report_dir'])\n",
        "        self.evaluation_report_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Calculate input_sequence_length based on 3 hours history and interpolation_time_gap\n",
        "        length_of_history_seconds = 3 * 60 * 60 # 3 hours\n",
        "        # The time_gap is now an integer, so we can use it directly\n",
        "        time_gap_seconds = self.cli_args['time_gap'] * 60\n",
        "        calculated_input_sequence_length = int(length_of_history_seconds / time_gap_seconds) + 1\n",
        "        self.cli_args['input_sequence_length'] = calculated_input_sequence_length\n",
        "\n",
        "        # input_shape: (timesteps_history, num_features)\n",
        "        self.input_shape = (\n",
        "            self.cli_args['input_sequence_length'],\n",
        "            self.cli_args['num_features']\n",
        "        )\n",
        "        # output_dim: total predicted values (timesteps * 2 for lat/lon)\n",
        "        self.output_dim = self.cli_args['output_sequence_length'] * 2\n",
        "\n",
        "        self.model_types = ['lstm', 'gru', 'bilstm', 'bilstm_attention']\n",
        "\n",
        "        print(\"Training and Evaluation initialized to run for all model types.\")\n",
        "        print(f\"Loading best hyperparameters per model from: {self.best_params_path}\")\n",
        "        print(f\"Trained models will be saved to: {self.model_save_dir}\")\n",
        "        print(f\"Evaluation reports will be saved to: {self.evaluation_report_dir}\")\n",
        "\n",
        "    def load_best_hyperparameters_per_model(self) -> Dict[str, Any]:\n",
        "        try:\n",
        "            with open(self.best_params_path, 'r') as f:\n",
        "                best_hps_per_model = json.load(f)\n",
        "            #  Inject model_type into each hps dict \n",
        "            for model_type, hps in best_hps_per_model.items():\n",
        "                hps['model_type'] = model_type\n",
        "            print(f\"Loaded best hyperparameters from {self.best_params_path}\")\n",
        "            return best_hps_per_model\n",
        "        except FileNotFoundError:\n",
        "            print(f\"ERROR: Best-params JSON not found at {self.best_params_path}\")\n",
        "            raise\n",
        "\n",
        "    def load_and_prepare_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Loads and returns the train, validation, and test datasets.\n",
        "        \"\"\"\n",
        "        print(\"Loading data for training and evaluation...\")\n",
        "        # The DataLoader requires a 'time_gap' key in cli_args, which is a string.\n",
        "        # This has been fixed to an integer, so we must convert it back to a string for DataLoader.\n",
        "        # Note: The original override has been removed, so this uses the correct `time_gap` value.\n",
        "        self.cli_args['time_gap'] = str(self.cli_args['time_gap'])\n",
        "\n",
        "        dl = DataLoader(self.cli_args)\n",
        "        dl.load_data('train')\n",
        "        dl.load_data('val')\n",
        "        dl.load_data('test')\n",
        "\n",
        "        X_train, Y_train = dl.get_data('train')\n",
        "        X_val, Y_val = dl.get_data('val')\n",
        "        X_test, Y_test = dl.get_data('test')\n",
        "\n",
        "        print(f\"Training data shape: X={X_train.shape}, Y={Y_train.shape}\")\n",
        "        print(f\"Validation data shape: X={X_val.shape}, Y={Y_val.shape}\")\n",
        "        print(f\"Test data shape: X={X_test.shape}, Y={Y_test.shape}\")\n",
        "\n",
        "        return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
        "\n",
        "    def build_model(self, hps: Dict[str, Any]) -> tf.keras.Model:\n",
        "        \"\"\"Builds a Keras model using the provided hyperparameters.\"\"\"\n",
        "        model_type = hps.get('model_type')\n",
        "        print(f\"Building {model_type} model with best hyperparameters...\")\n",
        "\n",
        "        if model_type == 'lstm':\n",
        "            model = build_lstm_model(hps, self.input_shape, self.output_dim)\n",
        "        elif model_type == 'gru':\n",
        "            model = build_gru_model(hps, self.input_shape, self.output_dim)\n",
        "        elif model_type == 'bilstm':\n",
        "            model = build_bilstm_model(hps, self.input_shape, self.output_dim)\n",
        "        elif model_type == 'bilstm_attention':\n",
        "            model = build_bilstm_attention_model(hps, self.input_shape, self.output_dim)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=hps.get('learning_rate', 1e-3)),\n",
        "            loss=haversine_loss,\n",
        "            metrics=[HaversineMetric(name='haversine_metric')]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def train_model(self, model: tf.keras.Model, X_train: np.ndarray, Y_train: np.ndarray, X_val: np.ndarray, Y_val: np.ndarray, hps: Dict[str, Any]) -> None:\n",
        "        \"\"\"Trains the model with early stopping.\"\"\"\n",
        "        model_type = hps['model_type']\n",
        "        print(f\"Training {model_type} model...\")\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_haversine_metric',\n",
        "            patience=int(hps.get('patience', 10)),\n",
        "            mode='min',\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "             monitor='val_haversine_metric',\n",
        "             factor=0.5,\n",
        "             patience=5,\n",
        "             min_lr=1e-6,\n",
        "             verbose=1\n",
        "        )\n",
        "\n",
        "        start_time = time.time() # Start time measurement\n",
        "        model.fit(\n",
        "            X_train, Y_train,\n",
        "            epochs=int(hps.get('epochs', 100)),\n",
        "            validation_data=(X_val, Y_val),\n",
        "            batch_size=int(hps.get('batch_size', 32)),\n",
        "            callbacks=[early_stopping, reduce_lr],\n",
        "            verbose=1\n",
        "        )\n",
        "        end_time = time.time() # End time measurement\n",
        "        training_time = end_time - start_time\n",
        "        print(f\"{model_type} model training finished in {training_time:.2f} seconds.\")\n",
        "\n",
        "        # Store training time and parameter count in the HPS dictionary or a separate results dictionary\n",
        "        hps['training_time_seconds'] = training_time\n",
        "        hps['num_parameters'] = model.count_params() # Capture parameter count here\n",
        "        print(f\"Model Parameters: {hps['num_parameters']}\")\n",
        "\n",
        "        # Save the trained model\n",
        "        model_path = self.model_save_dir / f\"{model_type}_best_model.h5\"\n",
        "        model.save(model_path)\n",
        "        print(f\"Trained model saved to: {model_path}\")\n",
        "\n",
        "        # Capture model file size after saving\n",
        "        hps['model_file_size_bytes'] = os.path.getsize(model_path)\n",
        "        print(f\"Model File Size: {hps['model_file_size_bytes']} bytes\")\n",
        "\n",
        "\n",
        "    def evaluate_metrics(self, model: tf.keras.Model, X_test: np.ndarray, Y_test: np.ndarray, model_type: str, training_time: float, num_parameters: int, model_file_size_bytes: int) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Performs a full evaluation on the test set, calculating all the metrics\n",
        "        specified in the paper.\n",
        "        \"\"\"\n",
        "        print(f\"\\nEvaluating {model_type} model on test data...\")\n",
        "\n",
        "        y_pred = model.predict(X_test, verbose=1)\n",
        "\n",
        "        Y_test_reshaped = Y_test.reshape(Y_test.shape[0], -1, 2)\n",
        "        y_pred_reshaped = y_pred.reshape(y_pred.shape[0], -1, 2)\n",
        "\n",
        "        num_timesteps = Y_test_reshaped.shape[1]\n",
        "\n",
        "        # We now use the correct `time_gap` from the cli_args dictionary\n",
        "        # This value is now an integer, as requested\n",
        "        time_gap_minutes = self.cli_args.get('time_gap', 30)\n",
        "\n",
        "        evaluation_results = {\n",
        "            \"model_type\": model_type,\n",
        "            \"training_time_seconds\": training_time,\n",
        "            \"num_parameters\": num_parameters, # Add parameter count here\n",
        "            \"model_file_size_bytes\": model_file_size_bytes # Add file size here\n",
        "        }\n",
        "\n",
        "        # --- Haversine-based Metrics ---\n",
        "        all_haversine_errors = haversine_distance(Y_test_reshaped, y_pred_reshaped)\n",
        "\n",
        "        # Correctly calculate metrics for 1h, 2h, and 3h horizons\n",
        "        # Forecasts are at 30, 60, 90, 120, 150, 180 minutes\n",
        "        # Index 1 corresponds to 60 min (1h)\n",
        "        # Index 3 corresponds to 120 min (2h)\n",
        "        # Index 5 corresponds to 180 min (3h)\n",
        "\n",
        "        # Mean Haversine error across all predictions\n",
        "        evaluation_results[\"mean_haversine_error_overall_km\"] = float(np.mean(all_haversine_errors))\n",
        "        evaluation_results[\"std_haversine_error_overall_km\"] = float(np.std(all_haversine_errors))\n",
        "        evaluation_results[\"median_haversine_error_overall_km\"] = float(np.median(all_haversine_errors))\n",
        "        evaluation_results[\"max_haversine_error_overall_km\"] = float(np.max(all_haversine_errors))\n",
        "\n",
        "        # RMSE Haversine\n",
        "        evaluation_results[\"rmse_haversine_overall_km\"] = float(np.sqrt(np.mean(np.square(all_haversine_errors))))\n",
        "\n",
        "\n",
        "        # Per-horizon Haversine errors\n",
        "        for i in range(num_timesteps):\n",
        "            horizon_minutes = (i + 1) * time_gap_minutes\n",
        "            horizon_error = all_haversine_errors[:, i]\n",
        "            evaluation_results[f\"mean_haversine_error_{horizon_minutes}min_km\"] = float(np.mean(horizon_error))\n",
        "            evaluation_results[f\"std_haversine_error_{horizon_minutes}min_km\"] = float(np.std(horizon_error))\n",
        "            evaluation_results[f\"median_haversine_error_{horizon_minutes}min_km\"] = float(np.median(horizon_error))\n",
        "            evaluation_results[f\"max_haversine_error_{horizon_minutes}min_km\"] = float(np.max(horizon_error))\n",
        "            evaluation_results[f\"rmse_haversine_{horizon_minutes}min_km\"] = float(np.sqrt(np.mean(np.square(horizon_error))))\n",
        "\n",
        "        # Specific 1h, 2h, 3h horizons if they exist\n",
        "        if time_gap_minutes > 0: # Avoid division by zero\n",
        "            if (60 / time_gap_minutes) - 1 < num_timesteps and (60 % time_gap_minutes == 0): # Check if 1h horizon exists\n",
        "                idx_1h = (60 // time_gap_minutes) - 1\n",
        "                horizon_error_1h = all_haversine_errors[:, idx_1h]\n",
        "                evaluation_results[\"mean_haversine_error_1h_km\"] = float(np.mean(horizon_error_1h))\n",
        "                evaluation_results[\"rmse_haversine_1h_km\"] = float(np.sqrt(np.mean(np.square(horizon_error_1h))))\n",
        "\n",
        "            if (120 / time_gap_minutes) - 1 < num_timesteps and (120 % time_gap_minutes == 0): # Check if 2h horizon exists\n",
        "                idx_2h = (120 // time_gap_minutes) - 1\n",
        "                horizon_error_2h = all_haversine_errors[:, idx_2h]\n",
        "                evaluation_results[\"mean_haversine_error_2h_km\"] = float(np.mean(horizon_error_2h))\n",
        "                evaluation_results[\"rmse_haversine_2h_km\"] = float(np.sqrt(np.mean(np.square(horizon_error_2h))))\n",
        "\n",
        "            if (180 / time_gap_minutes) - 1 < num_timesteps and (180 % time_gap_minutes == 0): # Check if 3h horizon exists\n",
        "                idx_3h = (180 // time_gap_minutes) - 1\n",
        "                horizon_error_3h = all_haversine_errors[:, idx_3h]\n",
        "                evaluation_results[\"mean_haversine_error_3h_km\"] = float(np.mean(horizon_error_3h))\n",
        "                evaluation_results[\"rmse_haversine_3h_km\"] = float(np.sqrt(np.mean(np.square(horizon_error_3h))))\n",
        "\n",
        "\n",
        "        # --- Coordinate-based Metrics (using Lat/Lon directly) ---\n",
        "        y_test_flat = Y_test_reshaped.reshape(-1, 2)\n",
        "        y_pred_flat = y_pred_reshaped.reshape(-1, 2)\n",
        "\n",
        "        lat_true = y_test_flat[:, 0]\n",
        "        lat_pred = y_pred_flat[:, 0]\n",
        "        lon_true = y_test_flat[:, 1]\n",
        "        lon_pred = y_pred_flat[:, 1]\n",
        "\n",
        "        # Latitude Metrics\n",
        "        evaluation_results[\"lat_mae_deg\"] = float(mean_absolute_error(lat_true, lat_pred))\n",
        "        evaluation_results[\"lat_mse_deg\"] = float(mean_squared_error(lat_true, lat_pred))\n",
        "        evaluation_results[\"lat_rmse_deg\"] = float(np.sqrt(evaluation_results[\"lat_mse_deg\"]))\n",
        "        evaluation_results[\"lat_r2\"] = float(r2_score(lat_true, lat_pred))\n",
        "        evaluation_results[\"lat_median_abs_error_deg\"] = float(np.median(np.abs(lat_true - lat_pred)))\n",
        "        evaluation_results[\"lat_max_abs_error_deg\"] = float(np.max(np.abs(lat_true - lat_pred)))\n",
        "\n",
        "        # Longitude Metrics\n",
        "        evaluation_results[\"lon_mae_deg\"] = float(mean_absolute_error(lon_true, lon_pred))\n",
        "        evaluation_results[\"lon_mse_deg\"] = float(mean_squared_error(lon_true, lon_pred))\n",
        "        evaluation_results[\"lon_rmse_deg\"] = float(np.sqrt(evaluation_results[\"lon_mse_deg\"]))\n",
        "        evaluation_results[\"lon_r2\"] = float(r2_score(lon_true, lon_pred))\n",
        "        evaluation_results[\"lon_median_abs_error_deg\"] = float(np.median(np.abs(lon_true - lon_pred)))\n",
        "        evaluation_results[\"lon_max_abs_error_deg\"] = float(np.max(np.abs(lon_true - lon_pred)))\n",
        "\n",
        "        # Average Coordinate Metrics\n",
        "        evaluation_results[\"avg_mae_deg\"] = float((evaluation_results[\"lat_mae_deg\"] + evaluation_results[\"lon_mae_deg\"]) / 2)\n",
        "        evaluation_results[\"avg_mse_deg\"] = float((evaluation_results[\"lat_mse_deg\"] + evaluation_results[\"lon_mse_deg\"]) / 2)\n",
        "        evaluation_results[\"avg_rmse_deg\"] = float((evaluation_results[\"lat_rmse_deg\"] + evaluation_results[\"lon_rmse_deg\"]) / 2)\n",
        "        evaluation_results[\"avg_r2\"] = float((evaluation_results[\"lat_r2\"] + evaluation_results[\"lon_r2\"]) / 2)\n",
        "        evaluation_results[\"avg_median_abs_error_deg\"] = float((evaluation_results[\"lat_median_abs_error_deg\"] + evaluation_results[\"lon_median_abs_error_deg\"]) / 2)\n",
        "        evaluation_results[\"avg_max_abs_error_deg\"] = float((evaluation_results[\"lat_max_abs_error_deg\"] + evaluation_results[\"lon_max_abs_error_deg\"]) / 2)\n",
        "\n",
        "\n",
        "        print(f\"\\n--- {model_type.upper()} Evaluation Complete ---\")\n",
        "        for k, v in evaluation_results.items():\n",
        "            print(f\"{k:<40}: {v:.6f}\") # Increased padding for better alignment\n",
        "\n",
        "        return evaluation_results\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Orchestrates the entire training and evaluation process for all models.\"\"\"\n",
        "        try:\n",
        "            best_hps_per_model = self.load_best_hyperparameters_per_model()\n",
        "            X_train, Y_train, X_val, Y_val, X_test, Y_test = self.load_and_prepare_data()\n",
        "\n",
        "            for model_type, hps in best_hps_per_model.items():\n",
        "                print(f\"\\n--- Starting process for model: {model_type} ---\")\n",
        "                model = self.build_model(hps)\n",
        "\n",
        "                # train_model now stores training time, parameter count, and model file size in hps\n",
        "                self.train_model(model, X_train, Y_train, X_val, Y_val, hps)\n",
        "\n",
        "                # Use a fresh instance of the model with restored weights for evaluation\n",
        "                best_model_path = self.model_save_dir / f\"{model_type}_best_model.h5\"\n",
        "                best_model = tf.keras.models.load_model(best_model_path, custom_objects=custom_objects)\n",
        "\n",
        "                # Pass the training time, num_parameters, and model_file_size_bytes to evaluate_metrics\n",
        "                evaluation_results = self.evaluate_metrics(\n",
        "                    best_model,\n",
        "                    X_test,\n",
        "                    Y_test,\n",
        "                    model_type,\n",
        "                    hps.get('training_time_seconds', 0.0),\n",
        "                    hps.get('num_parameters', 0), # Pass num_parameters\n",
        "                    hps.get('model_file_size_bytes', 0) # Pass model_file_size_bytes\n",
        "                )\n",
        "\n",
        "                # Save the final evaluation report to a JSON file\n",
        "                evaluation_report_path = self.evaluation_report_dir / f\"{model_type}_evaluation_report.json\"\n",
        "                with open(evaluation_report_path, 'w') as f:\n",
        "                    json.dump(evaluation_results, f, indent=4)\n",
        "                print(f\"\\nEvaluation report saved to: {evaluation_report_path}\")\n",
        "                print(f\"--- Process for model {model_type} completed. ---\\n\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during training or evaluation: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    real_data_args = {\n",
        "        'base_data_path': '/content/data/new_york/formatted_debug',\n",
        "        # Fixed typo in the cache directory path\n",
        "        'cache_base_dir': '/content/data/new_york/dataloader_cache',\n",
        "        'tuning_results_dir': '/content/tuner_logs',\n",
        "        # FIX: Changed the tuning_project_name to match the user's provided file path.\n",
        "        'tuning_project_name': 'ship_pred_tuning',\n",
        "        'model_save_dir': './trained_models',\n",
        "        'evaluation_report_dir': './evaluation_reports',\n",
        "        'output_sequence_length': 6,\n",
        "        'num_features': 49,\n",
        "        # Changed time_gap to an integer as requested\n",
        "        'time_gap': 30,\n",
        "        'epochs': 10,\n",
        "        'batch_size': 16,\n",
        "        'patience': 4,\n",
        "    }\n",
        "\n",
        "    print(\"--- Initializing Training and Evaluation Script for All Models ---\")\n",
        "    trainer = TrainAndEvaluateAllModels(real_data_args)\n",
        "    print(\"\\n--- Running Training and Evaluation for All Models ---\")\n",
        "    trainer.run()\n",
        "    print(\"Training and evaluation process for all models completed.\")"
      ],
      "metadata": {
        "id": "-BlP4hzXArk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_and_evaluate.py"
      ],
      "metadata": {
        "id": "XFgTjXkeAeP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def benchmark_models():\n",
        "    \"\"\"\n",
        "    Reads the evaluation reports for each model and presents a benchmark\n",
        "    of their most significant performance metrics in a concise table.\n",
        "    \"\"\"\n",
        "    # Define the directory where the evaluation reports are saved\n",
        "    evaluation_report_dir = \"./evaluation_reports\"\n",
        "\n",
        "    # List of models to benchmark\n",
        "    models = [\"lstm\", \"gru\", \"bilstm\", \"bilstm_attention\"]\n",
        "\n",
        "    benchmark_data: List[Dict[str, Any]] = []\n",
        "\n",
        "    print(\"--- Starting Model Performance Benchmark ---\")\n",
        "\n",
        "    # Iterate through each model to load its evaluation report\n",
        "    for model_name in models:\n",
        "        report_file_path = os.path.join(evaluation_report_dir, f\"{model_name}_evaluation_report.json\")\n",
        "\n",
        "        # Check if the evaluation report file exists\n",
        "        if not os.path.exists(report_file_path):\n",
        "            print(f\"Warning: Evaluation report for {model_name} not found at {report_file_path}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(report_file_path, 'r') as f:\n",
        "                report_data = json.load(f)\n",
        "\n",
        "            # Extract only the key metrics for a less wide table\n",
        "            metrics = {\n",
        "                \"Model\": model_name,\n",
        "                \"Median Haversine Error (m)\": report_data.get(\"median_haversine_error\", \"N/A\"),\n",
        "                \"Mean Haversine Error (1h)\": report_data.get(\"mean_haversine_error_1h\", \"N/A\"),\n",
        "                \"Avg RMSE (deg)\": report_data.get(\"avg_rmse_deg\", \"N/A\"),\n",
        "                \"Avg MSE (deg)\": report_data.get(\"avg_mse_deg\", \"N/A\"),\n",
        "                \"Avg R2 Score\": report_data.get(\"avg_r2\", \"N/A\")\n",
        "            }\n",
        "            benchmark_data.append(metrics)\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: Could not decode JSON from file {report_file_path}\")\n",
        "            continue\n",
        "\n",
        "    # Create a pandas DataFrame from the collected data\n",
        "    if benchmark_data:\n",
        "        benchmark_df = pd.DataFrame(benchmark_data)\n",
        "        benchmark_df.set_index(\"Model\", inplace=True)\n",
        "\n",
        "        print(\"\\n--- Model Performance Comparison (Key Metrics) ---\")\n",
        "        # Display the DataFrame\n",
        "        display(benchmark_df)\n",
        "    else:\n",
        "        print(\"\\nNo evaluation reports were found. Please ensure the training and evaluation scripts ran successfully.\")\n",
        "\n",
        "# Run the benchmark function\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_models()\n"
      ],
      "metadata": {
        "id": "Z_UsU1KjIsIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "def plot_haversine_error_line_chart(report_dir: str, models: list):\n",
        "    \"\"\"\n",
        "    Generates a line chart comparing the mean Haversine error for all models\n",
        "    at 1, 2, and 3-hour forecast horizons.\n",
        "\n",
        "    Args:\n",
        "        report_dir (str): The directory where the evaluation reports are stored.\n",
        "        models (list): A list of model names (e.g., ['lstm', 'gru']).\n",
        "    \"\"\"\n",
        "    model_data = {}\n",
        "    print(f\"Reading evaluation reports from directory: {report_dir}\")\n",
        "\n",
        "    # Loop through each model to load its corresponding evaluation report\n",
        "    for model_name in models:\n",
        "        report_path = Path(report_dir) / f\"{model_name}_evaluation_report.json\"\n",
        "\n",
        "        if not report_path.exists():\n",
        "            print(f\"Warning: Report not found for model '{model_name}' at {report_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(report_path, 'r') as f:\n",
        "                report_data = json.load(f)\n",
        "\n",
        "            # Extract the specific metrics we want to plot.\n",
        "            errors = [\n",
        "                report_data.get(\"mean_haversine_error_1h\"),\n",
        "                report_data.get(\"mean_haversine_error_2h\"),\n",
        "                report_data.get(\"mean_haversine_error_3h\")\n",
        "            ]\n",
        "            if all(e is not None for e in errors):\n",
        "                model_data[model_name.upper()] = errors\n",
        "                print(f\"Successfully loaded '{model_name}' data.\")\n",
        "            else:\n",
        "                print(f\"Warning: Missing Haversine error data for '{model_name}'.\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: Could not decode JSON from {report_path}.\")\n",
        "\n",
        "    if not model_data:\n",
        "        print(\"No valid error data found to plot. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # --- Create the line chart ---\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    forecast_horizons = [1, 2, 3]  # Horizons in hours\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    # Define a color palette for the lines\n",
        "    colors = plt.cm.tab10.colors\n",
        "\n",
        "    # Plot lines for each model\n",
        "    for i, (model_name, errors) in enumerate(model_data.items()):\n",
        "        ax.plot(forecast_horizons, errors, marker='o', linestyle='-', label=model_name, color=colors[i])\n",
        "\n",
        "        # Annotate each point on the line\n",
        "        for x, y in zip(forecast_horizons, errors):\n",
        "            ax.annotate(f'{y:.0f}', (x, y), textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=8)\n",
        "\n",
        "    # Add labels and title for clarity\n",
        "    ax.set_title('Mean Haversine Error Trend by Forecast Horizon', fontsize=16, fontweight='bold')\n",
        "    ax.set_xlabel('Forecast Horizon (hours)', fontsize=12)\n",
        "    ax.set_ylabel('Mean Haversine Error (km)', fontsize=12)\n",
        "    ax.set_xticks(forecast_horizons)\n",
        "    ax.set_xticklabels([f'{h}-Hour' for h in forecast_horizons])\n",
        "    ax.legend(title='Model Architecture')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    all_model_types = ['lstm', 'gru', 'bilstm', 'bilstm_attention']\n",
        "    evaluation_report_directory = 'evaluation_reports'\n",
        "    plot_haversine_error_line_chart(evaluation_report_directory, all_model_types)\n"
      ],
      "metadata": {
        "id": "SNhA8zhd6Hr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "def load_evaluation_data(report_dir: Path) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Loads all JSON evaluation reports from a specified directory into a list.\n",
        "    \"\"\"\n",
        "    evaluation_data = []\n",
        "    if not report_dir.is_dir():\n",
        "        print(f\"Error: Directory '{report_dir}' not found.\")\n",
        "        return evaluation_data\n",
        "\n",
        "    # Find all JSON files that end with '_evaluation_report.json'\n",
        "    json_files = sorted([f for f in report_dir.glob(\"*_evaluation_report.json\")])\n",
        "    if not json_files:\n",
        "        print(f\"No evaluation report files found in '{report_dir}'. Please ensure they are present.\")\n",
        "        return evaluation_data\n",
        "\n",
        "    for file_path in json_files:\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                # Extract model type from the filename\n",
        "                model_type = file_path.name.replace('_evaluation_report.json', '')\n",
        "                data['model_type'] = model_type  # Add model type for identification\n",
        "                evaluation_data.append(data)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON from file: {file_path}. Error: {e}\")\n",
        "\n",
        "    return evaluation_data\n",
        "\n",
        "def plot_mse_over_time_haversine_style(evaluation_reports: List[Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Generates a line graph of the average MSE over different forecast horizons,\n",
        "    styled to match the provided Haversine plot example.\n",
        "    \"\"\"\n",
        "    if not evaluation_reports:\n",
        "        return\n",
        "\n",
        "    # Convert the list of dictionaries to a pandas DataFrame for easier plotting\n",
        "    df = pd.DataFrame(evaluation_reports)\n",
        "    df.set_index('model_type', inplace=True)\n",
        "    df.index = df.index.str.upper()\n",
        "\n",
        "    print(\"--- Generating Average MSE Trend Line Graph (Haversine Style) ---\")\n",
        "\n",
        "    # Dynamically find all columns that contain 'mse_'\n",
        "    mse_metrics = [col for col in df.columns if 'mse_' in col]\n",
        "\n",
        "    if not mse_metrics:\n",
        "        print(\"Error: No columns with 'mse_' found in the evaluation data. Please check your JSON report keys.\")\n",
        "        return\n",
        "\n",
        "    # --- Create the line chart with seaborn style and annotations ---\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    # Transpose the data to have forecast horizons on the x-axis\n",
        "    mse_data = df[mse_metrics].T\n",
        "\n",
        "    # Create clearer labels for the x-axis from the metric names\n",
        "    x_labels = [metric.replace('mse_', '').replace('_deg', '') for metric in mse_metrics]\n",
        "    forecast_horizons = np.arange(len(x_labels))\n",
        "\n",
        "    # Define a color palette for the lines\n",
        "    colors = plt.cm.tab10.colors\n",
        "\n",
        "    # Plot lines for each model\n",
        "    for i, model_name in enumerate(mse_data.columns):\n",
        "        errors = mse_data[model_name].values\n",
        "        ax.plot(forecast_horizons, errors, marker='o', linestyle='-', label=model_name, color=colors[i])\n",
        "\n",
        "        # Annotate each point on the line with the MSE value\n",
        "        for x, y in zip(forecast_horizons, errors):\n",
        "            ax.annotate(f'{y:.2f}', (x, y), textcoords=\"offset points\", xytext=(0, 10), ha='center', fontsize=9)\n",
        "\n",
        "    ax.set_title('Mean Squared Error Trend by Forecast Horizon', fontsize=16, fontweight='bold')\n",
        "    ax.set_ylabel('Mean Squared Error (Degrees squared)', fontsize=12)\n",
        "    ax.set_xlabel('Forecast Horizon', fontsize=12)\n",
        "    ax.set_xticks(forecast_horizons)\n",
        "    ax.set_xticklabels([f'{h}' for h in x_labels])\n",
        "    ax.legend(title='Model Architecture')\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define the directory where your evaluation reports are saved\n",
        "    evaluation_reports_directory = Path('/content/evaluation_reports')\n",
        "\n",
        "    # Load the data and plot the graph\n",
        "    reports = load_evaluation_data(evaluation_reports_directory)\n",
        "    if reports:\n",
        "        plot_mse_over_time_haversine_style(reports)\n",
        "\n",
        "    print(\"Plotting complete.\")\n"
      ],
      "metadata": {
        "id": "xxIfDU6sAZiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "def plot_median_vs_max_haversine(report_dir: str, models: list):\n",
        "    \"\"\"\n",
        "    Generates a grouped bar chart comparing the median and max Haversine error\n",
        "    for all models.\n",
        "\n",
        "    Args:\n",
        "        report_dir (str): The directory where the evaluation reports are stored.\n",
        "        models (list): A list of model names (e.g., ['lstm', 'gru']).\n",
        "    \"\"\"\n",
        "    model_data = {}\n",
        "    print(f\"Reading evaluation reports from directory: {report_dir}\")\n",
        "\n",
        "    for model_name in models:\n",
        "        report_path = Path(report_dir) / f\"{model_name}_evaluation_report.json\"\n",
        "\n",
        "        if not report_path.exists():\n",
        "            print(f\"Warning: Report not found for model '{model_name}' at {report_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(report_path, 'r') as f:\n",
        "                report_data = json.load(f)\n",
        "\n",
        "            median_error = report_data.get(\"median_haversine_error\")\n",
        "            max_error = report_data.get(\"max_haversine_error\")\n",
        "\n",
        "            if median_error is not None and max_error is not None:\n",
        "                model_data[model_name.upper()] = [median_error, max_error]\n",
        "                print(f\"Successfully loaded '{model_name}' median and max error data.\")\n",
        "            else:\n",
        "                print(f\"Warning: Missing median or max Haversine error data for '{model_name}'.\")\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: Could not decode JSON from {report_path}.\")\n",
        "\n",
        "    if not model_data:\n",
        "        print(\"No valid error data found to plot. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # --- Create the grouped bar chart ---\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    error_types = ['Median Error', 'Max Error']\n",
        "    x = np.arange(len(error_types))\n",
        "    width = 0.2\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    for i, (model_name, errors) in enumerate(model_data.items()):\n",
        "        bars = ax.bar(x + i*width - width*1.5, errors, width, label=model_name)\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.annotate(f'{height:.0f}',\n",
        "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                        xytext=(0, 3),\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom', fontsize=8, rotation=90)\n",
        "\n",
        "    ax.set_title('Median vs. Max Haversine Error', fontsize=16, fontweight='bold')\n",
        "    ax.set_xlabel('Error Type', fontsize=12)\n",
        "    ax.set_ylabel('Haversine Error (km)', fontsize=12)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(error_types)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    all_model_types = ['lstm', 'gru', 'bilstm', 'bilstm_attention']\n",
        "    evaluation_report_directory = 'evaluation_reports'\n",
        "    plot_median_vs_max_haversine(evaluation_report_directory, all_model_types)\n"
      ],
      "metadata": {
        "id": "qSPiGhi88yoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "def plot_lat_lon_rmse(report_dir: str, models: list):\n",
        "    \"\"\"\n",
        "    Generates a grouped bar chart comparing the Root Mean Square Error (RMSE)\n",
        "    for latitude and longitude predictions for all models.\n",
        "\n",
        "    Args:\n",
        "        report_dir (str): The directory where the evaluation reports are stored.\n",
        "        models (list): A list of model names (e.g., ['lstm', 'gru']).\n",
        "    \"\"\"\n",
        "    model_data = {}\n",
        "    print(f\"Reading evaluation reports from directory: {report_dir}\")\n",
        "\n",
        "    for model_name in models:\n",
        "        report_path = Path(report_dir) / f\"{model_name}_evaluation_report.json\"\n",
        "\n",
        "        if not report_path.exists():\n",
        "            print(f\"Warning: Report not found for model '{model_name}' at {report_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(report_path, 'r') as f:\n",
        "                report_data = json.load(f)\n",
        "\n",
        "            lat_rmse = report_data.get(\"lat_rmse_deg\")\n",
        "            lon_rmse = report_data.get(\"lon_rmse_deg\")\n",
        "\n",
        "            if lat_rmse is not None and lon_rmse is not None:\n",
        "                model_data[model_name.upper()] = [lat_rmse, lon_rmse]\n",
        "                print(f\"Successfully loaded '{model_name}' latitude and longitude RMSE data.\")\n",
        "            else:\n",
        "                print(f\"Warning: Missing latitude or longitude RMSE data for '{model_name}'.\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: Could not decode JSON from {report_path}.\")\n",
        "\n",
        "    if not model_data:\n",
        "        print(\"No valid error data found to plot. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # --- Create the grouped bar chart ---\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    error_types = ['Latitude RMSE', 'Longitude RMSE']\n",
        "    x = np.arange(len(error_types))\n",
        "    width = 0.2\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "    for i, (model_name, errors) in enumerate(model_data.items()):\n",
        "        bars = ax.bar(x + i*width - width*1.5, errors, width, label=model_name)\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.annotate(f'{height:.2f}',\n",
        "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                        xytext=(0, 3),\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom', fontsize=8, rotation=90)\n",
        "\n",
        "    ax.set_title('Latitude vs. Longitude RMSE', fontsize=16, fontweight='bold')\n",
        "    ax.set_xlabel('Error Type', fontsize=12)\n",
        "    ax.set_ylabel('RMSE (degrees)', fontsize=12)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(error_types)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    all_model_types = ['lstm', 'gru', 'bilstm', 'bilstm_attention']\n",
        "    evaluation_report_directory = 'evaluation_reports'\n",
        "    plot_lat_lon_rmse(evaluation_report_directory, all_model_types)\n"
      ],
      "metadata": {
        "id": "tRJhqNoI82Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "def load_evaluation_data(report_dir: Path) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Loads all JSON evaluation reports from a specified directory into a list.\n",
        "    \"\"\"\n",
        "    evaluation_data = []\n",
        "    if not report_dir.is_dir():\n",
        "        print(f\"Error: Directory '{report_dir}' not found.\")\n",
        "        return evaluation_data\n",
        "\n",
        "    # Find all JSON files that end with '_evaluation_report.json'\n",
        "    json_files = sorted([f for f in report_dir.glob(\"*_evaluation_report.json\")])\n",
        "    if not json_files:\n",
        "        print(f\"No evaluation report files found in '{report_dir}'. Please run train_and_evaluate.py first.\")\n",
        "        return evaluation_data\n",
        "\n",
        "    for file_path in json_files:\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "                # Extract model type from the filename\n",
        "                model_type = file_path.name.replace('_evaluation_report.json', '')\n",
        "                data['model_type'] = model_type  # Add model type for identification\n",
        "                evaluation_data.append(data)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON from file: {file_path}. Error: {e}\")\n",
        "\n",
        "    return evaluation_data\n",
        "\n",
        "def plot_model_performance(evaluation_reports: List[Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Generates and displays performance plots for the models.\n",
        "    \"\"\"\n",
        "    if not evaluation_reports:\n",
        "        return\n",
        "\n",
        "    # Convert the list of dictionaries to a pandas DataFrame for easier plotting\n",
        "    df = pd.DataFrame(evaluation_reports)\n",
        "    df.set_index('model_type', inplace=True)\n",
        "    df.index = df.index.str.upper()\n",
        "\n",
        "    print(\"--- Generating Performance Plots ---\")\n",
        "\n",
        "    # --- Plot 1: Mean Haversine Error Over Time ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    haversine_metrics = ['mean_haversine_error_1h', 'mean_haversine_error_2h', 'mean_haversine_error_3h']\n",
        "    haversine_data = df[haversine_metrics].T\n",
        "\n",
        "    haversine_data.plot(kind='bar', ax=ax, width=0.8)\n",
        "\n",
        "    ax.set_title('Mean Haversine Error Over Time', fontsize=16)\n",
        "    ax.set_ylabel('Error (meters)', fontsize=12)\n",
        "    ax.set_xlabel('Forecast Horizon', fontsize=12)\n",
        "    ax.set_xticklabels(['1-hour', '2-hour', '3-hour'], rotation=0)\n",
        "    ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # --- Plot 2: Average MSE and RMSE ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    mse_rmse_metrics = ['avg_mse_deg', 'avg_rmse_deg']\n",
        "\n",
        "    df[mse_rmse_metrics].plot(kind='bar', ax=ax, width=0.8)\n",
        "\n",
        "    ax.set_title('Average MSE and RMSE', fontsize=16)\n",
        "    ax.set_ylabel('Error (Degrees)', fontsize=12)\n",
        "    ax.set_xlabel('Model Type', fontsize=12)\n",
        "    ax.set_xticklabels(df.index, rotation=0)\n",
        "    ax.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # --- Plot 3: Average R-squared Score ---\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    df['avg_r2'].plot(kind='bar', ax=ax, color='teal', width=0.6)\n",
        "\n",
        "    ax.set_title('Average R-squared (R) Score', fontsize=16)\n",
        "    ax.set_ylabel('R Score', fontsize=12)\n",
        "    ax.set_xlabel('Model Type', fontsize=12)\n",
        "    ax.set_xticklabels(df.index, rotation=0)\n",
        "    ax.axhline(0, color='grey', linestyle='--')\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    ax.set_ylim(bottom=0)  # Ensure the bar chart starts from 0 for fair comparison\n",
        "\n",
        "    # Add R values on top of the bars\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.3f', padding=5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Define the directory where your evaluation reports are saved\n",
        "    evaluation_reports_directory = Path('/content/evaluation_reports')\n",
        "\n",
        "    # Load the data and plot the graphs\n",
        "    reports = load_evaluation_data(evaluation_reports_directory)\n",
        "    if reports:\n",
        "        plot_model_performance(reports)\n",
        "\n",
        "    print(\"Plotting complete.\")\n"
      ],
      "metadata": {
        "id": "2ta2utiZ_4mK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}